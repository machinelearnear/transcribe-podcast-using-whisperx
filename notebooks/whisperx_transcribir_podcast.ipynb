{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/machinelearnear/transcribe-podcast-using-whisperx/blob/main/notebooks/whisperx_transcribir_podcast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TkBM6oS7ROk"
   },
   "source": [
    "## transcribir video de youtube identificando speakers y timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"<aca-pone-tu-hf-token>\" # https://huggingface.co/settings/tokens\n",
    "URL = 'https://www.youtube.com/watch?v=OA1biHKSyTw' # El Método Rebord #48 - Alejandro Dolina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxxNYfrT8MHh"
   },
   "source": [
    "### instalar todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psb7KUiJ6dwk",
    "outputId": "ab92fee9-ac6a-4245-a857-cfde1bef872b"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/m-bain/whisperx.git;\n",
    "!python3 -m pip install -U yt-dlp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3Trcf6-8TJV"
   },
   "source": [
    "### bajarse el podcast de youtube a un `wav`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yt_dlp\n",
    "\n",
    "# ℹ️ See help(yt_dlp.YoutubeDL) for a list of available options and public functions\n",
    "ydl_opts = {}\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    info = ydl.extract_info(URL, download=False)\n",
    "\n",
    "    # # ℹ️ ydl.sanitize_info makes the info json-serializable\n",
    "    # print(json.dumps(ydl.sanitize_info(info)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_title(title):\n",
    "    # Extract the episode number using a regex pattern\n",
    "    episode_number = re.search(r'#(\\d+)', title)\n",
    "    if episode_number:\n",
    "        episode_number = episode_number.group(1).zfill(3)\n",
    "    else:\n",
    "        raise ValueError(\"Episode number not found in the input string\")\n",
    "\n",
    "    # Remove any non-alphanumeric characters and split the words into a list\n",
    "    words = re.findall(r'\\b\\w+\\b', title)\n",
    "\n",
    "    # Find the index of the hyphen (-) separator\n",
    "    separator_index = title.find('-')\n",
    "\n",
    "    if separator_index == -1:\n",
    "        raise ValueError(\"Separator (-) not found in the input string\")\n",
    "\n",
    "    # Extract the names after the separator and remove any leading/trailing whitespace\n",
    "    names = title[separator_index+1:].strip()\n",
    "\n",
    "    # Combine the episode number and names into the desired format\n",
    "    result = f\"episode_{episode_number}_{names.replace(' ', '_')}\".lower()\n",
    "    return result\n",
    "\n",
    "input_title = ydl.sanitize_info(info)[\"title\"]\n",
    "output_title = convert_title(input_title)\n",
    "output_filename = f'output/{output_title}.wav'\n",
    "print(output_title)  # Output: episode_048_alejandro_dolina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydl_opts = {\n",
    "    'outtmpl': f'output/{output_title}',\n",
    "    'format': 'm4a/bestaudio/best',\n",
    "    # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
    "    'postprocessors': [{  # Extract audio using ffmpeg\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'wav',\n",
    "    }]\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    error_code = ydl.download(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPe15lct8h4D"
   },
   "source": [
    "### correr `openai/whisper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_guest_name(title):\n",
    "    # Find the index of the hyphen (-) separator\n",
    "    separator_index = title.find('-')\n",
    "\n",
    "    if separator_index == -1:\n",
    "        raise ValueError(\"Separator (-) not found in the input string\")\n",
    "\n",
    "    # Extract the guest's name after the separator and remove any leading/trailing whitespace\n",
    "    guest_name = title[separator_index+1:].strip()\n",
    "\n",
    "    return guest_name\n",
    "\n",
    "input_title = \"El Método Rebord #48 - Alejandro Dolina\"\n",
    "guest_name = extract_guest_name(input_title)\n",
    "host_name = \"Tomás Rebord\"\n",
    "print(guest_name)  # Output: Alejandro Dolina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!whisperx $output_filename --hf_token $hf_token --model small --language es --vad_filter --align_model WAV2VEC2_ASR_LARGE_LV60K_960H --diarize --min_speakers 2 --max_speakers 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detectar distintos speakers y generar archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_srt(input):\n",
    "    entries = input.strip().split(\"\\n\\n\")\n",
    "    output_lines = []\n",
    "    last_speaker = None\n",
    "    \n",
    "    for entry in entries:\n",
    "        lines = entry.strip().split(\"\\n\")\n",
    "        time_start = lines[1].split(\" --> \")[0]\n",
    "        speaker, text = re.match(r'\\[(.+)\\]:\\s*(.+)', lines[2]).groups()\n",
    "        \n",
    "        if last_speaker and speaker != last_speaker:\n",
    "            output_lines.append(\"\")\n",
    "        \n",
    "        if output_lines and speaker == last_speaker:\n",
    "            output_lines[-1] += \" \" + text\n",
    "        else:\n",
    "            output_lines.append(f\"[{speaker}, {time_start}] {text}\")\n",
    "        \n",
    "        last_speaker = speaker\n",
    "\n",
    "    return \"\\n\".join(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_srt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        srt_content = file.read()\n",
    "    return srt_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srt_string = read_srt_file(f\"{output_filename}.word.srt\")\n",
    "print(parse_srt(srt_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_transcript(input_str):\n",
    "    # Use regular expression to find and split the input by speaker and timestamps\n",
    "    speaker_timestamp_pattern = r'\\[(SPEAKER_\\d+), \\d{2}:\\d{2}:\\d{2},\\d{3}\\]'\n",
    "    segments = re.split(speaker_timestamp_pattern, input_str)\n",
    "    \n",
    "    # Remove the first empty string from the list\n",
    "    segments = segments[1:]\n",
    "\n",
    "    output = []\n",
    "    for i in range(0, len(segments), 2):\n",
    "        speaker = segments[i]\n",
    "        text = segments[i + 1].strip()\n",
    "        output.append(f\"{speaker} ha dicho: {text}\\n\")\n",
    "\n",
    "    return ''.join(output)\n",
    "\n",
    "def replace_speaker_names(input_str, host_name=host_name, guest_name=guest_name):\n",
    "    speaker_mapping = {\n",
    "        \"SPEAKER_00\": host_name,\n",
    "        \"SPEAKER_01\": guest_name,\n",
    "        # Add more mappings if needed\n",
    "    }\n",
    "\n",
    "    output_str = input_str\n",
    "    for old_speaker, new_speaker in speaker_mapping.items():\n",
    "        output_str = output_str.replace(old_speaker, new_speaker)\n",
    "        \n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_str = replace_speaker_names(process_transcript(parse_srt(srt_string)))\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt_file(input_str, filename=\"output.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_txt_file(output_str, f\"{output_title}_with_speakers_timestamped.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langflow\n",
      "  Downloading langflow-0.0.46.tar.gz (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-api-python-client<3.0.0,>=2.79.0\n",
      "  Downloading google_api_python_client-2.83.0-py2.py3-none-any.whl (11.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.2 MB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting types-pyyaml<7.0.0.0,>=6.0.12.8\n",
      "  Downloading types_PyYAML-6.0.12.9-py3-none-any.whl (14 kB)\n",
      "Collecting gunicorn<21.0.0,>=20.1.0\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting beautifulsoup4<5.0.0,>=4.11.2\n",
      "  Downloading beautifulsoup4-4.12.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 43.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-search-results<3.0.0,>=2.4.1\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "Collecting fastapi<0.93.0,>=0.92.0\n",
      "  Downloading fastapi-0.92.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 8.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting typer<0.8.0,>=0.7.0\n",
      "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting uvicorn<0.21.0,>=0.20.0\n",
      "  Downloading uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting langchain<0.0.114,>=0.0.113\n",
      "  Downloading langchain-0.0.113-py3-none-any.whl (396 kB)\n",
      "\u001b[K     |████████████████████████████████| 396 kB 79.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting openai<0.28.0,>=0.27.2\n",
      "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from beautifulsoup4<5.0.0,>=4.11.2->langflow) (2.3.2.post1)\n",
      "Collecting starlette<0.26.0,>=0.25.0\n",
      "  Downloading starlette-0.25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
      "  Downloading pydantic-1.10.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 43.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting uritemplate<5,>=3.0.1\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-auth<3.0.0dev,>=1.19.0\n",
      "  Downloading google_auth-2.17.1-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 55.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-httplib2>=0.1.0\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 47.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client<3.0.0,>=2.79.0->langflow) (2.28.2)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.22.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 77.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "\u001b[K     |████████████████████████████████| 223 kB 43.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client<3.0.0,>=2.79.0->langflow) (1.16.0)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: setuptools>=3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from gunicorn<21.0.0,>=20.1.0->langflow) (65.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<3.0.0,>=2.79.0->langflow) (3.0.9)\n",
      "Collecting PyYAML<7,>=6\n",
      "  Using cached PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
      "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain<0.0.114,>=0.0.113->langflow) (1.24.1)\n",
      "Collecting SQLAlchemy<2,>=1\n",
      "  Downloading SQLAlchemy-1.4.47-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 61.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.0.114,>=0.0.113->langflow) (3.0.1)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.0.114,>=0.0.113->langflow) (22.1.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0\n",
      "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain<0.0.114,>=0.0.113->langflow) (21.3)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi<0.93.0,>=0.92.0->langflow) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client<3.0.0,>=2.79.0->langflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client<3.0.0,>=2.79.0->langflow) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client<3.0.0,>=2.79.0->langflow) (3.4)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (610 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from starlette<0.26.0,>=0.25.0->fastapi<0.93.0,>=0.92.0->langflow) (3.6.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.26.0,>=0.25.0->fastapi<0.93.0,>=0.92.0->langflow) (1.3.0)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 2.6 MB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: langflow, google-search-results\n",
      "  Building wheel for langflow (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langflow: filename=langflow-0.0.46-py3-none-any.whl size=1036478 sha256=ec21f296b896106ffdef2def06f3649fdead2a2fa4e671d6b21d5bc4857350c5\n",
      "  Stored in directory: /home/studio-lab-user/.cache/pip/wheels/34/b8/13/536babc049bb9e642fd6b63fae74fda5c2f27523fbaa2c6b2b\n",
      "  Building wheel for google-search-results (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32003 sha256=d8b8aca8e09214232647da5fd2070218113e99a9a142e2a7148cab33e1698226\n",
      "  Stored in directory: /home/studio-lab-user/.cache/pip/wheels/68/8e/73/744b7d9d7ac618849d93081a20e1c0deccd2aef90901c9f5a9\n",
      "Successfully built langflow google-search-results\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, protobuf, mypy-extensions, multidict, marshmallow, frozenlist, cachetools, yarl, typing-inspect, marshmallow-enum, httplib2, greenlet, googleapis-common-protos, google-auth, async-timeout, aiosignal, uritemplate, tqdm, tenacity, starlette, SQLAlchemy, PyYAML, pydantic, h11, google-auth-httplib2, google-api-core, dataclasses-json, click, aiohttp, uvicorn, types-pyyaml, typer, openai, langchain, gunicorn, google-search-results, google-api-python-client, fastapi, beautifulsoup4, langflow\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.11.1\n",
      "    Uninstalling beautifulsoup4-4.11.1:\n",
      "      Successfully uninstalled beautifulsoup4-4.11.1\n",
      "Successfully installed PyYAML-6.0 SQLAlchemy-1.4.47 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 beautifulsoup4-4.12.0 cachetools-5.3.0 click-8.1.3 dataclasses-json-0.5.7 fastapi-0.92.0 frozenlist-1.3.3 google-api-core-2.11.0 google-api-python-client-2.83.0 google-auth-2.17.1 google-auth-httplib2-0.1.0 google-search-results-2.4.2 googleapis-common-protos-1.59.0 greenlet-2.0.2 gunicorn-20.1.0 h11-0.14.0 httplib2-0.22.0 langchain-0.0.113 langflow-0.0.46 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openai-0.27.2 protobuf-4.22.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydantic-1.10.7 rsa-4.9 starlette-0.25.0 tenacity-8.2.2 tqdm-4.65.0 typer-0.7.0 types-pyyaml-6.0.12.9 typing-inspect-0.8.0 uritemplate-4.1.1 uvicorn-0.20.0 yarl-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install langflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-01 23:58:15 +0000] [262] [INFO] Starting gunicorn 20.1.0\n",
      "[2023-04-01 23:58:15 +0000] [262] [INFO] Listening at: http://0.0.0.0:6006 (262)\n",
      "[2023-04-01 23:58:15 +0000] [262] [INFO] Using worker: uvicorn.workers.UvicornWorker\n",
      "[2023-04-01 23:58:15 +0000] [266] [INFO] Booting worker with pid: 266\n",
      "[2023-04-01 23:58:15 +0000] [266] [INFO] Started server process [266]\n",
      "[2023-04-01 23:58:15 +0000] [266] [INFO] Waiting for application startup.\n",
      "[2023-04-01 23:58:15 +0000] [266] [INFO] Application startup complete.\n"
     ]
    }
   ],
   "source": [
    "!langflow --host 0.0.0.0 --port 6006"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNs8FEieYIwYgV1rceaTH3i",
   "include_colab_link": true,
   "name": "whisperx-example-youtube.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
