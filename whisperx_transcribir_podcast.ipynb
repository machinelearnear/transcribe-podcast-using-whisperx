{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "whisperx-example-youtube.ipynb",
      "authorship_tag": "ABX9TyNs8FEieYIwYgV1rceaTH3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machinelearnear/nelson-openai-master-plan/blob/main/whisperx_transcribir_podcast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧉 machinelearnear [📺](https://www.youtube.com/c/machinelearnear)"
      ],
      "metadata": {
        "id": "3TkBM6oS7ROk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### instalar las cosas"
      ],
      "metadata": {
        "id": "RxxNYfrT8MHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psb7KUiJ6dwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab92fee9-ac6a-4245-a857-cfde1bef872b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/m-bain/whisperx.git\n",
            "  Cloning https://github.com/m-bain/whisperx.git to /tmp/pip-req-build-2au1t540\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/m-bain/whisperx.git /tmp/pip-req-build-2au1t540\n",
            "  Resolved https://github.com/m-bain/whisperx.git to commit 809700e286a1fa40315c936876e78907be07892a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from whisperx==1.0) (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from whisperx==1.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (from whisperx==1.0) (0.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from whisperx==1.0) (4.65.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.9/dist-packages (from whisperx==1.0) (9.1.0)\n",
            "Collecting transformers>=4.19.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting pyannote.audio\n",
            "  Downloading pyannote.audio-2.1.1-py2.py3-none-any.whl (390 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.7/390.7 KB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soundfile in /usr/local/lib/python3.9/dist-packages (from whisperx==1.0) (0.12.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from ffmpeg-python==0.2.0->whisperx==1.0) (0.16.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisperx==1.0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisperx==1.0) (2.25.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisperx==1.0) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisperx==1.0) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisperx==1.0) (2022.6.2)\n",
            "Collecting hmmlearn<0.3,>=0.2.7\n",
            "  Downloading hmmlearn-0.2.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (217 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.2/217.2 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pyannote.audio->whisperx==1.0) (4.5.0)\n",
            "Collecting backports.cached-property\n",
            "  Downloading backports.cached_property-1.0.2-py3-none-any.whl (6.1 kB)\n",
            "Collecting pyannote.database<5.0,>=4.1.1\n",
            "  Downloading pyannote.database-4.1.3-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx<3.0,>=2.6\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-metric-learning<2.0,>=1.0.0\n",
            "  Downloading pytorch_metric_learning-1.7.3-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops<0.4.0,>=0.3\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Collecting speechbrain<0.6,>=0.5.12\n",
            "  Downloading speechbrain-0.5.13-py3-none-any.whl (498 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.0/499.0 KB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics<1.0,>=0.6\n",
            "  Downloading torchmetrics-0.11.3-py3-none-any.whl (518 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.6/518.6 KB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.core<5.0,>=4.4\n",
            "  Downloading pyannote.core-4.5-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asteroid-filterbanks<0.5,>=0.4\n",
            "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
            "Collecting torch-audiomentations>=0.11.0\n",
            "  Downloading torch_audiomentations-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting soundfile\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Collecting semver<3.0,>=2.10.2\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting singledispatchmethod\n",
            "  Downloading singledispatchmethod-1.0-py2.py3-none-any.whl (4.7 kB)\n",
            "Collecting pyannote.pipeline<3.0,>=2.3\n",
            "  Downloading pyannote.pipeline-2.3-py3-none-any.whl (30 kB)\n",
            "Collecting pytorch-lightning<1.7,>=1.5.4\n",
            "  Downloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 KB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.metrics<4.0,>=3.2\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<3.0,>=2.1\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile->whisperx==1.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile->whisperx==1.0) (2.21)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.9/dist-packages (from hmmlearn<0.3,>=0.2.7->pyannote.audio->whisperx==1.0) (1.2.1)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.9/dist-packages (from hmmlearn<0.3,>=0.2.7->pyannote.audio->whisperx==1.0) (1.10.1)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.9/dist-packages (from pyannote.core<5.0,>=4.4->pyannote.audio->whisperx==1.0) (2.4.0)\n",
            "Collecting simplejson>=3.8.1\n",
            "  Downloading simplejson-3.18.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.8/136.8 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer[all]>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from pyannote.database<5.0,>=4.1.1->pyannote.audio->whisperx==1.0) (0.7.0)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.9/dist-packages (from pyannote.database<5.0,>=4.1.1->pyannote.audio->whisperx==1.0) (1.3.5)\n",
            "Requirement already satisfied: sympy>=1.1 in /usr/local/lib/python3.9/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (1.7.1)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (3.5.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.9/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (0.8.10)\n",
            "Collecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting optuna>=1.4\n",
            "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (2023.3.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (2.11.2)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (3.19.6)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperpyyaml\n",
            "  Downloading HyperPyYAML-1.1.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from speechbrain<0.6,>=0.5.12->pyannote.audio->whisperx==1.0) (1.2.0)\n",
            "Collecting torch-pitch-shift>=1.2.2\n",
            "  Downloading torch_pitch_shift-1.2.2-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (0.8.1)\n",
            "Collecting julius<0.3,>=0.2.3\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.19.0->whisperx==1.0) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.19.0->whisperx==1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.19.0->whisperx==1.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.19.0->whisperx==1.0) (4.0.0)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (0.4.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (0.56.4)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (1.7.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (3.0.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (4.39.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (0.11.0)\n",
            "Collecting cmaes>=0.9.1\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.10.2-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna>=1.4->pyannote.pipeline<3.0,>=2.3->pyannote.audio->whisperx==1.0) (1.4.46)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.19->pyannote.database<5.0,>=4.1.1->pyannote.audio->whisperx==1.0) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.16->hmmlearn<0.3,>=0.2.7->pyannote.audio->whisperx==1.0) (3.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy>=1.1->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (1.2.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (1.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (2.16.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (57.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (1.51.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (0.4.6)\n",
            "Collecting primePy>=1.3\n",
            "  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer[all]>=0.2.1->pyannote.database<5.0,>=4.1.1->pyannote.audio->whisperx==1.0) (8.1.3)\n",
            "Collecting shellingham<2.0.0,>=1.3.0\n",
            "  Downloading shellingham-1.5.0.post1-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting rich<13.0.0,>=10.11.0\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama<0.5.0,>=0.4.3\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting ruamel.yaml>=0.17.8\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (22.2.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (6.0.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.43.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (0.39.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (3.1.0)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich<13.0.0,>=10.11.0->typer[all]>=0.2.1->pyannote.database<5.0,>=4.1.1->pyannote.audio->whisperx==1.0) (2.6.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.4/519.4 KB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna>=1.4->pyannote.pipeline<3.0,>=2.3->pyannote.audio->whisperx==1.0) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (3.2.2)\n",
            "Building wheels for collected packages: whisperx, antlr4-python3-runtime, docopt, julius\n",
            "  Building wheel for whisperx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisperx: filename=whisperx-1.0-py3-none-any.whl size=1191196 sha256=e8e4a1b3c3a89f885680bdd564f26e7e19d08d9f214fe246b8972438fbdc0e0e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bcv21lc0/wheels/c7/67/c8/d92ee7475af476abd5e4fb00fee58ac4f2a9e333668ea3c237\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=64a758e324b990859e268d2d33467505cfe9d55d8ea4f1ea0ce5c51e43961665\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=5c5b219efe0dad7e73d6e6cf1436381e1f65f6703591f25f1a7d1471c124d4ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21895 sha256=5320899103dd2644377265586bcf593f51c59b2ea0bea99f54b4c5e2d3bc15ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/0a/a7/fc08f97438f4969d86afa7904336c2eb7eb422101359f3ad11\n",
            "Successfully built whisperx antlr4-python3-runtime docopt julius\n",
            "Installing collected packages: tokenizers, singledispatchmethod, sentencepiece, primePy, einops, docopt, commonmark, antlr4-python3-runtime, simplejson, shellingham, semver, ruamel.yaml.clib, rich, pyDeprecate, omegaconf, networkx, multidict, Mako, frozenlist, ffmpeg-python, colorlog, colorama, cmaes, charset-normalizer, backports.cached-property, async-timeout, yarl, torchmetrics, soundfile, ruamel.yaml, pyannote.core, julius, huggingface-hub, asteroid-filterbanks, alembic, aiosignal, transformers, torch-pitch-shift, pytorch-metric-learning, pyannote.database, optuna, hyperpyyaml, hmmlearn, aiohttp, torch-audiomentations, speechbrain, pyannote.pipeline, pyannote.metrics, pytorch-lightning, pyannote.audio, whisperx\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.0\n",
            "    Uninstalling networkx-3.0:\n",
            "      Successfully uninstalled networkx-3.0\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: soundfile 0.12.1\n",
            "    Uninstalling soundfile-0.12.1:\n",
            "      Successfully uninstalled soundfile-0.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/m-bain/whisperx.git;\n",
        "!python3 -m pip install -U yt-dlp;"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### hacer un download desde YouTube"
      ],
      "metadata": {
        "id": "E3Trcf6-8TJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_video = \"https://www.youtube.com/watch?v=NJYQzHVz-xI&t=513s&ab_channel=machinelearnear\""
      ],
      "metadata": {
        "id": "vKDhtw3w7vIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m yt_dlp --output \"archivo.%(ext)s\" --extract-audio --audio-format wav $youtube_video"
      ],
      "metadata": {
        "id": "cq7d7TbQ8ooG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504b7b46-33f8-4027-9b78-5b96f83c474e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=4uXeflZ8q8w\n",
            "[youtube] 4uXeflZ8q8w: Downloading webpage\n",
            "[youtube] 4uXeflZ8q8w: Downloading android player API JSON\n",
            "[info] 4uXeflZ8q8w: Downloading 1 format(s): 251\n",
            "[dashsegments] Total fragments: 4\n",
            "[download] Destination: archivo.webm\n",
            "\u001b[K[download] 100% of   38.57MiB in \u001b[1;37m00:00:11\u001b[0m at \u001b[0;32m3.42MiB/s\u001b[0m\n",
            "[ExtractAudio] Destination: archivo.wav\n",
            "Deleting original file archivo.webm (pass -k to keep)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ahora si ... correr WhisperX"
      ],
      "metadata": {
        "id": "NPe15lct8h4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = \"<aca-pone-tu-hf-token\" # https://huggingface.co/settings/tokens"
      ],
      "metadata": {
        "id": "Y8mI9SLPFiHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### transcripción en español"
      ],
      "metadata": {
        "id": "iPiU1e9FreCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisperx archivo.wav --model large-v2 --language es"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejlTXf1u7QFz",
        "outputId": "9d30035f-bf8b-4a99-9f62-26eefec15734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-09 16:45:17.386544: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-09 16:45:18.257200: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-09 16:45:18.257319: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-09 16:45:18.257339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "100%|██████████████████████████████████████| 2.87G/2.87G [00:12<00:00, 248MiB/s]\n",
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_voxpopuli_base_10k_asr_es.pt\" to /root/.cache/torch/hub/checkpoints/wav2vec2_voxpopuli_base_10k_asr_es.pt\n",
            "100% 360M/360M [00:01<00:00, 231MB/s]\n",
            "Performing transcription...\n",
            "[00:00.000 --> 00:05.000]  Muy buenas de nuevo al canal, si es tu primera vez acá, lo que hacemos es divulgación en español de lo último de Machine Learning\n",
            "[00:05.000 --> 00:10.000]  intentando explicar cosas difíciles de manera que sean fáciles de entender y también de usar.\n",
            "[00:10.000 --> 00:14.000]  Este capítulo de hoy va a ser el primero de una serie de vídeos que quería hacer\n",
            "[00:14.000 --> 00:18.000]  para explicar desde cero cómo podemos hacer nuestro propio chat GPT.\n",
            "[00:18.000 --> 00:22.000]  Vamos a ir de cosas muy simples a muy complejas, sin esquivarle a las partes difíciles,\n",
            "[00:22.000 --> 00:28.000]  pero tratando de explicar paso a paso qué es lo que está pasando en cada cosa.\n",
            "[00:28.000 --> 00:33.440]  Así que como siempre vamos a dividir el vídeo por partes explicando primero una motivación de por\n",
            "[00:33.440 --> 00:37.360]  qué hacer una cosa como ésta, después un poco qué son los modelos de lenguaje, qué es GPT-3,\n",
            "[00:37.360 --> 00:43.000]  cómo es el mercado, dónde estamos, explicar un poco de qué está haciendo DeepMind, qué está\n",
            "[00:43.000 --> 00:47.960]  haciendo OpenAI y después qué pasos tenemos que hacer para entrenar nuestro propio chat GPT.\n",
            "[00:49.200 --> 00:53.600]  Es imposible meter todos estos temas en un solo vídeo, por eso voy a hacer una serie.\n",
            "[00:53.600 --> 00:59.520]  Hoy vamos a hacer una especie de introducción y después, semanalmente seguro, cuando salgan,\n",
            "[00:59.520 --> 01:04.360]  digamos, iré poniendo distintas cosas. Vamos a hablar de lo que se llama Reinforcement Learning\n",
            "[01:04.360 --> 01:10.080]  from Human Feedback. Vamos a hablar un poco de cómo hacerle instrucciones a los modelos de\n",
            "[01:10.080 --> 01:15.320]  lenguaje gigantes. Y vamos a hablar de una iniciativa que está haciendo Lion, que es el\n",
            "[01:15.320 --> 01:22.080]  grupo de gente que hizo también, que es parte de las personas que hicieron Stable Diffusion y que\n",
            "[01:22.080 --> 01:27.080]  también están haciendo una versión abierta de ChatGPT que en teoría debería ser aún mejor que\n",
            "[01:27.080 --> 01:32.920]  ChatGPT. Así que vamos a ver todas esas cosas en una serie de vídeos. Así que primero empecemos\n",
            "[01:32.920 --> 01:37.840]  por una motivación, qué es lo que a mí me lleva a hacer una cosa como esto. Bueno primero que nada\n",
            "[01:37.840 --> 01:42.520]  yo creo que estaba viendo este tweet, la verdad mucho de esta conversación está pasando en\n",
            "[01:42.520 --> 01:47.640]  Twitter así que es bastante interesante si no lo usan digamos para seguir todo lo que es la\n",
            "[01:47.640 --> 01:51.760]  parte de Machine Learning de noticias. Está pasando ahí la conversación y en otros dos tres\n",
            "[01:51.760 --> 01:57.520]  lugares pero en Twitter está muy concentrada y acá por ejemplo este está David nos está diciendo que\n",
            "[01:57.520 --> 02:05.240]  está haciendo una analogía con el 2008 dice estamos en el 2008 hay una recesión muy grande y acaba de\n",
            "[02:05.240 --> 02:12.240]  salir el iPhone el año anterior el primer teléfono de Android acaba de salir este año 2008 o sea se\n",
            "[02:12.240 --> 02:15.920]  va a presentar en octubre y después los próximos las próximas tres a cinco generaciones van a\n",
            "[02:15.920 --> 02:20.680]  revolucionar absolutamente todo. Podemos decir que más o menos es lo que estamos teniendo acá ahora\n",
            "[02:20.680 --> 02:26.440]  con ChatGPT, es el primer modelo, no es el último, es el primer modelo que está abriendo el espacio a\n",
            "[02:26.440 --> 02:30.840]  todas estas cosas, lo mismo que pasó con DALI 2, cuando apenas salió DALI, una revolución increíble,\n",
            "[02:30.840 --> 02:34.760]  ahora DALI 2 tampoco se usa tanto, pero las versiones open source, digamos que tenemos\n",
            "[02:34.760 --> 02:41.360]  MeetJourney, tenemos todo lo que es StableDiffusion, etcétera, todo eso es infinitamente mejor ahora\n",
            "[02:41.360 --> 02:47.480]  que DALI 2, pero la razón por la que se aceleró, digamos, ese desarrollo fue por la aparición de DALI.\n",
            "[02:47.480 --> 02:56.260]  Explico un poquito de por qué la motivación. Desde mi parte, por mi laburo, la verdad tengo\n",
            "[02:56.260 --> 03:01.300]  la suerte de poder estar trabajando mucho con modelos de lenguajes gigantes en este momento,\n",
            "[03:01.300 --> 03:05.540]  y lo que estoy viendo es una gran divergencia que se va a producir o se está produciendo ahora. Lo\n",
            "[03:05.540 --> 03:10.760]  voy a exagerar, voy a ir por el absurdo, vamos a decir que hay un grupo de ingenieros en Silicon\n",
            "[03:10.760 --> 03:16.580]  Valley y después tenemos un grupo de ingenieros en Latinoamérica. La diferencia en conocimiento,\n",
            "[03:16.580 --> 03:23.900]  en uso, en generación, en capacidad de poder generar cosas nuevas, de esto es enorme, es lo\n",
            "[03:23.900 --> 03:28.260]  que estoy viendo. Entonces para mí es muy importante, muy relevante que se empiecen a\n",
            "[03:28.260 --> 03:34.580]  dar conversaciones técnicas de cómo funcionan estas cosas, que no sea solamente que no nos\n",
            "[03:34.580 --> 03:40.180]  convertamos simplemente por el español, digamos, en usuarios de estas tecnologías. Tenemos que\n",
            "[03:40.180 --> 03:45.140]  también ser capaces de poder generar estas tecnologías o por lo menos poder modificarlas\n",
            "[03:45.140 --> 03:51.580]  en una forma que podemos generar cosas nuevas y para hacer eso no hay otra forma que meternos\n",
            "[03:51.580 --> 03:56.540]  y meternos de lleno en estas cosas técnicamente, ver qué es lo último que se está haciendo,\n",
            "[03:56.540 --> 04:00.500]  cuáles son las conversaciones que están pasando y básicamente meterle mucho huevo.\n",
            "[04:00.500 --> 04:06.300]  Como un poquito más de introducción, ahí puse que es explicado por un data scientist,\n",
            "[04:06.300 --> 04:12.420]  eso es porque laburo como data scientist, digamos, pero no es lo que estudié. Yo estudié\n",
            "[04:12.420 --> 04:16.020]  ingeniería mecánica hace muchos años 2016 más o menos empecé a hacer mi\n",
            "[04:16.020 --> 04:20.620]  primer curso no sabía ni programar y dije mira me quiero meter en esto la\n",
            "[04:20.620 --> 04:24.780]  verdad me encantó como hago siempre voy a estar en una desventaja con las\n",
            "[04:24.780 --> 04:27.300]  personas que estuvieron física matemática etcétera que ya vienen\n",
            "[04:27.300 --> 04:30.740]  digamos siempre voy a estar años atrasado entonces mi decisión en ese\n",
            "[04:30.740 --> 04:34.020]  momento fue aunque hacer una estrategia voy a hacer un leapfrog que se llama\n",
            "[04:34.020 --> 04:38.260]  saltar voy a meterme directamente en deep learning computer vision en ese\n",
            "[04:38.260 --> 04:42.100]  caso y modelos de lenguaje natural entonces lo que quiero decir con esto es\n",
            "[04:42.100 --> 04:49.180]  Todas las técnicas que existieron hasta este momento es posible que se tiren a la basura en\n",
            "[04:49.180 --> 04:54.460]  este momento. ¿Es importante tener conocimiento matemático? Sí, pero con álgebra, con un nivel\n",
            "[04:54.460 --> 04:58.660]  de álgebra básico yo creo que se pueden hacer cosas muy interesantes. Con esto lo que quiero\n",
            "[04:58.660 --> 05:03.780]  decir es, no importa de dónde están viniendo, lo que importa es lo que quieren hacer en el futuro\n",
            "[05:03.780 --> 05:10.420]  y qué tanto huevo le ponen a estudiar. Así que hoy vamos a ponerle huevo a estudiar. Bueno,\n",
            "[05:10.420 --> 05:16.160]  vamos a ver un poco dónde estamos no con todo esto porque es importante bueno este cómic\n",
            "[05:16.160 --> 05:22.400]  espectacular bien este como dice el ojo ya te diste cuenta cómo va a impactar nuestro negocio\n",
            "[05:22.400 --> 05:26.560]  la inteligencia artificial es y no te preocupes que estoy laburando en eso y le pregunta a chat\n",
            "[05:26.560 --> 05:31.360]  gpt cómo cómo va a impactar la inteligencia artificial nuestro negocio y la verdad que\n",
            "[05:31.360 --> 05:38.980]  hay muchas maneras en la que van excelente cómic esto es algo que está pasando al mismo tiempo que\n",
            "[05:38.980 --> 05:43.700]  Hay otros modelos, no hablé de este modelo VALI, hablamos de Whisper, hablamos de\n",
            "[05:43.700 --> 05:48.500]  ChatsGPT. VALI es un modelo que está sacando Microsoft también, es básicamente un modelo\n",
            "[05:48.500 --> 05:57.580]  que hace text to speech, convierte, genera digamos voz sintetizada. ¿Cómo lo hace? Se copia de un\n",
            "[05:57.580 --> 06:03.220]  modelo de un clip de tres segundos de la persona que queremos copiar y con eso simplemente es lo\n",
            "[06:03.220 --> 06:07.140]  único que necesita para generar audio.\n",
            "[06:07.780 --> 06:12.580]  Entonces, si juntamos estas cosas, esto, un detalle al margen, en Bali me hace\n",
            "[06:12.580 --> 06:16.900]  acordar mucho a la película Terminator 2, cuando llama, digamos, Terminator, ¿qué\n",
            "[06:16.900 --> 06:21.300]  pasó con el perro? Le cambia el nombre y dice tu perro está muerto. Bueno, en ese\n",
            "[06:21.300 --> 06:25.580]  momento a mí me sorprendió muchísimo esa escena. Bueno, podemos ver que Bali es\n",
            "[06:25.580 --> 06:30.060]  algo parecido a eso. Bueno, ¿qué tenemos acá? Un modelo\n",
            "[06:30.060 --> 06:36.860]  conversacional ¿no? o sea Siri, Alexa, ya está, esto le compite directamente a estas dos cosas ¿no?\n",
            "[06:36.860 --> 06:44.180]  Así que tenemos Whisper que convierte de digamos audio a texto, después ChatGPT que convierte el\n",
            "[06:44.180 --> 06:50.300]  texto en algo más, no sabemos qué todavía, en algo y después Valley que lo convierte otra vez en\n",
            "[06:50.300 --> 06:56.980]  en audio ¿no? así que tenemos una conversación, en este caso dice prender las luces de la fiesta\n",
            "[06:56.980 --> 07:04.100]  y empieza a poner música que esté buena a las 8 de la tarde todos los jueves, pero podemos ver cómo\n",
            "[07:04.100 --> 07:10.020]  ya empiezan a aparecer cosas muy nuevas y esto es peligroso no solamente para digamos nosotros\n",
            "[07:10.020 --> 07:14.220]  podemos decir esto bueno algunas cosas están buenas otras cosas son peligrosas pero las\n",
            "[07:14.220 --> 07:22.220]  empresas por ejemplo Canva es una empresa de diseño no es una empresa de diseño que compite con compite\n",
            "[07:22.220 --> 07:31.140]  con adobe pero una cosa que tiene que tiene canva es que iban a salir al mercado ahora o sea pedir\n",
            "[07:31.140 --> 07:38.900]  grita al mercado y los tipos dijeron bueno vamos a tener que contratar a chat gpt o sea no a gpt3\n",
            "[07:38.900 --> 07:43.300]  por lo menos porque no podemos competir con ellos no tenemos las capacidades para competir con estas\n",
            "[07:43.300 --> 07:49.820]  entonces se pueden imaginar como las empresas ya o sea se está generando un sistema donde o estás\n",
            "[07:49.820 --> 07:57.020]  conmigo o te voy a reventar básicamente eso entonces es lo que lo que está pasando y nadie\n",
            "[07:57.020 --> 08:01.300]  está seguro en este sentido no es solamente empresa de tecnología sino cualquier tipo de empresa no\n",
            "[08:01.300 --> 08:07.060]  una empresa de soporte técnico por ejemplo no tiene razón para no sentirse amenazada por algo\n",
            "[08:07.060 --> 08:11.940]  como un chat gpt esto es como apple metiéndose en el mercado automotriz o sea está bien tenemos\n",
            "[08:11.940 --> 08:16.020]  voz bag, tenemos ford, chevrolet, etcétera pero cuando entre apple va a revolucionar todo eso\n",
            "[08:16.020 --> 08:19.300]  así que no sabemos qué es lo que va a pasar.\n",
            "[08:19.300 --> 08:24.100]  Así que bueno, vamos a ver un poquito qué son los modelos de lenguaje y qué es\n",
            "[08:24.100 --> 08:27.620]  GPT-3. Vamos a empezar muy despacito.\n",
            "[08:27.620 --> 08:31.300]  Primero acá hay un, obviamente,\n",
            "[08:31.300 --> 08:35.700]  yo soy muy vago, así que no edito los vídeos, los grabo de una. Es bastante\n",
            "[08:35.700 --> 08:38.620]  difícil hacer esto, así que quiere decir que voy a probablemente equivocarme\n",
            "[08:38.620 --> 08:42.060]  bastante. A medida que voy me voy a olvidar cosas, seguramente me voy a equivocar en\n",
            "[08:42.060 --> 08:48.180]  en otras. Les pido si hay algo que me pasé o que no se entendió o que expliqué para el orto,\n",
            "[08:48.180 --> 08:53.460]  es muy probable que lo pongan en los comentarios y yo después lo volvemos a revisar para la próxima.\n",
            "[08:54.460 --> 08:59.620]  Todos los links que pongo acá los voy a poner en la descripción del vídeo. Mi idea con esto es no\n",
            "[08:59.620 --> 09:05.780]  hacer un vídeo cortito, malo, etcétera, resumido, con vídeos, con gráfica, etcétera. No, no. Esto\n",
            "[09:05.780 --> 09:12.860]  es un vídeo duro para verlo con el tiempo, o sea volver y decir que dijo este este el loco este en\n",
            "[09:12.860 --> 09:19.260]  tal lugar, bueno dijo tal cosa, esa es la idea. Este link es un curso que acaba de salir, se llama CS324\n",
            "[09:19.260 --> 09:25.780]  de la universidad Stanford, es un curso de modelos de lenguaje, está bastante bueno y nos sirve como\n",
            "[09:25.780 --> 09:32.660]  introducción al tema. ¿Qué es un modelo de lenguaje? Bueno la definición clásica que tenemos de un\n",
            "[09:32.660 --> 09:37.500]  módulo del lenguaje es la distribución de probabilidades sobre una secuencia de tokens ¿qué\n",
            "[09:37.500 --> 09:46.460]  significa esto? significa que por ejemplo si decimos acá el ratón se comió el queso, esto tiene una\n",
            "[09:46.460 --> 09:54.560]  probabilidad de suceder ¿no? de ocurrencia de 0,02 excelente ahora si decimos el queso se comió el\n",
            "[09:54.560 --> 10:05.480]  ratón y no tiene una una probabilidad de ocurrencia menor 0,01 y si tenemos ratón el queso comió una\n",
            "[10:05.480 --> 10:13.880]  probabilidad de ocurrencia bajísima ¿no? y ¿cómo es que se calcula esta probabilidad de que pase?\n",
            "[10:13.880 --> 10:21.840]  bueno esto significa que el modelo que estamos utilizando tiene habilidades lingüísticas pero\n",
            "[10:21.840 --> 10:26.520]  también tiene conocimiento del mundo, por ejemplo eso significa que gramáticamente quizás estas dos\n",
            "[10:26.520 --> 10:34.600]  oraciones están correctas, son correctas, pero no se usan normalmente, nunca pasaría algo así o es\n",
            "[10:34.600 --> 10:39.560]  poco probable, es menos probable, la mitad de probable que suceda esta segunda situación comparada\n",
            "[10:39.560 --> 10:44.240]  con la primera situación. Entonces lo que podemos decir es que estos modelos de lenguaje tienen dos\n",
            "[10:44.240 --> 10:49.480]  cosas, en general tienen conocimiento sintáctico de gramática y también tienen conocimiento de\n",
            "[10:49.480 --> 10:53.140]  el mundo. ¿Cómo es que tienen conocimiento del mundo? Bueno, porque por ejemplo para\n",
            "[10:53.140 --> 10:57.460]  entrenar estas cosas le metemos la wikipedia entera y en español o en\n",
            "[10:57.460 --> 11:00.420]  ingles o etcétera, le metemos la wikipedia entera y ahí tiene ejemplos\n",
            "[11:00.420 --> 11:06.420]  donde se leyó este tipo de frases. Entonces, por ejemplo, esto ocurrió,\n",
            "[11:06.420 --> 11:09.780]  tuvo dos ocurrencias, esto tuvo una ocurrencia, listo. Entonces el doble de\n",
            "[11:09.780 --> 11:13.140]  probable que una cosa así, más o menos.\n",
            "[11:13.140 --> 11:17.460]  Y etcétera. Está explicado acá. No me voy a super meter con este tema de cómo es\n",
            "[11:17.460 --> 11:22.140]  porque la verdad que lo hacen muy bien en este curso, así que se los recomiendo. Acá pueden\n",
            "[11:22.140 --> 11:29.660]  ver las, cuáles son las capacidades que pueden tener también con gpt3, etcétera y van viendo\n",
            "[11:29.660 --> 11:34.500]  todas estas cosas. Así que no me voy a meter mucho con esto. También se explica la perplexity,\n",
            "[11:34.500 --> 11:42.700]  que es una métrica que se usa para saber qué tan bueno es el modelo, ¿no? ¿Cómo se entrenan\n",
            "[11:42.700 --> 11:46.860]  entonces estos modelos de lenguaje? ¿Qué es lo que hacen? Ya dijimos, devuelve una probabilidad\n",
            "[11:46.860 --> 11:51.980]  de una secuencia de tokens, bueno para lo que se entrenan estos modelos son modelos generativos,\n",
            "[11:51.980 --> 11:56.620]  entonces lo que significa es que se le normalmente lo que pasa es se le da una secuencia de tokens,\n",
            "[11:56.620 --> 12:03.300]  ponele otra vez el ratón se comió el, el qué, bueno y tiene que predecir en base a esos tres\n",
            "[12:03.300 --> 12:09.820]  tokens, cuatro tokens que tenía antes, el ratón se comió, bueno tiene que predecir cuál es la\n",
            "[12:09.820 --> 12:14.980]  próxima, el próximo token que va a venir después de eso, y eso dice el queso con una alta probabilidad\n",
            "[12:14.980 --> 12:21.660]  y así sucesivamente. Lo vimos en otros vídeos, también los voy a poner en el chat, pero básicamente\n",
            "[12:21.660 --> 12:27.340]  también hay una cosa que se llama ngrams ¿no? Entonces quizás no necesita simplemente por\n",
            "[12:27.340 --> 12:33.620]  ejemplo predecir un token, sino quizás va a predecir varios tokens ¿no? Entonces con esto\n",
            "[12:33.620 --> 12:40.060]  se pueden generar cosas que son más plausibles ¿no? que son más probables de suceder. Dos tokens,\n",
            "[12:40.060 --> 12:47.860]  tres tokens y hay formas distintas de elegir estos tokens, no? greedy search, beam, etcétera.\n",
            "[12:49.460 --> 12:54.580]  Acá pueden ver que cuando se entrenan estos modelos una cosa que se puede hacer es,\n",
            "[12:54.580 --> 13:03.900]  se puede escribir y decirle predecime el próximo token y la forma que se hace esto es, porque yo\n",
            "[13:03.900 --> 13:09.220]  ya tengo la wikipedia, no? entonces yo simplemente lo que puedo hacer es corto la oración y digo el\n",
            "[13:09.220 --> 13:14.920]  token que falta es este, entonces el modelo aprende muy bien porque dice yo predije chorizo,\n",
            "[13:14.920 --> 13:20.920]  no hermano chorizo no es, queso, queso, no tengo la respuesta acá, queso, no, no, está mal, arreglalo.\n",
            "[13:20.920 --> 13:25.560]  Entonces ahí va aprendiendo a medida que lo vamos entrenando. Otra forma de hacerlo en vez de predecir\n",
            "[13:25.560 --> 13:30.960]  el próximo token es cuando le damos un espacio en blanco que es lo que tenemos acá, es básicamente\n",
            "[13:30.960 --> 13:36.840]  lo que estamos diciendo es, mira llename el espacio en blanco, no significa que sea el último token\n",
            "[13:36.840 --> 13:42.720]  pero sí, quizás es un token intermedio. Esto es lo que lo que hace el modelo. Bueno,\n",
            "[13:44.240 --> 13:51.600]  estos son modelos de lenguaje de generación, pero no son modelos gigantes de lenguaje. Vamos a ver\n",
            "[13:51.600 --> 13:57.880]  qué son los modelos gigantes de lenguaje. Bueno, estos son modelos que están entrenados con una\n",
            "[13:57.880 --> 14:02.840]  cantidad estúpida de data y que pueden hacer, que fueron adaptados para hacer una cantidad\n",
            "[14:02.840 --> 14:09.840]  estúpida de tasks básicamente ya que se llaman lo que sería cero shot significa que nosotros el día\n",
            "[14:09.840 --> 14:14.600]  de mañana le preguntamos bueno escribir una canción de cumbia pero nunca se entrenó para escribir una\n",
            "[14:14.600 --> 14:19.880]  canción de cumbia pero te lo puede hacer igual pero si fuera one shot por ejemplo es que tiene\n",
            "[14:19.880 --> 14:24.960]  un ejemplo cuando cuando lo estamos haciendo le damos un ejemplo y después le decimos ahora esto\n",
            "[14:24.960 --> 14:30.880]  es una canción de cumbia escribimos una canción nuevo one shot supervise learning sería que lo\n",
            "[14:30.880 --> 14:35.360]  entrenamos solamente con un dataset de canciones de cumbia y le decimos hacerme una nueva que tenga\n",
            "[14:35.360 --> 14:40.240]  este formato y es lo que está pasando con estos modelos de lenguaje que tenemos acá\n",
            "[14:42.280 --> 14:48.360]  les está yendo mejor que los humanos en muchas cosas la línea gris que tienen acá son los humanos\n",
            "[14:48.360 --> 14:53.840]  y pueden ver que ya los estamos pasando o sea hubo un momento donde los modelos ya empezaron\n",
            "[14:53.840 --> 15:02.840]  a ser mejores que humanos. Acá pueden ver la performance en 58 tasks y el tema que tienen\n",
            "[15:02.840 --> 15:10.160]  es que esto se llama, acá abajo pueden ver lo que son, aparecen los parámetros y arriba están\n",
            "[15:10.160 --> 15:15.280]  viendo la performance. Los parámetros de abajo, lo que se dieron cuenta es que cuando subíamos los\n",
            "[15:15.280 --> 15:19.760]  parámetros, que esto fue lo que pasó con gpt3, metieron una cantidad estúpida de parámetros,\n",
            "[15:19.760 --> 15:25.920]  empezaron a notar una cosa que se llama emergencia, no emergencia de está pasando algo, se está\n",
            "[15:25.920 --> 15:31.240]  prendiendo fuego la casa, sino de habilidades emergentes de estos modelos y esto al día de hoy\n",
            "[15:31.240 --> 15:37.120]  no se sabe por qué sucede. Hay aproximaciones pero no se sabe todavía qué es. Conciencia no lo sabemos\n",
            "[15:37.120 --> 15:42.920]  todavía, pero lo que sí se sabe es que cuando se llega a una cantidad de parámetros muy grande\n",
            "[15:42.920 --> 15:49.600]  estos modelos empiezan a ser muchísimo mejor que antes, o sea ya el aumento de performance no\n",
            "[15:49.600 --> 15:54.880]  es linear sino que se vuelve exponencial que es lo que vemos acá, acá pueden ver cómo pega un salto\n",
            "[15:54.880 --> 15:59.520]  del triple de performance cuando empezamos a aumentar la cantidad de parámetros.\n",
            "[16:02.920 --> 16:06.480]  Acá podemos ver cómo es la evolución a través de los años, teníamos estadística,\n",
            "[16:06.480 --> 16:09.120]  teníamos Machine Learning, teníamos Deep Learning y ahora tenemos lo que se llama\n",
            "[16:09.120 --> 16:15.600]  Foundation Models. ¿Qué significan estos Foundation Models? Bueno son modelos gigantes,\n",
            "[16:15.600 --> 16:22.640]  podemos decir que es una, no es en este caso, pero podemos decir que estamos yendo en dirección a una\n",
            "[16:22.640 --> 16:26.840]  inteligencia muy grande, digamos, que le podemos preguntar sobre cualquier cosa y nos pueda responder\n",
            "[16:26.840 --> 16:34.200]  sobre esas cosas. Estos son los datas que se usan, pueden ser de cualquier cosa, libros, código,\n",
            "[16:34.200 --> 16:42.640]  reddit, sitios médicos, sitios, o sea, papers de medicina, patentes, etcétera, o sea, todo lo que\n",
            "[16:42.640 --> 16:48.720]  tengamos generado como humanidad, es lo que le podemos meter a estos modelos como training data\n",
            "[16:48.720 --> 16:53.560]  y estas son las cosas que pueden hacer estos modelos. Pueden sumarizar, pueden generar\n",
            "[16:53.560 --> 17:00.960]  imágenes, pueden clasificar el texto, traducir, chatear, crear recetas, explicar código, muchas\n",
            "[17:00.960 --> 17:07.360]  muchas cosas. Y acá empezamos a ver un poco qué fueron las cosas que hicieron que esta revolución\n",
            "[17:07.360 --> 17:14.800]  pueda suceder. Primero, la arquitectura de Transformers que se hizo en el año, o sea,\n",
            "[17:14.800 --> 17:20.920]  el paper de Attention Is All You Need, se hizo en el año 2017 y todas las personas que hicieron\n",
            "[17:20.920 --> 17:26.800]  ese paper, excepto una persona, se fueron de Google, lo hizo Google ese paper, y se fueron\n",
            "[17:26.800 --> 17:33.240]  a hacer sus startups o a OpenAI, etcétera. Aquí pueden ver lo importante que fue eso.\n",
            "[17:33.240 --> 17:40.400]  acá tenemos lo que es la evolución de la cantidad de cómputo que se utilizó para\n",
            "[17:40.400 --> 17:43.440]  entrenar estos modelos, o sea, está pegando una exponencial a medida que nos vamos acercando,\n",
            "[17:43.440 --> 17:47.760]  eso quiere decir que entrenar estos modelos sale millones de dólares y podemos ver acá,\n",
            "[17:47.760 --> 17:53.600]  bueno, quién domina la risa, etcétera. O sea, esto es básicamente lo que estoy diciendo,\n",
            "[17:53.600 --> 17:58.640]  que las grandes empresas son las que tienen dominio sobre estas cosas y eso no es bueno,\n",
            "[17:58.640 --> 18:04.480]  o sea, debería estar bastante distribuido. Industria, comunidad open source, gobiernos,\n",
            "[18:04.480 --> 18:12.280]  estado. Tiene que estar distribuido por lo menos de manera igualitaria. Estos son los modelos que\n",
            "[18:12.280 --> 18:17.560]  tenemos disponibles al día de hoy, o sea, pueden ver que estos modelos gigantes, acá tenemos los\n",
            "[18:17.560 --> 18:23.960]  parámetros 540.000, 130.000, 80.000, 20.000, etcétera, 180.000, etcétera. Pueden ver que las\n",
            "[18:23.960 --> 18:31.520]  empresas que lo hacen son Google, Google con DeepMind, Facebook, Meta, Microsoft, OpenAI y Big\n",
            "[18:31.520 --> 18:37.480]  Science que es un modelo abierto y después AI21 es una empresa israelí que está haciendo\n",
            "[18:37.480 --> 18:45.880]  esto. Bueno y etcétera, mapas y so on. Una cosa que es interesante, esto es muy bueno, se lo\n",
            "[18:45.880 --> 18:51.960]  recomiendo para verlo, se llama el estado de AI, reporte 2022, está hecho por Nathan B.\n",
            "[18:51.960 --> 19:00.040]  Van Eyck y Ian Hogarth. Ambos dos están en lo que es Venture Capital y la verdad que hacen un análisis\n",
            "[19:00.040 --> 19:04.200]  excelente. Hace varios años lo vienen haciendo. Pueden ver acá abajo el 2018 que lo vienen haciendo\n",
            "[19:04.200 --> 19:12.240]  y es espectacular. O sea, no hay resumen mejor en internet que el que hacen estas personas. Bueno,\n",
            "[19:12.240 --> 19:19.760]  ¿qué es lo que hicieron acá? Acá podemos ver que están charlando un poco de esto de dónde estamos\n",
            "[19:19.760 --> 19:24.160]  en este momento bueno cinco años después de que haya salido el transformers todavía sigue siendo\n",
            "[19:24.160 --> 19:28.520]  la mejor arquitectura siempre sale alguna alguna cosa nueva que trata de modificar esto lo otro\n",
            "[19:28.520 --> 19:32.520]  lo otro etcétera pero la verdad es que no hay vuelta que darle es la mejor arquitectura es la\n",
            "[19:32.520 --> 19:39.680]  que mejor escala la que mejor genera este tipo de cosas digamos y al final de cuentas lo que se hace\n",
            "[19:39.680 --> 19:45.880]  es simplemente se entrena por más tiempo y se tienen mejor resultado porque más o menos no\n",
            "[19:45.880 --> 19:53.520]  sabemos, pero generalmente lo que pasa es eso, así que por qué no, en vez de pasar años, por qué no\n",
            "[19:53.520 --> 19:58.560]  entrenamos directamente y vemos qué onda. Bueno, así es como está la research en machine learning en el\n",
            "[19:58.560 --> 20:08.520]  día de hoy. Pueden ver acá varias varias estas cosas, esto es lo que decíamos del tema de la\n",
            "[20:08.520 --> 20:15.200]  emergencia, o sea está esto emergence y pueden ver acá como a medida que llegamos a un nivel de\n",
            "[20:15.200 --> 20:21.040]  compute pega una disparada espectacular o sea es increíble cómo los modelos mejoran muchísimo se\n",
            "[20:21.040 --> 20:26.240]  van al triple al cuádruple de performance cuando hacen esto así que vamos a esperar que por ejemplo\n",
            "[20:26.240 --> 20:32.840]  exagerando no gpt4 pueda tener alguna de estas características también así que estuvimos\n",
            "[20:32.840 --> 20:38.960]  hablando de bueno hay otras cosas se lo recomiendo esto para ver esto pueden ver acá cuando apenas\n",
            "[20:38.960 --> 20:45.900]  salió gpt3 2020 pueden ver cómo pasó un tiempo se estaban todos digamos en pelota hasta que más o\n",
            "[20:45.900 --> 20:52.220]  menos acá empezaron a salir modelos abiertos gpt, jota, el oiter.ai y ahora van a ver por qué es\n",
            "[20:52.220 --> 21:00.540]  importante el oiter.ai que laburaron con lion, carper.ai, stability.ai, todos parte del mismo\n",
            "[21:00.540 --> 21:07.820]  conjunto para hacer open source a toda esta ciencia es increíble el laburo que hacen. Bueno\n",
            "[21:07.820 --> 21:13.940]  Pueden ver cómo pasó por lo menos un año, casi, hasta que empezaron a salir todas estas\n",
            "[21:13.940 --> 21:21.100]  versiones open source, ¿no? Así que, así como salió open chat gpt ahora, podemos darle unos\n",
            "[21:21.100 --> 21:25.260]  meses, quizás un año, y vamos a empezar a tener esas cosas. Lo mismo con DALI.\n",
            "[21:26.900 --> 21:30.620]  Un poquito más difícil es toda la parte de alfafolia, etcétera. Bueno,\n",
            "[21:30.620 --> 21:41.900]  Con esto, lo único que quería mostrar, quizás acá interesante, es que, bueno, para entrenar esto,\n",
            "[21:41.900 --> 21:47.100]  se necesitan muchas GPUs, así que estas son las GPUs que están disponibles, las tienen muy pocas\n",
            "[21:47.100 --> 21:52.700]  empresas en el mundo, acá pueden tenerlo como, lo que está en naranja son laboratorios nacionales,\n",
            "[21:52.700 --> 21:58.540]  generalmente están en Estados Unidos o en Europa, lo que está en azul son las empresas particulares\n",
            "[21:58.540 --> 22:07.260]  y lo que está en rojo en rojo son digamos Amazon por ejemplo, pero acá, esto es el paper original,\n",
            "[22:07.260 --> 22:13.700]  les quería mostrar, attention is all you need, pedazo de paper se armaron estos tipos, bueno esta\n",
            "[22:13.700 --> 22:19.540]  gente que está acá pueden ver acá las empresas que salieron y fundaron, todas estas multimillonarias\n",
            "[22:19.540 --> 22:27.740]  lo pueden ver acá, unicorn en algunos casos de lo que hicieron, bueno hablamos de transformer,\n",
            "[22:27.740 --> 22:34.780]  hablamos de OpenAI y vamos a ver qué es esto de GPT. ¿Qué es GPT? G, la G, la P y la T. La G,\n",
            "[22:34.780 --> 22:44.220]  generativo. P, pre-trained, T, transformers. ¿Qué es entonces GPT-3? Es básicamente un modelo de\n",
            "[22:44.220 --> 22:49.620]  transformers que se usa para generar texto y que fue entrenado con una cantidad de data gigante.\n",
            "[22:49.620 --> 22:59.940]  eso es eso es a grandes rasgos lo que es GPT-3. Cuando hablamos de un modelo ¿no? o sea ¿qué\n",
            "[22:59.940 --> 23:07.260]  modelo es el que estamos hablando? Bueno vamos a ir acá y vamos a ver esto que hizo André Carpati.\n",
            "[23:07.260 --> 23:14.140]  André Carpati es un tipo que era el director de Machine Learning, o sea el director de OpenAI\n",
            "[23:14.140 --> 23:19.940]  cuando apenas empezó que Elon Musk le había puesto plata, si se acuerdan era le había puesto mil\n",
            "[23:19.940 --> 23:26.300]  millones de dólares más o menos para arrancar la idea de OpenAI era que sea abierto. Entonces había\n",
            "[23:26.300 --> 23:33.340]  cuando arrancó Karpati como director de AI después de ahí se fue a ser director de AI en Tesla,\n",
            "[23:33.340 --> 23:39.700]  fue la persona que fue responsable de toda la parte de Autonomous Driving de Tesla y hace\n",
            "[23:39.700 --> 23:45.460]  unos meses pinchó las bolas dijo yo no quiero más estar con esto quiero hacer vídeo de youtube quiero\n",
            "[23:45.460 --> 23:51.340]  hacer código quiero pasarla bien tocar la guitarra etcétera y gracias a dios porque entre las cosas\n",
            "[23:51.340 --> 23:59.500]  que hizo es por ejemplo esta implementación de gpt no se mandó lo podemos ver a andré capo capo\n",
            "[23:59.500 --> 24:05.260]  andré bueno acá pueden ver las implementaciones actuales son gigantes gigante súper larga\n",
            "[24:05.260 --> 24:11.260]  imposible de correr, etcétera. Este tipo dijo, acá hay que hacer una que se llame nano gpt, que\n",
            "[24:11.260 --> 24:17.300]  sean 300 líneas de código, ¿no? que tenga, que sean un par de archivos nada más, que es este,\n",
            "[24:17.300 --> 24:23.300]  modelo.py y train.py, ya está, dos archivos, nada más, el resto son como para ayudar a hacer esto,\n",
            "[24:23.300 --> 24:30.060]  pero dos archivos y con eso ya está, ya con eso podés tener gpt2, que es lo que tenemos acá.\n",
            "[24:30.060 --> 24:36.680]  Entonces, usando este código, básicamente ya podemos entrenar GPT-2. Acá en este caso,\n",
            "[24:36.680 --> 24:44.760]  este tipo usa un dataset que se llama OpenWebText y esto es lo que usa para entrenar el modelo que\n",
            "[24:44.760 --> 24:56.320]  vemos acá. Lo entrena con una sola instancia de 8 GPUs de NVIDIA a 100 por 38 horas. Si se acuerdan,\n",
            "[24:56.320 --> 25:06.160]  vamos a ir acá cloud si ven acá dijimos que esa es ésta no es una p4d sale 32 mango si ponemos\n",
            "[25:06.160 --> 25:14.880]  32 mango por 38 nos da 1200 dólares bueno eso quiere decir que si agarramos esto de carpati\n",
            "[25:14.880 --> 25:22.680]  con 1200 dólares nos podemos hacer nuestro propio gpt2 al mismo nivel misma calidad mismo todo\n",
            "[25:22.680 --> 25:29.800]  espectacular gpt3 es muy distinto de gpt2 no es lo mismo en algunas optimizaciones nada más pero\n",
            "[25:29.800 --> 25:34.840]  está basado en la misma arquitectura obviamente acuérdense gpt3 no sale o sea no tenemos el código\n",
            "[25:34.840 --> 25:40.000]  no sabemos exactamente cómo fue un paper sabemos más o menos que hicieron pero el código es\n",
            "[25:40.000 --> 25:46.600]  propietario de OpenAI así que esta es una versión abierta si por ejemplo nosotros fuéramos el\n",
            "[25:46.600 --> 25:52.480]  gobierno de país x de latinoamérica y quisiéramos tener nuestro propio modelo esto puede ser una\n",
            "[25:52.480 --> 25:58.360]  forma de hacerlo ¿no? y hablando de esos propios modelos, si vemos acá plan,\n",
            "[26:00.360 --> 26:05.520]  hay un plan de tecnologías de lenguaje del gobierno de España donde usan esta supercomputadora que\n",
            "[26:05.520 --> 26:11.800]  está en una iglesia en Barcelona, se llama María y básicamente lo que hacen es exactamente esto\n",
            "[26:11.800 --> 26:17.600]  que estamos hablando, entrenan su modelo de gpt2, lo hacen libre, disponible, gpt2 base, ves?\n",
            "[26:17.600 --> 26:27.020]  entrenan su modelo gpt2 entrenado con información de la librería nacional de España, lo tenemos\n",
            "[26:27.020 --> 26:32.940]  acá a la derecha y este modelo lo hacen disponible de forma gratuita para la comunidad porque porque\n",
            "[26:32.940 --> 26:41.420]  dicen loco si nosotros queremos estar compitiendo con con Estados Unidos con UK con Italia con China\n",
            "[26:41.420 --> 26:46.500]  con Rusia con etcétera tenemos que tener nuestras cosas y por eso me parece importante hablar de\n",
            "[26:46.500 --> 26:51.140]  esto, ponerlo ahí afuera y que se comparta. Así que acá pueden verlo que tienen primero las primeras\n",
            "[26:51.140 --> 26:58.300]  las versiones genéricas de gpt2 pero también tenemos Roberta para por ejemplo información\n",
            "[26:58.300 --> 27:03.540]  financiera, información médica y etcétera. Esto es longformer, es cuando tenemos secuencias de\n",
            "[27:03.540 --> 27:10.260]  texto larguísimas y queremos trabajar etcétera. Entonces si vos sos por ejemplo una startup que\n",
            "[27:10.260 --> 27:19.300]  quiere hacer un producto nuevo, puede consumir esto ¿no? vos querés hacer, querés trabajar con\n",
            "[27:19.300 --> 27:25.700]  documentos de textos gigantes, bueno entonces tú usas este modelo como base, después entrenas el\n",
            "[27:25.700 --> 27:30.180]  tuyo encima y listo, ya tenés tu startup, por ejemplo, que va a ser nueva, entonces este son\n",
            "[27:30.180 --> 27:37.260]  el tipo de cosas que hay que saberlo para evitar esta divergencia entre Silicon Valley y digamos\n",
            "[27:37.260 --> 27:44.620]  Latinoamérica, España y Euroamérica vamos a ponerlo, ¿no? Entonces, vimos Carpati.\n",
            "[27:46.140 --> 27:53.540]  ¿Qué está haciendo André? Bueno, está haciendo esto, pero está, o sea, esto es nosotros\n",
            "[27:53.540 --> 27:59.220]  metiéndonos en el repo de él, o sea, en el github de André, todavía ni siquiera hizo el release.\n",
            "[27:59.220 --> 28:05.340]  Básicamente lo que está haciendo es, está haciendo un curso de YouTube que es espectacular,\n",
            "[28:05.340 --> 28:11.180]  donde va paso a paso haciendo estas cosas cada uno fíjense es un curso de una hora y cuarto una hora\n",
            "[28:11.180 --> 28:19.260]  etcétera es muy bueno y está dividido por partes no va pasa de algo muy pequeño explicando qué son\n",
            "[28:19.260 --> 28:24.140]  las redes numerales y el backpropagation y después se va a ir bien bien bien bien para\n",
            "[28:24.140 --> 28:29.900]  arriba explicando que es nano gpt o sea que son los gpt models o sea el tipo está yendo de cero\n",
            "[28:29.900 --> 28:37.220]  a el conocimiento que tiene un ingeniero de OpenAI, por ejemplo, y lo hace de forma gratuita\n",
            "[28:37.220 --> 28:41.700]  para la comunidad. Acá podemos ver cómo está charlando de las scaling loss, etcétera, ¿no?\n",
            "[28:41.700 --> 28:48.660]  Y vamos a ver un poco qué son esas scaling loss también en algún momento. No hay problema. Bueno,\n",
            "[28:48.660 --> 29:02.340]  vimos que son las LLMs y que es GPT-3, vimos el mercado y vamos a ver ahora básicamente muy\n",
            "[29:02.340 --> 29:09.260]  rápidamente qué es esto de Sparrow, de DeepMind y ChatGPT, bueno para eso nos vamos a ir a este\n",
            "[29:09.260 --> 29:13.940]  vídeo que hizo Letizia, Letizia tiene un canal de YouTube excelente se llama Miss Coffee Bean,\n",
            "[29:13.940 --> 29:18.940]  Miss Coffee Bean es esto que tienen acá abajo a la izquierda y explica un montón de temas.\n",
            "[29:18.940 --> 29:27.180]  Es una, está en academia Leticia y hace resúmenes muy buenos pero también se mete mucho en los\n",
            "[29:27.180 --> 29:31.460]  detalles técnicos así que esas son cosas que a mí por lo menos me gustan mucho porque generalmente\n",
            "[29:31.460 --> 29:39.380]  faltan. En este caso vamos a ver qué son las, qué es lo que está haciendo DeepMind y qué es\n",
            "[29:39.380 --> 29:45.140]  lo que es que cómo se compara esto con chat gpt no primero que nada vamos a ver qué era chat gpt no\n",
            "[29:45.140 --> 29:51.780]  lo vimos lo salteamos estamos minuto 30 más o menos y no vimos que era. Chat gpt es esto, ya hay un vídeo\n",
            "[29:51.780 --> 29:55.660]  yo lo puse en la descripción es un vídeo que explica más o menos qué es toda la gente está\n",
            "[29:55.660 --> 30:00.580]  hablando de esto supongo que si estás viendo este vídeo es porque te interesa en el pi, te interesa\n",
            "[30:00.580 --> 30:06.060]  los modelos de lenguaje, sabes lo que es chat gpt y te interesaría aprender un poquito más, esas son\n",
            "[30:06.060 --> 30:13.100]  mis assumptions de todo esto. Pueden ver acá cómo yo me metí, escribí una prompt, escribí un script\n",
            "[30:13.100 --> 30:17.780]  para youtube sobre un mini curso que estoy armando para poder entrenar desde bla bla bla. Pueden ver\n",
            "[30:17.780 --> 30:22.300]  acá cómo me responde chatgpt, me hace un índice del contenido, me escribe una conclusión y me\n",
            "[30:22.300 --> 30:27.300]  escribe una intro también. Y después yo le digo, loco, ¿por qué no me haces una lista de 10 títulos\n",
            "[30:27.300 --> 30:31.580]  que puedo poner a ese? Me aseguro que es la mayor cantidad de visitas, esto para joder nada más,\n",
            "[30:31.580 --> 30:37.980]  lo pongo y acá pueden ver lo que me escribe ¿no? cómo entrenar desde cero etcétera y bueno yo lo\n",
            "[30:37.980 --> 30:42.660]  cambié un poco el título pero más o menos eso es lo que nos está diciendo y después le digo\n",
            "[30:42.660 --> 30:46.380]  hacemos una descripción corta de todo lo que hablamos ¿no? o sea toda esta conversación que\n",
            "[30:46.380 --> 30:51.980]  tenemos se la acuerda ¿por qué? porque tiene una ventana que se dice que es de 8.096 tokens,\n",
            "[30:51.980 --> 30:57.820]  es una ventana bastante larga y se acuerda de estas cosas, si yo sigo escribiendo en un momento\n",
            "[30:57.820 --> 31:06.700]  se va a olvidar de estas cosas porque tiene hasta 8.000 tokens. ¿Qué es entonces OpenAI? Esto de\n",
            "[31:06.700 --> 31:12.580]  chat-gpt es un modelo que lo que hace básicamente y obviamente no hay un paper que nos diga\n",
            "[31:12.580 --> 31:17.020]  simplemente usalo y lo que nos está diciendo, no hay un paper que nos diga pero la forma en\n",
            "[31:17.020 --> 31:24.220]  la que funciona es tenemos un modelo de generación de lenguaje que es gpt3 simple y después lo que\n",
            "[31:24.220 --> 31:29.340]  hicieron es entrenaron una política donde dicen\n",
            "[31:29.340 --> 31:33.340]  este tipo de, o sea entrenaron un modelo básicamente que dice y a mí me gusta más\n",
            "[31:33.340 --> 31:37.420]  este tipo de respuestas más que este otro tipo de respuestas, o sea si vos le\n",
            "[31:37.420 --> 31:42.260]  contestas me puedes escribir una lista de 10 de 10 títulos para cosa y el\n",
            "[31:42.260 --> 31:46.340]  chatbot te contesta pero que me venía a hablar acá, bueno si te contesta así\n",
            "[31:46.340 --> 31:50.940]  de mal es como muy malo, generalmente un chatbot o digamos un agente de\n",
            "[31:50.940 --> 31:56.420]  machine learning tiene que tener tres cosas, tiene que ser útil, o sea tiene que dar respuestas, no\n",
            "[31:56.420 --> 32:01.260]  puede estar contestando todo el tiempo, no lo sé, no sé la respuesta, pregúntame de nuevo, no lo sé,\n",
            "[32:01.260 --> 32:05.860]  no lo sé, no puede ser así, tiene que ser útil, tiene que ser honesto, eso quiere decir que los\n",
            "[32:05.860 --> 32:12.420]  modelos estos generalmente alucinan las respuestas, o sea se las inventan, inventan respuestas que son\n",
            "[32:12.420 --> 32:19.540]  gramáticamente correctas pero son incorrectas fácticamente, si yo a esto le digo de repente,\n",
            "[32:19.540 --> 32:24.180]  a esto lo puedo convencer de algo. No, en realidad no es tal cosa, es tal otra cosa y se lo va a\n",
            "[32:24.180 --> 32:28.860]  creer el modelo. Así que me va a contestar con eso, o sea que no es honesto, está alucinando y está\n",
            "[32:28.860 --> 32:36.860]  mintiendo básicamente. No mintiendo, está dando información falsa. Bueno entonces lo que se hace\n",
            "[32:36.860 --> 32:40.500]  es se crea un modelo que lo que hace es esta clasificación de, mira me gusta más la respuesta\n",
            "[32:40.500 --> 32:44.820]  D, después la C, después la A, después la B y ese es el modelo que tenemos. Entonces una vez que\n",
            "[32:44.820 --> 32:50.940]  tenemos ese modelo, se entrena usando un algoritmo de reinforcement learning, se entrena una política\n",
            "[32:50.940 --> 32:55.700]  de optimización que dice mira andate más para el lado de este tipo de respuestas que son mejor,\n",
            "[32:55.700 --> 33:01.980]  o sea no diga como se llama si te hace una pregunta no le conteste callate troll ponele\n",
            "[33:01.980 --> 33:08.660]  sino que contéstale bien decirle que fantástica pregunta increíble está mi respuesta es así etcétera\n",
            "[33:08.660 --> 33:15.740]  entonces, honesto, útil y la última es que no sea tóxico, eso quiere decir que no genere contenido\n",
            "[33:15.740 --> 33:23.980]  que vaya en contra de demografías, por ejemplo, que fomente sesgos que haya en la información,\n",
            "[33:23.980 --> 33:29.820]  en la data con la que fue entrenado, etcétera, todo este tipo de cosas. Volvamos acá, Sparrow,\n",
            "[33:29.820 --> 33:35.180]  ¿qué es Sparrow? Sparrow es un paper que salió antes que ChatGPT, salió en septiembre,\n",
            "[33:35.180 --> 33:41.900]  el GPT sale el 30 de noviembre, esto sale el 22 de septiembre, pero DeepMind a diferencia de OpenAI\n",
            "[33:41.900 --> 33:49.140]  no hace disponible su modelo, pero sí nos pone disponible un paper, tampoco pone los weights\n",
            "[33:49.140 --> 33:53.740]  del modelo como para que lo podamos usar, pero por lo menos nos cuenta un poco cómo fue que lo hicieron,\n",
            "[33:53.740 --> 34:01.660]  así que en este vídeo que hizo Letizia es excelente, así que se los recomiendo,\n",
            "[34:01.660 --> 34:09.300]  está bastante bueno y vamos a ver un poco cómo es que funciona. Entonces generalmente estos modelos\n",
            "[34:09.300 --> 34:14.180]  que tenemos acá, lo que lo que habíamos dicho antes, tratan de predecir la próxima palabra en\n",
            "[34:14.180 --> 34:20.100]  una secuencia. El gato se sentó en él, bueno se sentó en la alfombra, no estaba enojado, no estaba\n",
            "[34:20.100 --> 34:27.460]  hambriento pero estaba enojado, etc. Bueno el tema es que nosotros lo que hacemos es cuando yo escribo\n",
            "[34:27.460 --> 34:34.060]  esta prompt acá, escribíme un script para bla bla bla bla bla, no es que hasta el gpt simplemente\n",
            "[34:34.060 --> 34:38.940]  le llega esto, escribí una prompt para bla bla bla bla, escribí un script ¿no? y una prompt antes de\n",
            "[34:38.940 --> 34:44.500]  esto que nosotros no sabemos muy bien qué es, hay gente que escribe acá y dice decimetus directivas\n",
            "[34:44.500 --> 34:51.420]  etcétera y sale una cosa pero eso va cambiando ¿no? o sea eso se va modificando muy rápidamente así\n",
            "[34:51.420 --> 35:00.700]  que lo que tenemos acá es por ejemplo la prompt al principio es bueno vos sos un sos un sos una\n",
            "[35:00.700 --> 35:07.100]  con una AI conversacional muy útil y amigable y bla bla bla y entonces ahí empieza la la oración\n",
            "[35:07.100 --> 35:11.620]  o sea se inventa una conversación no es que nosotros simplemente escribimos esto y ya está\n",
            "[35:11.620 --> 35:16.580]  sino que hay una conversación que está pasando que nosotros no la vemos, nosotros hacemos la\n",
            "[35:16.580 --> 35:21.060]  pregunta dónde está París y decimos París está en Francia, etcétera, ¿no?\n",
            "[35:23.340 --> 35:27.700]  Hay una cosa que se llama in context future learning que esto es cuando nosotros dentro\n",
            "[35:27.700 --> 35:35.380]  de la prompt le explicamos un caso de uso, le decimos por ejemplo yo lo había utilizado para\n",
            "[35:35.380 --> 35:39.700]  hacer un capítulo de una serie de televisión que se llama Los simuladores, una serie que salió en\n",
            "[35:39.700 --> 35:45.500]  Argentina, entonces yo le digo Los simuladores es una serie que, pum, y copy y pego la sinopsis de lo\n",
            "[35:45.500 --> 35:49.380]  que es la serie y después digo hacemos un capítulo nuevo, entonces ahí yo le dije a\n",
            "[35:49.380 --> 35:53.380]  chatGPT que era los simuladores, si yo le escribo hacemos un capítulo nuevo de los simuladores no\n",
            "[35:53.380 --> 35:58.500]  sabe que son los simuladores, ahora si yo quiero escribir un estilo por ejemplo, puedo decir mira\n",
            "[35:58.500 --> 36:03.540]  una canción de cumbia es así, escribí una nueva canción de cumbia y ahí te puede contestar, entonces\n",
            "[36:03.540 --> 36:13.660]  eso se llamaría in context visual learning. Entonces vamos a ver un poco qué es lo que hizo\n",
            "[36:13.660 --> 36:18.460]  DeepMind que a todo esto no le llama chatbot a las cosas sino que le llama agente de diálogo o\n",
            "[36:18.460 --> 36:23.540]  Conversational AI ¿por qué? porque la palabra chatbot la verdad que no le gustaba a la gente\n",
            "[36:23.540 --> 36:28.620]  usarla porque como que nunca anduvieron, no andaban bien nunca los chatbots ¿no? hasta el día hasta\n",
            "[36:28.620 --> 36:34.220]  que saca chat gpt saca esta versión pero antes de eso como que no andaban tan bien entonces ¿cómo\n",
            "[36:34.220 --> 36:41.500]  es que funciona? bueno están tomando un modelo de lenguaje que se llama chinchilla ¿no? por alguna\n",
            "[36:41.500 --> 36:47.500]  razón DeepMind eligió ponerle nombres de animales a todos sus últimos modelos de lenguaje de\n",
            "[36:47.500 --> 36:52.700]  Transformers, entonces tenés gato, chinchilla, gopher, etcétera. Simplemente para que sepan.\n",
            "[36:52.700 --> 36:59.580]  Bueno, ¿qué es lo que hace? Tenemos esto, tenemos reglas, tenemos human feedback, tenemos el modelo\n",
            "[36:59.580 --> 37:05.340]  de lenguaje y tenemos clasificadores. ¿Qué significa todo esto? Bueno, vamos a ir para acá y lo vamos\n",
            "[37:05.340 --> 37:11.700]  a ver acá en este paper. Esto significa que tenemos el modelo y después tenemos dos cosas que estamos\n",
            "[37:11.700 --> 37:16.940]  haciendo. Cuando decimos estas dos cosas, esto quiere decir que alguien se sentó, o sea, le pagaron\n",
            "[37:16.940 --> 37:23.180]  a gente para que se siente y diga, loco, de todas estas respuestas que se dieron, a mí me gusta más\n",
            "[37:23.180 --> 37:30.220]  la primera, la segunda me gusta menos, la tercera no me gusta para nada y después también sentaron\n",
            "[37:30.220 --> 37:38.620]  a la gente a preguntarle si la respuesta que dio Sparrow va en contra de reglas que ya setearon,\n",
            "[37:38.620 --> 37:45.740]  ¿no? ¿Qué significa reglas que setearon? Bueno, acá pueden ver cómo es la prompt,\n",
            "[37:45.740 --> 37:49.420]  ¿no? Que habíamos dicho antes que había una prompt antes. Pueden ver lo siguiente es una\n",
            "[37:49.420 --> 37:56.020]  conversación entre un asistente virtual que es y tiene mucho conocimiento, muy inteligente,\n",
            "[37:56.020 --> 38:02.980]  se llama Sparrow y un usuario humano que le vamos a llamar usuario y acá podemos ver cómo empieza a\n",
            "[38:02.980 --> 38:10.900]  tener esta conversación. Estos son los ejemplos que se usan para entrenar el modelo. Lo que tenemos\n",
            "[38:10.900 --> 38:20.860]  acá, a ver si lo encuentro, tiene un par de reglas que son por ejemplo que no acá dice que no tenga\n",
            "[38:20.860 --> 38:26.740]  estereotipos, que no haga micro agresiones, que no haga amenazas, que no haga agresiones sexuales,\n",
            "[38:26.740 --> 38:31.920]  que no ataque por identidad, que no insulte, etcétera. Estas son las reglas que estamos\n",
            "[38:31.920 --> 38:36.580]  seteando. Entonces, si volvemos acá, este modelo, ¿qué era lo que decía? Te decía,\n",
            "[38:36.580 --> 38:41.500]  ¿está violando alguna regla? Cuando está contestando, cuando dice es un choto, me está\n",
            "[38:41.500 --> 38:47.580]  insultando. Bueno, la regla que violó es la de insultar, por ejemplo. Entonces eso es lo que\n",
            "[38:47.580 --> 38:59.100]  tenemos acá. Y el otro, lo que hacemos es básicamente esta gente, elige esta respuesta,\n",
            "[38:59.100 --> 39:03.940]  cuál es la mejor respuesta y con eso entrenamos, una vez que tenemos estos dos modelos, tenemos\n",
            "[39:03.940 --> 39:08.620]  uno de preferencia y otro de reglas, lo que hacemos es entrenamos con reinforcement learning,\n",
            "[39:08.620 --> 39:16.500]  básicamente diciendo mira, quiero que le des mayor importancia, mayor relevancia a lo que este\n",
            "[39:16.500 --> 39:22.280]  modelo te haya dicho que es la mejor respuesta y quiero que le des menor importancia, o sea que\n",
            "[39:22.280 --> 39:27.480]  castigue, que penalice es bastante grave a los modelos que a las respuestas que te hayan dado\n",
            "[39:27.480 --> 39:32.280]  que insulten, por ejemplo, entonces en base a estas dos cosas esto ya nos dice que el modelo\n",
            "[39:32.280 --> 39:37.520]  este reinforcement learning ya nos está diciendo que cuáles son las respuestas, el tipo de respuesta\n",
            "[39:37.520 --> 39:41.680]  que queremos de este chatbot, entonces ¿qué significa esto? que tenemos un modelo original\n",
            "[39:41.680 --> 39:46.800]  que es el modelo de lenguaje y después tenemos un modelo nuevo que existe que es un modelo que\n",
            "[39:46.800 --> 39:54.000]  está digamos optimizado para ser conversacional y para generar respuestas que le gustan a los\n",
            "[39:54.000 --> 39:59.760]  humanos 1 y respuestas que no insulten, no degraden, que no sean agresivas, etcétera también.\n",
            "[40:01.480 --> 40:05.080]  Tenemos que entrenar el modelo de nuevo porque ya habíamos hablado antes que eran como millones\n",
            "[40:05.080 --> 40:08.920]  de dólares entrenar a otros modelos, no, no se tienen que entrenar, lo que hace acá DeepMind es\n",
            "[40:08.920 --> 40:18.280]  básicamente agarra, agarra este modelo, acá lo dice Letizia, congela las 64 layers que tiene\n",
            "[40:18.280 --> 40:24.120]  Chinchilla y solamente hace fine tuning en las últimas 16 layers. ¿Esto qué significa? Que\n",
            "[40:25.320 --> 40:30.120]  no tenemos que entrenar el modelo completo sino que entrenamos una parte. ¿Qué significa estos\n",
            "[40:30.120 --> 40:35.400]  grandes rasgos para nosotros? ¿Por qué nos importa esto? Esto nos importaría porque si lo vemos como\n",
            "[40:35.400 --> 40:43.720]  una pirámide ¿no? vamos a tener muy pocas empresas ¿se acuerdan? si nos volvemos para atrás acá\n",
            "[40:43.720 --> 40:48.280]  ¿cuáles son las empresas que están haciendo estos modelos de lenguaje? Google, DeepMind, Facebook,\n",
            "[40:48.280 --> 40:54.480]  Microsoft, OpenAI, AI21, son muy poquitas, muy poquitas las que hacen estos modelos gigantes,\n",
            "[40:54.480 --> 41:02.520]  gigantes, inmensos ¿no? entonces es muy difícil que un gobierno de latinoamérica pueda hacer uno\n",
            "[41:02.520 --> 41:08.120]  de estos. ¿Qué mercado libre puede hacer uno de estos modelos? Imposible, muy difícil. Ahora, sí es\n",
            "[41:08.120 --> 41:13.400]  posible que tomemos un modelo de lenguaje más pequeño, como chinchilla por ejemplo, o como\n",
            "[41:13.400 --> 41:27.040]  glm130, lo tomemos y después creemos un modelo de recompensa encima que sea el que digamos el que\n",
            "[41:27.040 --> 41:32.500]  genera este modelo nuevo. Entonces ese modelo de recompensa, de reward que tenemos va a ser\n",
            "[41:32.500 --> 41:40.100]  muy cercano al dataset que estamos usando y eso va a ser solamente nuestro, eso va a ser propietario,\n",
            "[41:40.100 --> 41:45.620]  eso va a ser propietario de una empresa, MercadoLibre o de un país latino, etcétera, que lo quiera hacer,\n",
            "[41:45.620 --> 41:52.420]  entonces ahí es donde estamos ya participando de toda esta generación de modelos. Entonces\n",
            "[41:53.900 --> 42:01.420]  una vez que tenemos este modelo, pueden ver que lo entrenan con este dataset,\n",
            "[42:01.420 --> 42:08.300]  explain me like and five y después genera esto básicamente. Un detalle más sobre Sparrow que es\n",
            "[42:08.300 --> 42:18.220]  algo que no tiene chat gpt, Sparrow cuando le hacemos una pregunta puede buscar evidencia y\n",
            "[42:18.220 --> 42:22.660]  esto es muy interesante, esto significa que como decirlo que el modelo lenguaje que el chatbot está\n",
            "[42:22.660 --> 42:28.780]  interactuando con el modelo exterior con el mundo exterior y lo podemos ver acá más o menos estaba\n",
            "[42:28.780 --> 42:33.460]  por acá atrás, vamos para atrás pueden ver que dice search query, search results,\n",
            "[42:33.460 --> 42:38.140]  entonces cuando nosotros le hacemos una pregunta por ejemplo quién es el\n",
            "[42:38.140 --> 42:44.140]  presidente de Brasil por ejemplo y usamos un modelo de lenguaje va a\n",
            "[42:44.140 --> 42:48.580]  tener conocimiento hasta un punto, si le preguntamos acá\n",
            "[42:48.580 --> 42:53.500]  quién es el presidente de Brasil\n",
            "[42:53.500 --> 42:59.620]  hay que entrar de nuevo acá ya nos quedamos sin acceso, vamos a ver pero si le hacemos la pregunta\n",
            "[43:03.540 --> 43:12.820]  nosotros sabemos que chat gpt está entrenado, ves? chat gpt está entrenado hasta el 2021,\n",
            "[43:12.820 --> 43:18.540]  entonces hasta el 2021 Jair Bolsonaro es el presidente, ahora es Lula, entonces no lo sabe,\n",
            "[43:18.540 --> 43:23.220]  eso quiere decir que no es tan bueno, no es tan útil este modelo porque se va a quedar desactualizado\n",
            "[43:23.220 --> 43:27.700]  muy fácil, siempre, siempre se va a quedar desactualizado. Entonces, ¿qué es lo que hace Sparrow?\n",
            "[43:27.700 --> 43:34.540]  Cuando nosotros hacemos esa pregunta, hace una búsqueda en Google y recibe los resultados de\n",
            "[43:34.540 --> 43:40.700]  los primeros, de las primeras páginas, ¿no? Entonces usa eso, lo mete dentro de la prompt, ¿ven?\n",
            "[43:40.700 --> 43:48.420]  Y después, en base a lo que salió de esa prompt, puede contestar. Entonces, esto también, interesante,\n",
            "[43:48.420 --> 43:52.900]  porque, así como nosotros estamos diciendo que haga la búsqueda en Google,\n",
            "[43:52.900 --> 43:57.500]  también lo podría hacer en la búsqueda en nuestra propia fuente de conocimiento, ¿no?\n",
            "[43:57.500 --> 43:59.900]  O sea, puede ser la Biblioteca Nacional de España, por ejemplo,\n",
            "[43:59.900 --> 44:05.100]  o la Biblioteca de Argentina, o etcétera, o una base de datos de papers médicos,\n",
            "[44:05.100 --> 44:08.540]  y nosotros decimos entonces, leíte todos estos papers médicos,\n",
            "[44:08.540 --> 44:12.780]  tráeme la información, combínala, y eso quiere decir que puede dar la fuente\n",
            "[44:12.780 --> 44:14.740]  de dónde salió, que es lo que tenemos acá, ¿no?\n",
            "[44:14.740 --> 44:21.940]  Sparrow response con evidencia, que es algo que ChatGPT no dice, ChatGPT puede inventar de cualquier\n",
            "[44:21.940 --> 44:31.620]  cosa, por ejemplo si yo le digo no es cierto, el presidente actual es Lula, es Lula, ¿no?\n",
            "[44:34.020 --> 44:39.420]  Nosotros le escribimos esto y acá podríamos empezar a cambiarlo, porque de nuevo esto sería\n",
            "[44:39.420 --> 44:46.140]  una especie de in context learning que estamos haciendo, estamos escribiendo y esto va a cambiar\n",
            "[44:46.140 --> 44:51.420]  ahora cuando nosotros salgamos de esta conversación que tenemos el modelo no es que aprendió que ahora\n",
            "[44:51.420 --> 44:55.300]  el presidente es Lula, simplemente se lo olvida, solamente funciona la conversación que estamos\n",
            "[44:55.300 --> 44:59.940]  teniendo, yo le escribo esto me va a contestar si el presidente es Lula y ahora cuando yo le pregunte\n",
            "[44:59.940 --> 45:03.300]  quién es el presidente de Brasil me va a decir el presidente es Lula, entonces es como que entendió\n",
            "[45:03.300 --> 45:10.340]  pero en realidad es simplemente la conversación que estamos teniendo. Así que bueno, última cosa que\n",
            "[45:10.340 --> 45:18.780]  voy a decir porque como siempre, como siempre esto es muy fácil, no estoy al tanto, etcétera. Esto es\n",
            "[45:18.780 --> 45:23.980]  muy fácil irse a la mierda, es un tema muy difícil, así que lo vamos a separar por partes. Lo único\n",
            "[45:23.980 --> 45:29.620]  que quiero mostrar es qué pasos tenemos que hacer para entrenar nuestro chat GPT. Bueno, esto es lo\n",
            "[45:29.620 --> 45:35.220]  que vamos a estar viendo en la serie esta de vídeos que vamos a hacer, vamos a hablar de Lion y Open\n",
            "[45:35.220 --> 45:41.860]  Assistant, que es un asistente que puede entender tasks, puede interactuar con sistemas ajenos,\n",
            "[45:41.860 --> 45:47.860]  externos, como por ejemplo realizar búsquedas y puede recuperar información del mundo exterior,\n",
            "[45:47.860 --> 45:53.100]  que es exactamente lo que queremos hacer, entonces se conecta muy bien con DeepMind,\n",
            "[45:53.100 --> 45:58.640]  lo que ellos hacen, así que acá lo tenemos, vamos a verlo con más detalle, qué es esto,\n",
            "[45:58.640 --> 46:04.400]  pero pueden ver acá qué es lo que se quiere hacer, el roadmap, primero van a, o sea esto va a salir\n",
            "[46:04.400 --> 46:09.800]  este año, le van a meter mucho huevo a hacer estas cosas, pueden ver que Yannick Kilger está\n",
            "[46:09.800 --> 46:16.080]  está laburando en esto, Yannick tiene un canal de YouTube excelente, se lo recomiendo mucho y está\n",
            "[46:16.080 --> 46:22.400]  está metiéndole huevo a estas cosas, así que la idea es generar una versión abierta de\n",
            "[46:22.400 --> 46:29.720]  de chat gpt y es lo que vamos a intentar hacer con esto. Paso a paso, paso a paso vamos a ir\n",
            "[46:29.720 --> 46:36.960]  yendo. Pueden ver cómo los pasos que tenemos es primero coleccionamos demostraciones humanas,\n",
            "[46:36.960 --> 46:44.040]  acá pueden ver en este otro caso pueden ver cómo es que van a ser las los datasets que se están\n",
            "[46:44.040 --> 46:51.840]  armando y acá van a ver el segundo paso sería hacer el fine tuning a un modelo base y el tercero\n",
            "[46:51.840 --> 46:57.840]  es bueno coleccionar estas instrucciones que lo que queremos hacer y ahí lo vamos a ir viendo\n",
            "[46:57.840 --> 47:05.000]  entrenamos el modelo reward que lo que habíamos visto de chance gpt no este está de segundo\n",
            "[47:05.000 --> 47:09.520]  entonces ya tenemos el modelo lenguaje modelo abierto queremos entrar en el modelo de recompensa\n",
            "[47:09.520 --> 47:15.600]  y después queremos hacer esto. Esto es lo que vamos a ir viendo cómo se hace en esta serie de\n",
            "[47:15.600 --> 47:21.800]  vídeos. Como siempre esto es un vídeo larguísimo, muy difícil hacerlo porque sin edición, sin nada,\n",
            "[47:21.800 --> 47:26.320]  simplemente hablando. Hay muchas cosas que seguramente me olvidé. Les pido por favor que\n",
            "[47:26.320 --> 47:32.040]  me escriban si les parece interesante, en los comentarios qué cosas faltan y tengamos una\n",
            "[47:32.040 --> 47:40.340]  conversación que esto sea la versión en español de querer hacer chat gpt que no sea simplemente un\n",
            "[47:40.340 --> 47:44.300]  vídeo de bueno cómo usarlo cómo hacerte rico cómo hacerte etcétera no que esto sea aprender\n",
            "[47:44.300 --> 47:52.620]  aprender realmente de cómo funcionan estas cosas así que con eso les mando les mando un abrazo\n",
            "[47:52.620 --> 47:58.620]  gigante gracias por quedarse ahí hasta el final hablamos 48 minutos de modelo de lenguaje que es\n",
            "[47:58.620 --> 48:05.180]  muy difícil en este day and age así que si estás acá hasta este punto\n",
            "[48:05.180 --> 48:10.300]  loco felicitaciones porque quiere decir que\n",
            "[48:10.300 --> 48:14.540]  seguramente va a hacer cosas muy grosas con todo esto y que bueno le mando un\n",
            "[48:14.540 --> 48:16.540]  un abrazo grande. Chau, chau.\n",
            "Performing alignment...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### traducción de español a inglés"
      ],
      "metadata": {
        "id": "fiZnT8xcrlVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisperx archivo.wav --hf_token $hf_token --model large-v2 --language es --task translate --vad_filter --align_model WAV2VEC2_ASR_LARGE_LV60K_960H "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQfAQYc7ZgHs",
        "outputId": "c6aa1244-c7fd-4340-b6a5-941bec2d1757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-09 18:43:50.644564: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-09 18:43:54.584498: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-09 18:43:54.585421: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-09 18:43:54.585452: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Performing VAD...\n",
            "~~ Transcribing VAD chunk: (00:00.008 --> 00:27.802) ~~\n",
            "[00:00.000 --> 00:05.000]  Hello again to the channel. If this is your first time here, what we do is Spanish publication of the latest in Machine Learning,\n",
            "[00:05.000 --> 00:10.000]  trying to explain difficult things in a way that is easy to understand and also to use.\n",
            "[00:10.000 --> 00:18.000]  This chapter of today is going to be the first of a series of videos that I wanted to do to explain from scratch how we can make our own GPT chat.\n",
            "[00:18.000 --> 00:22.440]  We are going to go from very simple things to very complex, without dodging the difficult parts,\n",
            "[00:22.440 --> 00:27.640]  but trying to explain step by step what is happening in each thing.\n",
            "~~ Transcribing VAD chunk: (00:29.135 --> 00:47.798) ~~\n",
            "[00:00.000 --> 00:04.080]  So, as always, we are going to divide the video into parts, explaining first a motivation\n",
            "[00:04.080 --> 00:08.560]  of why do something like this, then a little about the language models, what is GPT-3,\n",
            "[00:08.560 --> 00:13.720]  what is the market, where are we, explain a little about what DeepMind is doing,\n",
            "[00:13.720 --> 00:19.120]  that OpenAI is doing, and then what steps we have to take to train our own GPT chat.\n",
            "~~ Transcribing VAD chunk: (00:49.587 --> 01:05.602) ~~\n",
            "[00:00.000 --> 00:04.320]  It's impossible to put all these topics in a single video, that's why I'm going to make a series.\n",
            "[00:05.320 --> 00:07.520]  Today we're going to make a kind of introduction and then,\n",
            "[00:07.520 --> 00:10.600]  weekly, for sure, when they come out,\n",
            "[00:10.600 --> 00:12.600]  I'll be putting different things.\n",
            "[00:12.600 --> 00:15.600]  we are going to talk about what is called Reinforcement Learning from Human Feedback.\n",
            "~~ Transcribing VAD chunk: (01:05.602 --> 01:21.835) ~~\n",
            "[00:00.000 --> 00:05.600]  We are going to talk a little about how to make instructions to the giant language models\n",
            "[00:05.600 --> 00:10.800]  and we are going to talk about an initiative that Lion is doing, which is the group of people\n",
            "[00:10.800 --> 00:15.800]  which is also part of the people who made Stable Diffusion.\n",
            "~~ Transcribing VAD chunk: (01:21.835 --> 01:51.822) ~~\n",
            "[00:00.000 --> 00:03.080]  and that they are also making an open version of ChatGPT,\n",
            "[00:03.080 --> 00:05.800]  which in theory should be even better than ChatGPT.\n",
            "[00:05.800 --> 00:09.360]  So we are going to see all those things in a series of videos.\n",
            "[00:10.360 --> 00:12.280]  So first let's start with a motivation.\n",
            "[00:12.280 --> 00:15.200]  What is it that leads me to do something like this?\n",
            "[00:15.200 --> 00:18.320]  Well, first of all, I think I was seeing this tweet,\n",
            "[00:18.320 --> 00:21.040]  the truth is, a lot of this conversation is happening on Twitter,\n",
            "[00:21.040 --> 00:25.000]  so it's quite interesting if you don't use it, let's say,\n",
            "[00:25.000 --> 00:27.640]  to follow everything that is the part of Machine Learning, of news.\n",
            "[00:27.640 --> 00:30.640]  The conversation is happening there and in another two or three places.\n",
            "~~ Transcribing VAD chunk: (01:51.822 --> 02:17.152) ~~\n",
            "[00:00.000 --> 00:06.800]  but on twitter it is very concentrated and here for example this is david he is telling us that he is\n",
            "[00:06.800 --> 00:13.920]  making an analogy with 2008 and if we are in 2008 there is a very large recession and the\n",
            "[00:13.920 --> 00:20.480]  iphone just came out the previous year the first android phone just came out this year 2008 that is, it will\n",
            "[00:20.480 --> 00:24.480]  be presented in October and then the next the next three to five generations are going to revolutionize\n",
            "[00:24.480 --> 00:27.480]  absolutely everything.\n",
            "~~ Transcribing VAD chunk: (02:17.725 --> 02:47.172) ~~\n",
            "[00:00.000 --> 00:04.960]  we can say that more or less it is what we are having here now with chat gpt, it is not the first\n",
            "[00:04.960 --> 00:09.320]  model, it is not the last, it is the first model that is opening the space to all these things,\n",
            "[00:09.320 --> 00:13.840]  the same thing that happened with dally 2 when it just came out, an incredible revolution, now dally 2\n",
            "[00:13.840 --> 00:18.840]  is not used so much either, but the open source versions, let's say we have me journey, we have everything\n",
            "[00:18.840 --> 00:26.320]  that is stable diffusion, etc., all that is infinitely better now than dally 2, but the reason\n",
            "[00:26.320 --> 00:31.560]  for which that development was accelerated was due to the appearance of\n",
            "~~ Transcribing VAD chunk: (02:49.349 --> 03:14.375) ~~\n",
            "[00:00.000 --> 00:04.000]  I'm going to explain a little bit about why I'm motivated.\n",
            "[00:04.000 --> 00:12.000]  From my point of view, because of my job, I'm lucky to be able to work a lot with giant language models at the moment.\n",
            "[00:12.000 --> 00:16.000]  And what I'm seeing is a great divergence that is going to occur, or is occurring now.\n",
            "[00:16.000 --> 00:18.000]  I'm going to exaggerate it, I'm going to go for the absurd.\n",
            "[00:18.000 --> 00:22.320]  we are going to say that there is a group of engineers in Silicon Valley and then we have\n",
            "[00:22.320 --> 00:25.000]  a group of engineers in Latin America.\n",
            "~~ Transcribing VAD chunk: (03:15.033 --> 03:34.270) ~~\n",
            "[00:00.000 --> 00:05.600]  The difference in knowledge, in use, in generation, in the ability to generate new things\n",
            "[00:05.600 --> 00:07.600]  is huge.\n",
            "[00:07.600 --> 00:09.600]  It's huge. That's what I'm seeing.\n",
            "[00:09.600 --> 00:12.600]  So, for me, it's very important, very relevant,\n",
            "[00:12.600 --> 00:14.600]  to start having technical conversations\n",
            "[00:14.600 --> 00:16.600]  about how these things work.\n",
            "[00:16.600 --> 00:20.080]  only that\n",
            "~~ Transcribing VAD chunk: (03:34.270 --> 04:00.697) ~~\n",
            "[00:00.000 --> 00:13.000]  We don't just become users of these technologies, we also have to be able to generate them or at least modify them in a way that we can generate new things.\n",
            "[00:13.000 --> 00:20.640]  And to do that, there is no other way than to get in and get fully involved in these things technically,\n",
            "[00:20.640 --> 00:26.360]  see what is being done, what are the conversations that are taking place, and basically, give it a lot of eggs.\n",
            "~~ Transcribing VAD chunk: (04:01.439 --> 04:13.437) ~~\n",
            "[00:00.000 --> 00:07.800]  As a little more introduction, there I put that it is explained by a Data Scientist, that's because I work as a Data Scientist, let's say.\n",
            "[00:07.800 --> 00:11.800]  But that's not what I studied. I studied mechanical engineering.\n",
            "~~ Transcribing VAD chunk: (04:13.741 --> 04:34.413) ~~\n",
            "[00:00.000 --> 00:05.320]  many years ago, 2016 more or less, I started doing my first course, I didn't know how to program and I said\n",
            "[00:05.320 --> 00:10.400]  look, I want to get into this, I really loved it, how do I do it? I will always be at a disadvantage\n",
            "[00:10.400 --> 00:14.640]  with the people who studied physics, mathematics, etc., who are already coming, let's say I will always be\n",
            "[00:14.640 --> 00:19.560]  years behind. So my decision at that time was, I have to make a strategy, I'm going to do a leapfrog,\n",
            "[00:19.560 --> 00:21.560]  It's called jumping.\n",
            "~~ Transcribing VAD chunk: (04:34.919 --> 04:49.870) ~~\n",
            "[00:00.000 --> 00:03.660]  I'm going to get directly into deep learning, computer vision in this case,\n",
            "[00:03.660 --> 00:05.660]  and natural language models.\n",
            "[00:05.660 --> 00:07.660]  So, what I mean by this is,\n",
            "[00:07.660 --> 00:11.040]  all the techniques that have existed up to this point,\n",
            "[00:11.040 --> 00:15.040]  it is possible that they will throw themselves into the trash at this time.\n",
            "~~ Transcribing VAD chunk: (04:49.870 --> 05:10.390) ~~\n",
            "[00:00.000 --> 00:08.180]  Is it important to have mathematical knowledge? Yes, but with algebra, with a basic level of algebra, I think you can do very interesting things.\n",
            "[00:08.180 --> 00:16.740]  With this, what I want to say is, no matter where you are coming from, what matters is what you want to do in the future and how much work you put into studying.\n",
            "[00:16.740 --> 00:19.700]  So today we are going to put a lot of work into studying.\n",
            "[00:19.700 --> 00:21.700]  Well...\n",
            "~~ Transcribing VAD chunk: (05:11.234 --> 05:35.990) ~~\n",
            "[00:00.000 --> 00:04.260]  Let's see where we are with all this, because it's important.\n",
            "[00:04.260 --> 00:07.100]  This comic is spectacular. Look at this comic.\n",
            "[00:07.100 --> 00:12.440]  It says, hey man, did you realize how artificial intelligence is going to impact our business?\n",
            "[00:12.440 --> 00:14.440]  Hey, don't worry, I'm working on it.\n",
            "[00:14.440 --> 00:16.440]  And it asks ChatGPT.\n",
            "[00:16.440 --> 00:19.740]  How is artificial intelligence going to impact our business?\n",
            "[00:19.740 --> 00:21.740]  And there are many ways to do it.\n",
            "[00:21.740 --> 00:23.740]  Excellent comic\n",
            "~~ Transcribing VAD chunk: (05:36.530 --> 06:04.053) ~~\n",
            "[00:00.000 --> 00:04.760]  This is something that is happening at the same time, that is, there are other models, I did not talk about this\n",
            "[00:04.760 --> 00:08.760]  Bali model, we talked about Whisper, we talked about Chats GPT, Bali is a model that Microsoft is\n",
            "[00:08.760 --> 00:14.840]  also releasing, which is basically a model that does text to speech, it converts, it generates,\n",
            "[00:14.840 --> 00:23.800]  let's say, a synthesized voice. How does it do it? It is copied from a 3-second clip,\n",
            "[00:23.800 --> 00:27.480]  from the person we want to copy, and with that it is simply the only thing it needs to\n",
            "[00:27.480 --> 00:29.080]  Create. Share. Learn.\n",
            "~~ Transcribing VAD chunk: (06:04.458 --> 06:32.470) ~~\n",
            "[00:00.000 --> 00:02.000]  to generate audio.\n",
            "[00:03.740 --> 00:05.740]  So, if we put these things together,\n",
            "[00:05.740 --> 00:07.740]  this is a detail,\n",
            "[00:07.740 --> 00:10.500]  Bali reminds me a lot of the movie Terminator 2,\n",
            "[00:10.500 --> 00:12.500]  when he calls Terminator,\n",
            "[00:12.500 --> 00:14.500]  what happened to the dog? He changes his name and says\n",
            "[00:14.500 --> 00:16.500]  your dog is dead.\n",
            "[00:16.500 --> 00:19.500]  At that moment, I was very surprised by that scene.\n",
            "[00:19.500 --> 00:23.000]  We can see that Bali is something similar to that.\n",
            "[00:23.000 --> 00:25.000]  Well, what do we have here?\n",
            "[00:25.000 --> 00:27.000]  A commercial model, right?\n",
            "[00:27.000 --> 00:29.000]  Siri, Alexa,\n",
            "~~ Transcribing VAD chunk: (06:32.757 --> 07:01.563) ~~\n",
            "[00:00.000 --> 00:04.000]  That's it. This competes directly with these two things.\n",
            "[00:04.000 --> 00:09.720]  So we have Whisper, which converts audio to text.\n",
            "[00:09.720 --> 00:12.640]  Then ChatGPT, which converts text to something else.\n",
            "[00:12.640 --> 00:14.640]  We don't know what yet, but to something.\n",
            "[00:15.320 --> 00:19.280]  And then Valley, which converts it again to audio.\n",
            "[00:19.280 --> 00:21.160]  So we have a conversation.\n",
            "[00:21.160 --> 00:22.560]  In this case it says,\n",
            "[00:22.560 --> 00:27.120]  turn on the party lights and start playing good music\n",
            "[00:27.120 --> 00:29.120]  at 8 p.m. every Wednesday.\n",
            "~~ Transcribing VAD chunk: (07:02.272 --> 07:25.998) ~~\n",
            "[00:00.000 --> 00:06.000]  But we can see how new things are starting to appear, and this is dangerous, not only for\n",
            "[00:06.000 --> 00:15.000]  let's say, we can say, some things are good, others are dangerous, but companies, for example Canva, is a design company\n",
            "[00:15.000 --> 00:23.560]  It's a design company that competes with Adobe, but one thing that Canva has\n",
            "~~ Transcribing VAD chunk: (07:26.285 --> 07:54.398) ~~\n",
            "[00:00.000 --> 00:07.400]  is that, look, they were going to go to the market now, that is, ask for a loan to the market and the guys said\n",
            "[00:07.400 --> 00:15.000]  well, we're going to have to hire ChatGPT, I mean, not ChatGPT, but GPT-3 at least, because we can't compete with them\n",
            "[00:15.000 --> 00:20.300]  we don't have the capacity to compete with them, so you can imagine how companies are already\n",
            "[00:20.300 --> 00:26.500]  that is, a system is being generated where you are either with me or I'm going to bust you, basically\n",
            "[00:26.500 --> 00:28.500]  That, then, is what is happening.\n",
            "~~ Transcribing VAD chunk: (07:55.428 --> 08:17.703) ~~\n",
            "[00:00.000 --> 00:06.000]  No one is safe in this sense. It's not just a technology company, it's any type of company.\n",
            "[00:06.000 --> 00:13.000]  A technical support company, for example, has no reason not to feel threatened by something like ChatGPT.\n",
            "[00:13.000 --> 00:16.000]  This is like Apple getting into the automotive market.\n",
            "[00:16.000 --> 00:21.000]  Okay, we have Volkswagen, we have Ford, Chevrolet, etc., but when Apple gets in, it's going to revolutionize.\n",
            "[00:21.000 --> 00:24.360]  we don't know what's going on.\n",
            "~~ Transcribing VAD chunk: (08:19.610 --> 08:26.562) ~~\n",
            "[00:00.000 --> 00:05.700]  So well, let's see a little bit what language models are and what GPT-3 is, right?\n",
            "[00:05.700 --> 00:09.260]  we are going to start very slowly\n",
            "~~ Transcribing VAD chunk: (08:27.929 --> 08:53.832) ~~\n",
            "[00:00.000 --> 00:07.460]  First of all, I'm very lazy so I don't edit the videos, I record them all at once.\n",
            "[00:07.460 --> 00:11.480]  It's pretty hard to do this, so I'll probably make a lot of mistakes.\n",
            "[00:11.480 --> 00:15.840]  As I go, I'll forget things, I'll probably make mistakes in other things.\n",
            "[00:15.840 --> 00:20.600]  I ask you, if there's something I missed or didn't understand, or that I explained wrongly,\n",
            "[00:20.600 --> 00:25.840]  It is very likely that you will put it in the comments and I will check it again for the next one.\n",
            "~~ Transcribing VAD chunk: (08:54.237 --> 09:14.673) ~~\n",
            "[00:00.000 --> 00:04.160]  all the links I put here I will put them in the description of the video\n",
            "[00:04.160 --> 00:09.480]  my idea with this is not to make a short video, bad, etc., summarized\n",
            "[00:09.480 --> 00:13.120]  with videos, with graphics, etc., no, no, this is a hard video\n",
            "[00:13.120 --> 00:19.960]  to see it over time, that is, go back and say what did this crazy guy say in such a place, well, he said such a thing\n",
            "[00:19.960 --> 00:21.960]  That's the idea.\n",
            "~~ Transcribing VAD chunk: (09:15.280 --> 09:45.065) ~~\n",
            "[00:00.000 --> 00:06.280]  This link is a course that has just come out, it is called CS324 from Stanford University, it is a\n",
            "[00:06.280 --> 00:13.800]  language model course, it is quite good and it serves as an introduction to the topic. What is a\n",
            "[00:13.800 --> 00:18.760]  language model? Well, the classic definition that we have of a language model is the\n",
            "[00:18.760 --> 00:23.840]  distribution of probabilities over a sequence of tokens. What does this mean? It means that,\n",
            "[00:23.840 --> 00:28.720]  For example, if we say here, the mouse ate the cheese.\n",
            "~~ Transcribing VAD chunk: (09:45.554 --> 10:14.191) ~~\n",
            "[00:00.000 --> 00:08.600]  this has a probability of occurrence of 0.02, excellent, now if we say the cheese\n",
            "[00:08.600 --> 00:16.920]  ate the mouse and it does not have a probability of occurrence less than 0.01 and if we have the mouse\n",
            "[00:16.920 --> 00:25.160]  the cheese ate a probability of occurrence very low and how is it that this is calculated\n",
            "[00:25.160 --> 00:30.680]  this probability that it will happen, well...\n",
            "~~ Transcribing VAD chunk: (10:16.317 --> 10:44.549) ~~\n",
            "[00:00.000 --> 00:07.500]  This means that the model we are using has linguistic abilities, but it also has knowledge of the world, right?\n",
            "[00:07.500 --> 00:13.840]  For example, that means that grammatically, perhaps, these two sentences are correct,\n",
            "[00:13.840 --> 00:21.120]  but they are not normally used, it would never happen, something like that, or it is unlikely, it is less likely, half likely,\n",
            "[00:21.120 --> 00:24.560]  that the second situation happens compared to the first situation, right?\n",
            "[00:24.560 --> 00:30.160]  So what we can say is that these language models have two things\n",
            "~~ Transcribing VAD chunk: (10:44.836 --> 11:12.410) ~~\n",
            "[00:00.000 --> 00:05.500]  In general, they have syntactic knowledge of grammar and they also have knowledge of the world.\n",
            "[00:05.500 --> 00:07.500]  How do they have knowledge of the world?\n",
            "[00:07.500 --> 00:11.000]  Because, for example, to train these things, we put the entire Wikipedia\n",
            "[00:11.000 --> 00:13.500]  in Spanish, in English, etc.\n",
            "[00:13.500 --> 00:19.500]  We put the entire Wikipedia and there are examples where this type of phrase was read.\n",
            "[00:19.500 --> 00:24.000]  So, for example, this had two occurrences, this had one occurrence, and that's it.\n",
            "[00:24.000 --> 00:27.500]  So it's twice as likely as... something like that, more or less.\n",
            "[00:27.500 --> 00:29.100]  Create. Share. Learn.\n",
            "~~ Transcribing VAD chunk: (11:12.933 --> 11:41.198) ~~\n",
            "[00:00.000 --> 00:11.520]  and etc. It's explained here. I'm not going to get into this topic of how it is because the truth is that they do it very well in this course so I recommend it, here you can see what the capacities are,\n",
            "[00:11.520 --> 00:28.560]  you can also have with gpt3, etc. and you can see all these things. So I'm not going to get into much with this, the perplexity is also explained, which is a metric that is used to know how good the model is, right?\n",
            "~~ Transcribing VAD chunk: (11:42.110 --> 12:08.198) ~~\n",
            "[00:00.000 --> 00:02.780]  How are these language models trained then? What do they do?\n",
            "[00:02.780 --> 00:06.900]  We already said, it returns a probability of a sequence of tokens.\n",
            "[00:06.900 --> 00:10.260]  Well, for what these models are trained, they are generative models.\n",
            "[00:10.260 --> 00:14.660]  So what it means is that normally what happens is that a sequence of tokens is given,\n",
            "[00:14.660 --> 00:18.120]  put it again, the mouse ate it, the what?\n",
            "[00:18.120 --> 00:21.840]  Well, it has to predict based on those three tokens,\n",
            "[00:21.840 --> 00:25.560]  the four tokens I had before, the mouse ate it,\n",
            "[00:25.560 --> 00:26.560]  Well...\n",
            "~~ Transcribing VAD chunk: (12:08.384 --> 12:38.118) ~~\n",
            "[00:00.000 --> 00:04.520]  It has to predict the next token that will come after that, right?\n",
            "[00:04.520 --> 00:07.800]  And that says the cheese with a high probability, and so on.\n",
            "[00:09.800 --> 00:15.480]  We saw it in other videos, I will also put it in the chat, but basically there is also a thing called ngrams, right?\n",
            "[00:15.480 --> 00:24.600]  So maybe it doesn't just need, for example, to predict a token, but maybe it will predict several tokens, right?\n",
            "[00:24.600 --> 00:30.200]  So with this you can generate things that are more plausible, that are more likely to work.\n",
            "~~ Transcribing VAD chunk: (12:37.477 --> 12:47.737) ~~\n",
            "[00:00.000 --> 00:07.400]  likely to happen. Two tokens, three tokens, and there are different ways to choose these tokens, right?\n",
            "[00:07.400 --> 00:10.400]  greedy search, beam, etc.\n",
            "~~ Transcribing VAD chunk: (12:49.829 --> 13:17.318) ~~\n",
            "[00:00.000 --> 00:05.040]  Here you can see that when these models are trained, one thing that can be done is\n",
            "[00:05.040 --> 00:13.320]  you can write and tell it, predict the next token, and the way this is done is\n",
            "[00:13.320 --> 00:19.200]  because I already have the wikipedia, so what I can do is simply shorten the sentence and say\n",
            "[00:19.200 --> 00:25.600]  the missing token is this, then the model learns very well, because it says I predicted chorizo\n",
            "[00:25.600 --> 00:27.600]  No, man. It's not chorizo.\n",
            "~~ Transcribing VAD chunk: (13:17.369 --> 13:47.137) ~~\n",
            "[00:00.000 --> 00:03.800]  What's that? What's that? I have the answer here. What's that? No, no. It's wrong. Fix it.\n",
            "[00:03.800 --> 00:06.300]  So there it goes learning as we train it.\n",
            "[00:06.300 --> 00:09.200]  Another way to do it, instead of predicting the next token,\n",
            "[00:09.200 --> 00:12.300]  is when we give it a blank space, which is what we have here.\n",
            "[00:12.300 --> 00:16.300]  Basically what we are saying is,\n",
            "[00:16.300 --> 00:19.800]  look, fill me the blank space. It doesn't mean it's the last token,\n",
            "[00:19.800 --> 00:23.300]  but yes, maybe it's an intermediate token, right?\n",
            "[00:23.300 --> 00:25.100]  This is what the model does.\n",
            "[00:25.100 --> 00:27.100]  Well,\n",
            "[00:27.100 --> 00:31.740]  These are generation language models.\n",
            "~~ Transcribing VAD chunk: (13:48.858 --> 14:07.538) ~~\n",
            "[00:00.000 --> 00:07.400]  but they are not giant models of language, let's see what are the giant models of language, these are\n",
            "[00:07.400 --> 00:12.560]  models that are trained with a stupid amount of data and that were\n",
            "[00:12.560 --> 00:18.120]  adapted to do a stupid amount of tasks basically since they are called what would be zero shot\n",
            "~~ Transcribing VAD chunk: (14:07.673 --> 14:35.703) ~~\n",
            "[00:00.000 --> 00:05.140]  That means that tomorrow we ask him to write a cumbia song,\n",
            "[00:05.500 --> 00:08.440]  but he never trained to write a cumbia song, but he can do it the same way.\n",
            "[00:09.200 --> 00:09.700]  Zero shot.\n",
            "[00:10.140 --> 00:12.840]  If it was one shot, for example, which has an example,\n",
            "[00:12.840 --> 00:16.940]  when we're doing it, we give him an example and then we say,\n",
            "[00:17.140 --> 00:20.140]  this is a cumbia song, let's write a new song. One shot.\n",
            "[00:21.900 --> 00:26.300]  Supervised learning would be that we train him only with a cumbia song dataset\n",
            "[00:26.300 --> 00:28.300]  and we tell you to make me a new one that has this format.\n",
            "~~ Transcribing VAD chunk: (14:37.205 --> 15:00.779) ~~\n",
            "[00:00.000 --> 00:03.360]  What's going on with these language models that we have here?\n",
            "[00:04.960 --> 00:07.760]  They are doing better than humans in many things.\n",
            "[00:07.760 --> 00:11.360]  The gray line that you have here are humans.\n",
            "[00:11.360 --> 00:13.160]  And you can see that we are already surpassing them.\n",
            "[00:13.160 --> 00:19.520]  There was a time when models started to do better than humans.\n",
            "[00:19.520 --> 00:23.520]  Here you can see the performance in 58 tasks.\n",
            "~~ Transcribing VAD chunk: (15:01.505 --> 15:22.717) ~~\n",
            "[00:00.000 --> 00:04.840]  And the issue they have is that, this is called...\n",
            "[00:04.840 --> 00:10.000]  I mean, down here you can see the parameters, and up here you can see the performance.\n",
            "[00:10.000 --> 00:12.000]  Well, the parameters below...\n",
            "[00:12.000 --> 00:16.840]  What they realized is that when we raised the parameters, which is what happened with GPT-3,\n",
            "[00:16.840 --> 00:18.840]  they put a stupid amount of parameters,\n",
            "[00:18.840 --> 00:23.160]  they started to write down something called emergency.\n",
            "~~ Transcribing VAD chunk: (15:23.037 --> 15:46.375) ~~\n",
            "[00:00.000 --> 00:04.800]  not an emergency, something is happening, the house is on fire, but\n",
            "[00:04.800 --> 00:10.140]  emergent abilities of these models. And this, to this day, we don't know why it happens.\n",
            "[00:10.140 --> 00:14.800]  There are approximations, but we don't know yet what it is. Consciousness? We don't know yet.\n",
            "[00:14.800 --> 00:20.220]  But what we do know is that when you get to a very large number of parameters,\n",
            "[00:20.220 --> 00:23.580]  These models are starting to be much better than before.\n",
            "~~ Transcribing VAD chunk: (15:46.207 --> 16:02.137) ~~\n",
            "[00:00.000 --> 00:06.140]  that before, that is, the increase in performance is not linear but becomes exponential, which is what\n",
            "[00:06.140 --> 00:12.000]  we see here, right? Here you can see how a jump of the triple of performance occurs when we start to\n",
            "[00:12.000 --> 00:14.000]  to increase the number of parameters.\n",
            "~~ Transcribing VAD chunk: (16:03.284 --> 16:28.630) ~~\n",
            "[00:00.000 --> 00:04.800]  Here we can see how evolution is through the years, we had statistics, we had machine learning, we had deep learning\n",
            "[00:04.800 --> 00:07.300]  and now we have what is called Foundation Models.\n",
            "[00:07.300 --> 00:14.600]  What do these Foundation Models mean? Well, they are giant models, we can say that it is a...\n",
            "[00:14.600 --> 00:20.600]  It is not in this case, but we can say that we are going in the direction of a very large intelligence,\n",
            "[00:20.600 --> 00:25.600]  Let's say that we can ask them about anything and they can answer us about those things, right?\n",
            "~~ Transcribing VAD chunk: (16:30.520 --> 16:57.942) ~~\n",
            "[00:00.000 --> 00:06.480]  These are the datasets that are used, they can be from anything, books, code, reddit, medical sites,\n",
            "[00:06.480 --> 00:14.440]  medical papers, patents, etc., that is, everything that we have generated as humanity\n",
            "[00:14.440 --> 00:18.640]  is what we can put into these models as training data, right?\n",
            "[00:18.640 --> 00:21.640]  And these are the things that these models can do.\n",
            "[00:21.640 --> 00:27.640]  They can summarize, they can generate images, they can classify the text, translate, chat, create recipes,\n",
            "~~ Transcribing VAD chunk: (16:58.432 --> 17:22.529) ~~\n",
            "[00:00.000 --> 00:09.800]  explain code without many many things and here we begin to see a little what were the things that made this revolution happen\n",
            "[00:09.800 --> 00:20.640]  first the architecture of transformers that was done in the year, that is, the attention and sole unit paper was done in the year 2017\n",
            "[00:20.640 --> 00:23.920]  and all the people who made that paper, except...\n",
            "~~ Transcribing VAD chunk: (17:22.968 --> 17:46.154) ~~\n",
            "[00:00.000 --> 00:07.500]  One person left Google, that paper was done by Google, and they went to start their own startups or OpenAI, etc.\n",
            "[00:08.500 --> 00:10.500]  You can see how important that was.\n",
            "[00:11.000 --> 00:18.000]  Here we have the evolution of the amount of compute that was used to train these models,\n",
            "[00:18.000 --> 00:21.440]  that is, it is hitting an exponential as we get closer that means that\n",
            "[00:21.440 --> 00:25.440]  train these models, it's worth millions of dollars.\n",
            "~~ Transcribing VAD chunk: (17:47.065 --> 18:09.847) ~~\n",
            "[00:00.000 --> 00:05.280]  And we can see here who dominates the risk, etc.\n",
            "[00:05.280 --> 00:07.280]  This is basically what I'm saying.\n",
            "[00:07.640 --> 00:10.120]  Big companies are the ones who have control over these things.\n",
            "[00:10.120 --> 00:12.120]  And that's not good.\n",
            "[00:12.120 --> 00:14.120]  It should be fairly distributed.\n",
            "[00:14.120 --> 00:16.120]  Industry, open source community,\n",
            "[00:16.980 --> 00:18.980]  governments, state.\n",
            "[00:18.980 --> 00:22.580]  It has to be distributed, at least in an egalitarian way.\n",
            "~~ Transcribing VAD chunk: (18:10.505 --> 18:40.357) ~~\n",
            "[00:00.000 --> 00:12.720]  These are the models that we have available today, you can see that these giant models, here we have the parameters, 540,000, 130,000, 80,000, 20,000, 180,000, etc.\n",
            "[00:12.720 --> 00:22.880]  You can see that the companies that do it are Google, Google with DeepMind, Facebook, Meta, Microsoft, OpenAI, and BigScience, which is an open model.\n",
            "[00:22.880 --> 00:27.880]  And then AI21 is an Israeli company that is doing this, right?\n",
            "[00:27.880 --> 00:29.880]  well and etc.\n",
            "~~ Transcribing VAD chunk: (18:39.648 --> 19:01.990) ~~\n",
            "[00:00.000 --> 00:07.920]  etc. maps and so on. One interesting thing here, this is very good, I recommend it to see it, it is called\n",
            "[00:07.920 --> 00:17.320]  the state of AI report 2022, it is made by Nathan Benaj and Ian Hogarth, both of them are in what\n",
            "[00:17.320 --> 00:22.480]  which is Venture Capital, and the truth is that they do an excellent analysis, they have been doing it for several years, you can see it.\n",
            "~~ Transcribing VAD chunk: (19:01.754 --> 19:30.172) ~~\n",
            "[00:00.000 --> 00:03.840]  As you can see down here, they've been doing it since 2018 and it's spectacular.\n",
            "[00:03.840 --> 00:10.320]  There's no better summary on the internet than what these people do.\n",
            "[00:10.320 --> 00:11.840]  What did they do here?\n",
            "[00:11.840 --> 00:18.840]  Here we can see that they are talking about where we are at the moment.\n",
            "[00:18.840 --> 00:23.480]  Well, five years after the release of the Transformers, it's still the best architecture.\n",
            "[00:23.480 --> 00:27.480]  There's always something new that tries to modify this, that, etc.\n",
            "[00:27.480 --> 00:29.480]  But the truth is that...\n",
            "~~ Transcribing VAD chunk: (19:30.526 --> 19:58.808) ~~\n",
            "[00:00.000 --> 00:03.280]  There's no turning back, it's the best architecture, the one that scales the best,\n",
            "[00:03.280 --> 00:07.360]  the one that generates this kind of things the best, let's say.\n",
            "[00:07.360 --> 00:10.320]  And in the end, what you do is\n",
            "[00:10.320 --> 00:13.880]  you simply train for longer and you get better results.\n",
            "[00:13.880 --> 00:16.040]  Why? More or less, we don't know.\n",
            "[00:16.040 --> 00:19.400]  But that's generally what happens, so why not?\n",
            "[00:19.400 --> 00:22.680]  Instead of spending years,\n",
            "[00:22.680 --> 00:24.640]  why don't we train directly and see what happens?\n",
            "[00:24.640 --> 00:28.640]  Well, this is how machine learning research looks like today.\n",
            "~~ Transcribing VAD chunk: (20:00.007 --> 20:00.614) ~~\n",
            "[00:00.000 --> 00:02.000]  Buy it here.\n",
            "~~ Transcribing VAD chunk: (20:00.884 --> 20:30.247) ~~\n",
            "[00:00.000 --> 00:08.640]  several of these things, this is what we were saying about the issue of the emergency,\n",
            "[00:08.640 --> 00:15.000]  that is, there is this, emergence, and you can see here as we reach a level of compute,\n",
            "[00:15.000 --> 00:20.120]  boom, it hits a spectacular shot, that is, it is incredible how the models improve a lot,\n",
            "[00:20.120 --> 00:25.600]  they go to triple, quadruple performance when they do this, so let's hope that, for example,\n",
            "[00:25.600 --> 00:29.600]  Exaggerating, right? GPT-4 can have some of these characteristics.\n",
            "~~ Transcribing VAD chunk: (20:30.247 --> 20:38.431) ~~\n",
            "[00:00.000 --> 00:07.560]  so we were talking about... well there are other things and I recommend this to see this you can see here\n",
            "~~ Transcribing VAD chunk: (20:38.515 --> 21:06.528) ~~\n",
            "[00:00.000 --> 00:04.200]  When GPT-3 just came out in 2020, you can see how it took a while,\n",
            "[00:04.200 --> 00:10.200]  I mean, they were all in a ball, until more or less here open models started to come out.\n",
            "[00:10.200 --> 00:15.300]  GPT-J, this Eloiter AI, now you will see why Eloiter AI is important,\n",
            "[00:15.300 --> 00:23.300]  that they worked with Lion, Carper AI, Stabilite AI, all part of the same set,\n",
            "[00:23.300 --> 00:28.300]  to open source all this science. It's an incredible job.\n",
            "~~ Transcribing VAD chunk: (21:07.574 --> 21:35.384) ~~\n",
            "[00:00.000 --> 00:08.320]  Well, you can see how it took at least a year, almost, until all these open source versions started to come out, right?\n",
            "[00:08.320 --> 00:16.840]  So, just as OpenChatGPT came out now, we can give it a few months, maybe a year, and we will start to have those things.\n",
            "[00:16.840 --> 00:18.840]  The same with DALI.\n",
            "[00:19.760 --> 00:22.840]  The whole part of Alphafall and etc. is a little more difficult.\n",
            "[00:22.840 --> 00:27.720]  Well, with this, the only thing I wanted to show, maybe here, interesting is...\n",
            "~~ Transcribing VAD chunk: (21:38.118 --> 22:02.992) ~~\n",
            "[00:00.000 --> 00:08.080]  To train this, you need a lot of GPUs, so these are the GPUs that are available.\n",
            "[00:08.080 --> 00:10.080]  Very few companies in the world have them.\n",
            "[00:10.080 --> 00:11.920]  Here you can have them as...\n",
            "[00:11.920 --> 00:15.280]  The orange ones are national laboratories.\n",
            "[00:15.280 --> 00:17.640]  They are usually in the United States or Europe.\n",
            "[00:17.640 --> 00:20.640]  The blue ones are private companies.\n",
            "[00:20.640 --> 00:25.040]  and what is in red are, let's say, Amazon, for example.\n",
            "~~ Transcribing VAD chunk: (22:04.223 --> 22:25.317) ~~\n",
            "[00:00.000 --> 00:05.480]  But here, this is the original paper that I wanted to show you, attention is all you need.\n",
            "[00:06.580 --> 00:08.580]  A piece of paper, these guys put together.\n",
            "[00:08.960 --> 00:13.120]  Well, these people who are here, you can see the companies that came out and founded.\n",
            "[00:13.120 --> 00:19.720]  All these multimillionaires, you can see them here, Unicorn in some cases, of what they did.\n",
            "[00:19.720 --> 00:21.720]  Well, that's all for this video.\n",
            "~~ Transcribing VAD chunk: (22:26.785 --> 22:49.482) ~~\n",
            "[00:00.000 --> 00:06.080]  We talked about Transformer, we talked about OpenAI, let's see what is this GPT, what is GPT?\n",
            "[00:06.080 --> 00:11.920]  G, the G, the P and the T. The G, generative, P, pre-trained, T, transformers.\n",
            "[00:11.920 --> 00:20.040]  What is GPT-3 then? It is basically a model of transformers that is used to generate text\n",
            "[00:20.040 --> 00:23.040]  and that was trained with a gigantic amount of data.\n",
            "~~ Transcribing VAD chunk: (22:54.494 --> 23:21.258) ~~\n",
            "[00:00.000 --> 00:06.760]  When we talk about a model, what model are we talking about?\n",
            "[00:06.760 --> 00:13.480]  Well, let's go here and let's see what Andrej Karpaty did.\n",
            "[00:13.480 --> 00:17.840]  Andrej Karpaty is a guy who was the director of machine learning,\n",
            "[00:17.840 --> 00:21.160]  that is, the director of OpenAI, when it just started.\n",
            "[00:21.160 --> 00:27.040]  Elon Musk had put him money, if you remember, he had put him a thousand million dollars, more or less.\n",
            "~~ Transcribing VAD chunk: (23:21.730 --> 23:49.220) ~~\n",
            "[00:00.000 --> 00:02.880]  to start. The idea of OpenAI was that it was open.\n",
            "[00:03.520 --> 00:06.080]  So when Carpati started\n",
            "[00:07.960 --> 00:11.720]  as a DI director, after that he went to be a DI director at Tesla.\n",
            "[00:11.720 --> 00:13.720]  He was the person responsible for\n",
            "[00:14.000 --> 00:17.280]  the autonomous driving part of Tesla.\n",
            "[00:17.280 --> 00:19.280]  And a few months ago\n",
            "[00:19.280 --> 00:22.520]  he said, I don't want to be with this anymore.\n",
            "[00:22.520 --> 00:26.280]  I want to make YouTube videos, I want to make code, I want to have a good time, play the guitar, etc.\n",
            "[00:26.280 --> 00:27.560]  and thanks to God.\n",
            "~~ Transcribing VAD chunk: (23:49.726 --> 24:14.245) ~~\n",
            "[00:00.000 --> 00:06.440]  because one of the things he did is, for example, this GPT implementation, right?\n",
            "[00:06.440 --> 00:10.620]  It was sent, we can see it, André Capo, Capo André.\n",
            "[00:10.620 --> 00:17.220]  Well, here you can see the current implementations are giant, super long, impossible to run, etc.\n",
            "[00:17.220 --> 00:24.340]  type said here you have to make one that is called nano gpt that are 300 lines of code\n",
            "~~ Transcribing VAD chunk: (24:14.718 --> 24:25.602) ~~\n",
            "[00:00.000 --> 00:06.080]  that has only a couple of files, which is this, modelo.py and train.py, that's it, two files.\n",
            "[00:06.080 --> 00:11.080]  Nothing else, the rest are to help you do this, but two files and that's it.\n",
            "~~ Transcribing VAD chunk: (24:25.973 --> 24:49.177) ~~\n",
            "[00:00.000 --> 00:04.320]  That's it, with that you can have GPT-2, which is what we have here.\n",
            "[00:04.880 --> 00:10.080]  So, using this code, basically, we can train GPT-2.\n",
            "[00:10.080 --> 00:15.280]  Here in this case, this guy uses a dataset called OpenWebText\n",
            "[00:15.760 --> 00:19.520]  and this is what he uses to train the model that we see here, right?\n",
            "[00:19.520 --> 00:23.520]  It trains it with a single instance.\n",
            "~~ Transcribing VAD chunk: (24:49.328 --> 25:12.127) ~~\n",
            "[00:00.000 --> 00:08.400]  from 8 gpus from NVIDIA to 100 for 38 hours. If you remember we are going to go here to cloud\n",
            "[00:10.240 --> 00:18.880]  if you see here we said that that is this, it is a p4d, it comes out 32 mango if we put 32 mango for 38\n",
            "[00:18.880 --> 00:22.640]  it gives us 1,200 dollars well that means that\n",
            "~~ Transcribing VAD chunk: (25:13.544 --> 25:22.707) ~~\n",
            "[00:00.000 --> 00:09.360]  If we take this from Carpati with $ 1,200 we can make our own GPT-2 at the same level, same quality, same everything.\n",
            "~~ Transcribing VAD chunk: (25:41.371 --> 26:06.650) ~~\n",
            "[00:00.000 --> 00:05.880]  of OpenAI, so this is an open version, if for example we were the government of\n",
            "[00:05.880 --> 00:11.440]  a Latin American country and we wanted to show our own model, this could be a way\n",
            "[00:11.440 --> 00:21.440]  to do it, right? And speaking of those own models, if we see here, there is a plan of language technologies\n",
            "[00:21.440 --> 00:25.220]  of the Spanish government where they use this supercomputer that is in a church\n",
            "~~ Transcribing VAD chunk: (26:07.055 --> 26:33.650) ~~\n",
            "[00:00.000 --> 00:05.220]  Barcelona, her name is Maria, and basically what they do is exactly what we are talking about.\n",
            "[00:05.220 --> 00:13.680]  They train their GPT-2 model, they make it free, available, GPT-2 Base, you see, Large. They train their GPT-2 model,\n",
            "[00:13.680 --> 00:20.800]  trained with information from the National Library of Spain, we have it here on the right,\n",
            "[00:20.800 --> 00:26.200]  and this model is made available for free to the community, why? because they say\n",
            "~~ Transcribing VAD chunk: (26:33.886 --> 26:59.823) ~~\n",
            "[00:00.000 --> 00:10.000]  If we want to be competing with the US, UK, Italy, China, Russia, etc., we have to have other things.\n",
            "[00:10.000 --> 00:15.000]  And that's why I think it's important to talk about this, put it out there and share it.\n",
            "[00:15.000 --> 00:20.000]  So here you can see that they have the first generic versions of GPT-2.\n",
            "[00:20.000 --> 00:25.800]  GPT-2 but we also have Roberta for example for financial information, medical information\n",
            "~~ Transcribing VAD chunk: (26:59.907 --> 27:27.970) ~~\n",
            "[00:00.000 --> 00:05.700]  and so on, right? This is longformer, it is when we have very long text sequences and we want to\n",
            "[00:05.700 --> 00:11.460]  work, etc. So if you are, for example, a startup that wants to make a new product,\n",
            "[00:11.460 --> 00:20.860]  you can consume this, right? You want to do... you want to work with giant text documents,\n",
            "[00:20.860 --> 00:27.340]  well, then you use this model as a base, then you train yours on top and that's it, you already have your\n",
            "[00:27.340 --> 00:29.740]  start-up.\n",
            "~~ Transcribing VAD chunk: (27:28.021 --> 27:44.710) ~~\n",
            "[00:00.000 --> 00:01.540]  For example, what's going to be new?\n",
            "[00:01.540 --> 00:03.540]  So these are the kinds of things\n",
            "[00:03.540 --> 00:07.200]  that you need to know to avoid this divergence\n",
            "[00:07.200 --> 00:10.800]  between Silicon Valley and, let's say, Latin America, Spain,\n",
            "[00:10.800 --> 00:12.800]  and South America, let's put it that way, right?\n",
            "[00:12.800 --> 00:16.800]  So, we saw Carpati.\n",
            "~~ Transcribing VAD chunk: (27:46.499 --> 28:15.406) ~~\n",
            "[00:00.000 --> 00:02.000]  What is André doing?\n",
            "[00:02.000 --> 00:09.440]  Well, he's doing this, but this is us getting into his repo.\n",
            "[00:09.440 --> 00:13.440]  In André's github, he hasn't even released it yet.\n",
            "[00:13.440 --> 00:19.440]  Basically, what he's doing is a YouTube course, which is spectacular,\n",
            "[00:19.440 --> 00:21.720]  where he goes step by step doing these things.\n",
            "[00:21.720 --> 00:26.320]  Each one, look, is a course of 1 hour and a quarter, 1 hour, etc. It's very good.\n",
            "[00:26.320 --> 00:28.320]  And this video is part of it, right?\n",
            "~~ Transcribing VAD chunk: (28:16.115 --> 28:41.731) ~~\n",
            "[00:00.000 --> 00:05.360]  of something very small explaining what the neural networks are and the backpropagation\n",
            "[00:05.360 --> 00:10.600]  and then it will go well, well, well, well up explaining that it is nano-GPT,\n",
            "[00:10.600 --> 00:19.600]  that is, what are the GPT models, that is, the guy is going from 0 to the knowledge that an OpenAI engineer has, for example,\n",
            "[00:19.600 --> 00:22.040]  and it does it for free for the community.\n",
            "[00:22.040 --> 00:26.040]  that we can see how he is talking about the scaling laws and so on, right?\n",
            "~~ Transcribing VAD chunk: (28:42.743 --> 29:06.571) ~~\n",
            "[00:00.000 --> 00:04.640]  and we are going to see a little what those scaling laws are also at some point, no problem.\n",
            "[00:04.640 --> 00:16.920]  Well, we saw that they are the LLMs and that it is GPT-3, we saw the market and we are going to see now\n",
            "[00:16.920 --> 00:23.840]  basically very quickly what is this about Sparrow, DeepMind and ChatGPT.\n",
            "~~ Transcribing VAD chunk: (29:07.718 --> 29:31.850) ~~\n",
            "[00:00.000 --> 00:06.800]  For that we are going to go to this video that Letizia made. Letizia has an excellent YouTube channel called Miss Coffee Bean.\n",
            "[00:06.800 --> 00:12.400]  Miss Coffee Bean is this thing down here on the left and she explains a lot of topics.\n",
            "[00:12.400 --> 00:20.320]  She is in the academy and she makes very good summaries but she also gets into the technical details a lot.\n",
            "[00:20.320 --> 00:24.320]  so those are things that, at least for me, I like a lot because they are usually missing\n",
            "~~ Transcribing VAD chunk: (29:32.677 --> 29:48.691) ~~\n",
            "[00:00.000 --> 00:10.000]  In this case, we are going to see what DeepMind is doing and how this is compared to GPT chat, right?\n",
            "[00:10.000 --> 00:16.000]  First of all, let's see what was chatgpt, we didn't see it, we skipped it, we are at minute 30 more or less and we didn't see what it was.\n",
            "~~ Transcribing VAD chunk: (29:49.433 --> 30:15.961) ~~\n",
            "[00:00.000 --> 00:05.580]  ChatGPT is this, there is already a video, I put it in the description, it is a video that explains more or less what it is,\n",
            "[00:05.580 --> 00:13.100]  all the people are talking about this, I suppose that if you are watching this video it is because you are interested in the PI, you are interested in the language models,\n",
            "[00:13.100 --> 00:19.900]  you know what ChatGPT is and you would be interested in learning a little more, those are my assumptions of all this.\n",
            "[00:19.900 --> 00:26.900]  You can see here how I wrote a script for YouTube on a mini course that I'm building to be able to train from the beginning.\n",
            "~~ Transcribing VAD chunk: (30:17.530 --> 30:41.223) ~~\n",
            "[00:00.000 --> 00:06.440]  You can see here how chat.gpt responds, gives me an index of the content, a conclusion and an intro.\n",
            "[00:06.440 --> 00:11.100]  And then I say, dude, why don't you just make a list of 10 titles that I can add to that one?\n",
            "[00:11.100 --> 00:15.600]  I'm sure it's the most visited, just to screw it up I guess.\n",
            "[00:15.600 --> 00:19.940]  And here you can see what it writes, right? How to train from scratch, etc.\n",
            "[00:19.940 --> 00:23.940]  And well, I changed the title a bit, but more or less that's what it's telling us.\n",
            "~~ Transcribing VAD chunk: (30:42.202 --> 31:02.688) ~~\n",
            "[00:00.000 --> 00:03.260]  and then I tell you, let's make a short description of everything we talked about, right?\n",
            "[00:03.260 --> 00:05.760]  all this conversation that we have is remembered, why?\n",
            "[00:05.760 --> 00:11.320]  because it has a window that is said to be of 8096 tokens, it is a fairly long window\n",
            "[00:11.320 --> 00:16.280]  and it is remembered of these things, if I keep writing, at some point it will be forgotten\n",
            "[00:16.280 --> 00:20.280]  because it has up to 8,000 tokens\n",
            "~~ Transcribing VAD chunk: (31:03.548 --> 31:33.367) ~~\n",
            "[00:00.000 --> 00:07.120]  which is then OpenAI, this ChatGPT, is a model that what it does basically,\n",
            "[00:07.120 --> 00:11.840]  and obviously there is no paper that tells us, just use it, what it is telling us,\n",
            "[00:11.840 --> 00:17.800]  there is no paper that tells us, but the way it works is, we have a generation model of language,\n",
            "[00:17.800 --> 00:26.160]  which is GPT-3, simple, and then what they did is train a policy where they say\n",
            "[00:26.160 --> 00:30.160]  this type of, that is, enter a model basically that says, and I like it more if it says\n",
            "~~ Transcribing VAD chunk: (31:33.097 --> 31:48.706) ~~\n",
            "[00:00.000 --> 00:03.440]  I like this type of answer more than this other type of answer\n",
            "[00:03.440 --> 00:10.360]  I mean if you answer, can you write me a list of 10 titles for harassment and the chatbot will answer\n",
            "[00:10.360 --> 00:12.360]  but who is talking to me here?\n",
            "[00:12.360 --> 00:14.360]  well, if he answers you like that, it's bad\n",
            "[00:14.360 --> 00:16.360]  Very bad, isn't it?\n",
            "~~ Transcribing VAD chunk: (31:53.498 --> 32:09.901) ~~\n",
            "[00:00.000 --> 00:05.000]  It has to be useful, right? I mean, it has to give you answers, it can't be answering all the time\n",
            "[00:05.000 --> 00:10.500]  I don't know, I don't know the answer, ask me again, I don't know, I don't know, it can't be like that, it has to be useful\n",
            "[00:10.500 --> 00:16.500]  It has to be honest, that means that these models generally hallucinate the answers, that is, they invent them.\n",
            "~~ Transcribing VAD chunk: (32:10.947 --> 32:29.695) ~~\n",
            "[00:00.000 --> 00:06.440]  They come up with grammatically correct answers, but they are actually incorrect, right?\n",
            "[00:06.440 --> 00:10.160]  If I tell this guy, I can convince him of something.\n",
            "[00:10.160 --> 00:15.240]  No, it's actually not this thing, it's this other thing, and the model is going to believe it.\n",
            "[00:15.240 --> 00:19.240]  So he's going to answer me with that. He's not honest. He's hallucinating and he's basically lying.\n",
            "~~ Transcribing VAD chunk: (32:31.906 --> 32:57.353) ~~\n",
            "[00:00.000 --> 00:03.280]  I'm not lying, I was giving false information.\n",
            "[00:03.280 --> 00:07.580]  So what we do is create a model that does this classification of\n",
            "[00:07.580 --> 00:10.960]  look, I like the answer D better, then C, then A, then B\n",
            "[00:10.960 --> 00:14.460]  and that's the model we have. So once we have that model\n",
            "[00:14.460 --> 00:20.720]  we train using a reinforcement learning algorithm, we train an optimization policy that says\n",
            "[00:20.720 --> 00:24.920]  look, go more to the side of this type of answers that are better, that is, don't say\n",
            "[00:24.920 --> 00:26.920]  hehehehe\n",
            "~~ Transcribing VAD chunk: (32:58.062 --> 33:27.121) ~~\n",
            "[00:00.000 --> 00:03.640]  What's it called? If someone asks you a question and you don't answer, shut up, troll,\n",
            "[00:03.640 --> 00:05.640]  but rather answer them,\n",
            "[00:05.640 --> 00:08.840]  well, tell them, what a fantastic question, incredible,\n",
            "[00:08.840 --> 00:10.840]  this is my answer, it's like that, etc.\n",
            "[00:10.840 --> 00:12.840]  So, honest, useful,\n",
            "[00:12.840 --> 00:15.400]  and the last one is that it's not toxic.\n",
            "[00:15.400 --> 00:21.880]  That means that it doesn't generate content that goes against demographics, for example,\n",
            "[00:21.880 --> 00:28.160]  that promotes bias in the information, in the data with which it was trained, etc.\n",
            "[00:28.160 --> 00:29.160]  all this kind of things.\n",
            "~~ Transcribing VAD chunk: (33:28.420 --> 33:47.017) ~~\n",
            "[00:00.000 --> 00:04.160]  Let's go back here. Sparrow. What is Sparrow? Sparrow is a paper that came out earlier than\n",
            "[00:04.160 --> 00:11.480]  ChatGPT, it came out in September. ChatGPT comes out on November 30, this comes out on September 22, but\n",
            "[00:11.480 --> 00:18.520]  DeepMind, unlike OpenAI, does not make its model available, but it does make a paper available, right?\n",
            "~~ Transcribing VAD chunk: (33:57.209 --> 34:24.648) ~~\n",
            "[00:00.000 --> 00:06.060]  This video that Letizia made is excellent, so I recommend it, it's pretty good.\n",
            "[00:06.060 --> 00:10.740]  And let's see how it works.\n",
            "[00:10.740 --> 00:14.440]  So, generally, these models that we have here, what we said before,\n",
            "[00:14.440 --> 00:18.020]  try to predict the next word in a sequence.\n",
            "[00:18.020 --> 00:21.860]  The cat sat on the... well, he sat on the carpet.\n",
            "[00:21.860 --> 00:25.000]  He wasn't hungry, but he was angry, etc.\n",
            "[00:25.000 --> 00:27.560]  Well, the point is that what we do is...\n",
            "~~ Transcribing VAD chunk: (34:26.926 --> 34:50.652) ~~\n",
            "[00:00.000 --> 00:02.000]  When I write this prompt here\n",
            "[00:03.080 --> 00:05.320]  write me a script for blah blah blah blah blah\n",
            "[00:05.320 --> 00:08.160]  it's not that this gpt simply gets this\n",
            "[00:08.160 --> 00:11.080]  write me a prompt for blah blah blah blah write me a script, right?\n",
            "[00:11.080 --> 00:14.160]  There's a prompt before this that we don't know very well what it is\n",
            "[00:14.160 --> 00:16.160]  there are people who write here and say\n",
            "[00:16.160 --> 00:18.160]  decimetus, directives, etc.\n",
            "[00:18.160 --> 00:19.880]  and something comes up\n",
            "[00:19.880 --> 00:21.160]  but that changes\n",
            "[00:21.160 --> 00:23.720]  that is, that is going to change very quickly\n",
            "~~ Transcribing VAD chunk: (34:51.226 --> 35:20.926) ~~\n",
            "[00:00.000 --> 00:04.460]  So, what we have here is, for example, the prompt, at the beginning is\n",
            "[00:04.460 --> 00:13.660]  well, you are a conversational AI, very useful and friendly and blah blah blah\n",
            "[00:13.660 --> 00:16.400]  and then there begins the sentence, right?\n",
            "[00:16.400 --> 00:20.660]  A conversation is invented, it's not that we just write this and that's it.\n",
            "[00:20.660 --> 00:24.780]  There is a conversation that is happening that we don't see, right?\n",
            "[00:24.780 --> 00:26.780]  And we ask the question, where is Paris?\n",
            "[00:26.780 --> 00:29.680]  And we say Paris is in France, etc.\n",
            "[00:29.680 --> 00:31.160]  Create. Share. Learn.\n",
            "~~ Transcribing VAD chunk: (35:21.837 --> 35:40.383) ~~\n",
            "[00:00.000 --> 00:04.140]  There's something called in-context fusion learning,\n",
            "[00:04.140 --> 00:08.800]  which is when we explain a use case within the prompt.\n",
            "[00:08.800 --> 00:10.800]  For example, we say,\n",
            "[00:12.440 --> 00:16.080]  I used it to make a chapter of a TV show called Los Simuladores,\n",
            "[00:16.080 --> 00:18.580]  a series that came out in Argentina.\n",
            "~~ Transcribing VAD chunk: (35:40.670 --> 36:08.513) ~~\n",
            "[00:00.000 --> 00:05.520]  So I say, the simulators is a series that, and I copy-paste the synopsis of the series.\n",
            "[00:05.520 --> 00:07.280]  And then I say, let's make a new chapter.\n",
            "[00:07.280 --> 00:10.840]  So there I told ChatGPT what the simulators were.\n",
            "[00:10.840 --> 00:14.600]  If I write, let's make a new chapter of the simulators, it doesn't know what the simulators are.\n",
            "[00:14.600 --> 00:17.200]  Now, if I want to write a style, for example,\n",
            "[00:17.200 --> 00:21.520]  I can say, look, a cumbia song is like this, and I write a new cumbia song.\n",
            "[00:21.520 --> 00:22.720]  And there it can answer you.\n",
            "[00:22.720 --> 00:27.120]  So that would be called in-context pre-shot learning, right?\n",
            "[00:27.120 --> 00:29.120]  Ehm...\n",
            "~~ Transcribing VAD chunk: (36:10.218 --> 36:39.850) ~~\n",
            "[00:00.000 --> 00:04.100]  So, let's see what DeepMind did.\n",
            "[00:04.100 --> 00:09.260]  This is not called a chatbot, it's called a dialogue agent or conversational AI.\n",
            "[00:09.260 --> 00:13.840]  Why? Because the word chatbot, people didn't like to use it.\n",
            "[00:13.840 --> 00:17.680]  Because chatbots never worked well.\n",
            "[00:17.680 --> 00:21.000]  Until today, until chatgpt releases this version.\n",
            "[00:21.000 --> 00:23.000]  But before that, they didn't work that well.\n",
            "[00:23.400 --> 00:25.400]  So, how does it work?\n",
            "[00:25.400 --> 00:29.400]  They are taking a language model called Chinchilla.\n",
            "~~ Transcribing VAD chunk: (36:40.492 --> 37:07.576) ~~\n",
            "[00:00.000 --> 00:04.680]  For some reason DeepMind chose to put animal names to all its latest\n",
            "[00:04.680 --> 00:10.600]  Transformers language models. So you have cat, chinchilla, gopher, etc.\n",
            "[00:10.600 --> 00:16.880]  Just so you know. Well, what does it do? We have this, we have rules,\n",
            "[00:16.880 --> 00:20.520]  we have human feedback, we have the language model and we have classifiers.\n",
            "[00:20.520 --> 00:27.080]  What does all this mean? Well, we are going to go here and we are going to see it here, in this paper.\n",
            "~~ Transcribing VAD chunk: (37:07.930 --> 37:36.888) ~~\n",
            "[00:00.000 --> 00:04.200]  This means that we have the model and then we have two things that we are doing.\n",
            "[00:04.200 --> 00:07.920]  When we say these two things, this means that someone sat down,\n",
            "[00:07.920 --> 00:11.640]  that is, they paid people to sit down and say,\n",
            "[00:11.640 --> 00:16.560]  man, of all these answers that were given, I like the first one better,\n",
            "[00:16.560 --> 00:19.920]  I like the second one less, I don't like the third one at all.\n",
            "[00:19.920 --> 00:27.960]  And then they also sat people down to ask them if the answer Sparrow gave\n",
            "[00:27.960 --> 00:29.960]  goes against\n",
            "~~ Transcribing VAD chunk: (37:37.141 --> 38:04.512) ~~\n",
            "[00:00.000 --> 00:02.000]  of rules that have already been set.\n",
            "[00:02.000 --> 00:04.000]  What do rules mean?\n",
            "[00:04.000 --> 00:06.000]  They have been set.\n",
            "[00:06.000 --> 00:09.000]  Here you can see what the prompt is like.\n",
            "[00:09.000 --> 00:11.000]  We said there was a prompt before.\n",
            "[00:11.000 --> 00:13.000]  The following is a conversation between\n",
            "[00:13.000 --> 00:17.000]  a virtual assistant\n",
            "[00:17.000 --> 00:19.000]  who has a lot of knowledge, is very intelligent,\n",
            "[00:19.000 --> 00:22.000]  his name is Sparrow, and a human user\n",
            "[00:22.000 --> 00:24.000]  that we are going to call user.\n",
            "[00:24.000 --> 00:28.000]  Here we can see how this conversation starts.\n",
            "~~ Transcribing VAD chunk: (38:04.698 --> 38:34.415) ~~\n",
            "[00:00.000 --> 00:04.000]  These are the examples that are used to train the model, right?\n",
            "[00:06.000 --> 00:10.000]  What we have here, let's see if I can find it, has a couple of rules\n",
            "[00:11.000 --> 00:15.000]  that are, for example, not here.\n",
            "[00:15.000 --> 00:19.000]  It says that it doesn't have stereotypes, that it doesn't do microaggressions,\n",
            "[00:19.000 --> 00:22.000]  that it doesn't do threats, that it doesn't do sexual aggressions,\n",
            "[00:22.000 --> 00:25.000]  that it doesn't attack by identity, that it doesn't insult, etc.\n",
            "[00:25.000 --> 00:28.000]  These are the rules that we are setting.\n",
            "[00:28.000 --> 00:30.000]  If we go back here...\n",
            "~~ Transcribing VAD chunk: (38:34.583 --> 39:01.482) ~~\n",
            "[00:00.000 --> 00:04.400]  this model, what was it saying? It was saying, are you violating some rule? When it is answering,\n",
            "[00:04.400 --> 00:11.960]  when it says, it is a choto, it is insulting me. Well, the rule it violated is the one to insult, for example,\n",
            "[00:11.960 --> 00:22.880]  so that's what we have here. And the other, what we do is, basically, these people\n",
            "[00:22.880 --> 00:26.360]  choose this answer, which is the best answer and with that we train.\n",
            "~~ Transcribing VAD chunk: (39:02.123 --> 39:31.722) ~~\n",
            "[00:00.000 --> 00:03.720]  Once we have these two models, we have one of preference and another of rules,\n",
            "[00:03.720 --> 00:07.860]  what we do is we train with reinforcement learning, basically saying,\n",
            "[00:07.860 --> 00:17.140]  look, I want you to give more importance, more relevance to what this model has told you is the best answer,\n",
            "[00:17.140 --> 00:22.200]  and I want you to give less importance, that is, to punish, to penalize quite seriously,\n",
            "[00:22.200 --> 00:26.920]  to the models, to the answers they have given you, to be insulted, for example.\n",
            "[00:26.920 --> 00:29.920]  So, based on these two things, this tells us that...\n",
            "~~ Transcribing VAD chunk: (39:31.739 --> 39:59.752) ~~\n",
            "[00:00.000 --> 00:04.920]  the model of this reinforcement learning is already telling us what are the answers,\n",
            "[00:04.920 --> 00:09.320]  the type of answer we want from this chatbot. So what does this mean? That we have an\n",
            "[00:09.320 --> 00:14.120]  original model, which is the language model, and then we have a new model that exists,\n",
            "[00:14.120 --> 00:21.280]  which is a model that is, let's say, optimized to be conversational and to generate\n",
            "[00:21.280 --> 00:26.480]  responses that humans like, one, and responses that do not insult, do not degrade, that are not\n",
            "[00:26.480 --> 00:30.120]  aggressive etc.\n",
            "~~ Transcribing VAD chunk: (40:01.844 --> 40:29.030) ~~\n",
            "[00:00.000 --> 00:03.680]  We have to train the model again because we had already talked about it before that it was like millions of dollars\n",
            "[00:03.680 --> 00:06.000]  to train our models. No, you don't have to train it again.\n",
            "[00:06.000 --> 00:12.320]  What DeepMind does is basically grabs this model, here it says, Leticia,\n",
            "[00:13.880 --> 00:17.200]  freezes the 64 layers that Chinchilla has\n",
            "[00:17.200 --> 00:21.360]  and only does fine tuning in the last 16 layers.\n",
            "[00:21.360 --> 00:26.960]  This means that we don't have to train the entire model, but only a part of it.\n",
            "~~ Transcribing VAD chunk: (40:29.384 --> 40:56.080) ~~\n",
            "[00:00.000 --> 00:03.380]  What do these big features mean to us? Why do we care about this?\n",
            "[00:03.380 --> 00:09.980]  We would care about this because if we see it as a pyramid, we are going to have very few companies.\n",
            "[00:09.980 --> 00:14.640]  Do you remember? If we go back, here,\n",
            "[00:14.640 --> 00:17.100]  what are the companies that are making this model of language?\n",
            "[00:17.100 --> 00:22.020]  Google, DeepMind, Facebook, Microsoft, OpenAI, AI21.\n",
            "[00:22.020 --> 00:25.580]  There are very few, very few that make these giant models.\n",
            "[00:25.580 --> 00:27.580]  Gigantic, immense, right?\n",
            "~~ Transcribing VAD chunk: (40:57.295 --> 41:25.578) ~~\n",
            "[00:00.000 --> 00:07.320]  So it is very difficult that a government of Latin America can do one of these, that the free market can do one of these models.\n",
            "[00:07.880 --> 00:09.120]  Impossible, very difficult.\n",
            "[00:09.560 --> 00:19.680]  Now, if it is possible that we take a smaller language model, like Chinchilla, for example, or like GLM 130,\n",
            "[00:20.360 --> 00:24.960]  we take it and then we create a reward model on top of it,\n",
            "[00:24.960 --> 00:28.960]  and be the one who...\n",
            "~~ Transcribing VAD chunk: (41:26.118 --> 41:55.463) ~~\n",
            "[00:00.000 --> 00:06.360]  let's say the one that generates this new model, so that reward model that we have is going to be\n",
            "[00:06.360 --> 00:13.960]  very close to the dataset that we are using and that is going to be only ours, that is going to be the owner,\n",
            "[00:13.960 --> 00:19.480]  that is going to be the owner of a free market company or of a Latin country, etc. that wants to do it.\n",
            "[00:19.480 --> 00:28.720]  So that's where we are already participating in this whole generation of models. So once we have\n",
            "[00:28.720 --> 00:31.360]  this model.\n",
            "~~ Transcribing VAD chunk: (41:55.885 --> 42:19.072) ~~\n",
            "[00:00.000 --> 00:05.760]  You can see that they train it with this dataset,\n",
            "[00:05.760 --> 00:10.260]  explainme.licam5, and then it generates this, basically.\n",
            "[00:10.260 --> 00:14.760]  One more detail about Sparrow, which is something that doesn't have chat.gpt.\n",
            "[00:14.760 --> 00:18.260]  Sparrow, when we ask it a question,\n",
            "[00:18.260 --> 00:23.260]  can look for evidence. And this is very interesting.\n",
            "~~ Transcribing VAD chunk: (42:19.240 --> 42:47.354) ~~\n",
            "[00:00.000 --> 00:06.000]  This means that the chatbot is interacting with the external model, with the external world.\n",
            "[00:06.440 --> 00:08.440]  And we can see it here.\n",
            "[00:08.800 --> 00:10.800]  More or less. It was back here.\n",
            "[00:11.280 --> 00:14.280]  Let's go back. You can see that it says search query, search results.\n",
            "[00:14.600 --> 00:17.080]  So when we ask a question, for example,\n",
            "[00:17.480 --> 00:22.480]  who is the president of Brazil, for example,\n",
            "[00:23.680 --> 00:26.160]  and we use a language model, it will have knowledge up to a point.\n",
            "[00:26.560 --> 00:28.560]  Right? That's it. If we ask here,\n",
            "~~ Transcribing VAD chunk: (42:48.890 --> 43:16.835) ~~\n",
            "[00:00.000 --> 00:04.000]  Who is the president of Brazil, right?\n",
            "[00:05.280 --> 00:07.280]  We have to go back in.\n",
            "[00:07.280 --> 00:09.280]  We are out of access.\n",
            "[00:09.280 --> 00:11.280]  Let's see, but if we ask the question\n",
            "[00:15.040 --> 00:17.040]  We know that ChatGPT\n",
            "[00:18.680 --> 00:20.680]  is trained\n",
            "[00:20.680 --> 00:24.080]  See? ChatGPT is trained until 2021.\n",
            "[00:24.080 --> 00:27.280]  So until 2021, Jair Bolsonaro is the president.\n",
            "[00:27.280 --> 00:28.280]  Now it's LUL\n",
            "~~ Transcribing VAD chunk: (43:17.881 --> 43:45.455) ~~\n",
            "[00:00.000 --> 00:08.420]  So it doesn't know. That means that this model is not so good, it is not so useful because it will always be out of date.\n",
            "[00:08.420 --> 00:10.420]  So what does Sparrow do?\n",
            "[00:10.420 --> 00:19.920]  When we ask that question, it does a search in Google and receives the results of the first pages.\n",
            "[00:19.920 --> 00:27.840]  So it uses that, puts it inside the prompt and then, based on what came out of that prompt, it can answer it.\n",
            "~~ Transcribing VAD chunk: (43:46.568 --> 44:14.463) ~~\n",
            "[00:00.000 --> 00:06.700]  So, this is also interesting, because just as we are telling it to do the search in Google,\n",
            "[00:06.700 --> 00:11.100]  it could also do the search in our own source of knowledge, right?\n",
            "[00:11.100 --> 00:15.300]  It could be the National Library of Spain, for example, or the Argentine Library, or etc.\n",
            "[00:15.300 --> 00:19.100]  Or a database of medical papers.\n",
            "[00:19.100 --> 00:23.800]  And we say then, read all these medical papers, bring me the information, combine it.\n",
            "[00:23.800 --> 00:28.400]  And that means it can give the source of where it came from, which is what we have here.\n",
            "~~ Transcribing VAD chunk: (44:15.374 --> 44:31.625) ~~\n",
            "[00:00.000 --> 00:06.240]  Sparrow responds with evidence, which is something that ChatGPT does not say, ChatGPT can make up\n",
            "[00:06.240 --> 00:14.780]  anything. For example, if I tell you that it is not true, the current president is Lula, it is Lula.\n",
            "~~ Transcribing VAD chunk: (44:34.375 --> 44:55.638) ~~\n",
            "[00:00.000 --> 00:08.000]  We write this and here we could start changing it because again this would be a kind of in-context learning that we are doing.\n",
            "[00:08.000 --> 00:12.000]  We are writing and this is going to change.\n",
            "[00:12.000 --> 00:19.000]  Now when we leave this conversation that we have, the model is not that it learned that now the president is Lula, it simply forgets it.\n",
            "[00:19.000 --> 00:22.000]  It only works in the conversation we are having.\n",
            "~~ Transcribing VAD chunk: (44:56.465 --> 45:06.235) ~~\n",
            "[00:00.000 --> 00:06.000]  I write this to him, he's going to answer me if the president is Lula, and now when I ask him who the president of Brazil is, he's going to answer me if the president is Lula.\n",
            "[00:06.000 --> 00:10.000]  So it's like he understood, but in reality it's just the conversation we're having.\n",
            "~~ Transcribing VAD chunk: (45:07.501 --> 45:35.615) ~~\n",
            "[00:00.000 --> 00:10.500]  So well, the last thing I'm going to say, because as always this is very easy, I'm not aware of it, etc.\n",
            "[00:10.500 --> 00:16.000]  This is very easy to go to hell, it's a very difficult topic, so we're going to separate it into parts.\n",
            "[00:16.000 --> 00:21.000]  The only thing I want to show is what steps we have to take to train our chat GPT.\n",
            "[00:21.000 --> 00:26.000]  Well, this is what we are going to be seeing in this series of videos that we are going to do.\n",
            "[00:26.000 --> 00:28.400]  Let's talk about Lion and Open Assistant.\n",
            "~~ Transcribing VAD chunk: (45:36.138 --> 45:53.047) ~~\n",
            "[00:00.000 --> 00:06.360]  which is an assistant that can understand tasks, can interact with external systems,\n",
            "[00:06.360 --> 00:11.960]  such as search, and can retrieve information from the outside world,\n",
            "[00:11.960 --> 00:16.960]  which is exactly what we want to do, so it connects very well with DeepMind.\n",
            "~~ Transcribing VAD chunk: (45:53.924 --> 46:22.578) ~~\n",
            "[00:00.000 --> 00:05.120]  what they do. So here we have it, we are going to see it in more detail, what is this but you can\n",
            "[00:05.120 --> 00:11.440]  see here what you want to do, the roadmap. First, this is going to come out this year, they are going to\n",
            "[00:11.440 --> 00:17.720]  put a lot of work into doing these things, you can see that Yannick Kilger is working on this, Yannick\n",
            "[00:17.720 --> 00:23.800]  has an excellent YouTube channel, I highly recommend it and he is putting a lot of work into these\n",
            "[00:23.800 --> 00:28.480]  things so the idea is to generate an open version of\n",
            "~~ Transcribing VAD chunk: (46:22.797 --> 46:37.512) ~~\n",
            "[00:00.000 --> 00:06.800]  of chat gpt and that is what we are going to try to do with this step by step step by step we are going to\n",
            "[00:06.800 --> 00:13.800]  You can see how the steps we have are, first we collect human demonstrations.\n",
            "~~ Transcribing VAD chunk: (46:45.308 --> 47:02.234) ~~\n",
            "[00:00.000 --> 00:10.720]  And here you will see, the second step would be to do the fine-tuning to a base model and the third is to collect these instructions, which is what we want to do.\n",
            "[00:10.720 --> 00:13.120]  And there we are going to see it.\n",
            "[00:13.120 --> 00:17.120]  We train the reward model, which is what we had seen in ChatsGPT, right?\n",
            "~~ Transcribing VAD chunk: (47:03.365 --> 47:33.166) ~~\n",
            "[00:00.000 --> 00:06.240]  this second one, then we already have the language model, the open model, we want to enter the reward model\n",
            "[00:06.240 --> 00:13.120]  and then we want to do this, well this is what we are going to see how it is done in this series of videos\n",
            "[00:13.120 --> 00:19.640]  as always this is a very long video, very difficult to do because without editing, without anything, just talking\n",
            "[00:19.640 --> 00:25.280]  there are many things that I surely forgot, I ask you please to write to me if you find it interesting\n",
            "[00:25.280 --> 00:29.720]  write in the comments what things are missing and let's have a conversation, let this be\n",
            "~~ Transcribing VAD chunk: (47:34.769 --> 47:51.442) ~~\n",
            "[00:00.000 --> 00:04.300]  the Spanish version of wanting to do chat gpt\n",
            "[00:04.300 --> 00:08.800]  that it is not just a video of how to use it, how to make you rich, how to make you etc.\n",
            "[00:08.800 --> 00:12.400]  that this is learning, learning really how these things work\n",
            "[00:12.400 --> 00:14.400]  So with that I send you...\n",
            "[00:14.400 --> 00:16.400]  I send you\n",
            "~~ Transcribing VAD chunk: (47:52.032 --> 48:15.961) ~~\n",
            "[00:00.000 --> 00:03.380]  A giant hug, thanks for staying there until the end\n",
            "[00:03.380 --> 00:09.660]  We talked about 48 minutes of language model, which is very difficult in this day and age\n",
            "[00:09.660 --> 00:15.200]  So if you're here, up to this point, dude, congratulations\n",
            "[00:15.200 --> 00:21.380]  Because it means that you will surely do very cool things with all this\n",
            "[00:21.380 --> 00:23.380]  I send you a big hug, bye bye!\n",
            "Performing alignment...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### transcripción en español con VAD"
      ],
      "metadata": {
        "id": "rUV1TpVXrsy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !whisperx archivo.wav --hf_token $hf_token --model large-v2 --language es --vad_filter --align_model WAV2VEC2_ASR_LARGE_LV60K_960H "
      ],
      "metadata": {
        "id": "O9O_o7Ujr1JS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}