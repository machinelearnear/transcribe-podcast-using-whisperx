{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "whisperx-example-youtube.ipynb",
      "authorship_tag": "ABX9TyNs8FEieYIwYgV1rceaTH3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machinelearnear/nelson-openai-master-plan/blob/main/whisperx_transcribir_podcast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§‰ machinelearnear [ğŸ“º](https://www.youtube.com/c/machinelearnear)"
      ],
      "metadata": {
        "id": "3TkBM6oS7ROk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### instalar las cosas"
      ],
      "metadata": {
        "id": "RxxNYfrT8MHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psb7KUiJ6dwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab92fee9-ac6a-4245-a857-cfde1bef872b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/m-bain/whisperx.git\n",
            "  Cloning https://github.com/m-bain/whisperx.git to /tmp/pip-req-build-2au1t540\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/m-bain/whisperx.git /tmp/pip-req-build-2au1t540\n",
            "  Resolved https://github.com/m-bain/whisperx.git to commit 809700e286a1fa40315c936876e78907be07892a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from whisperx==1.0) (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from whisperx==1.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (from whisperx==1.0) (0.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from whisperx==1.0) (4.65.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.9/dist-packages (from whisperx==1.0) (9.1.0)\n",
            "Collecting transformers>=4.19.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting pyannote.audio\n",
            "  Downloading pyannote.audio-2.1.1-py2.py3-none-any.whl (390 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m390.7/390.7 KB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soundfile in /usr/local/lib/python3.9/dist-packages (from whisperx==1.0) (0.12.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from ffmpeg-python==0.2.0->whisperx==1.0) (0.16.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisperx==1.0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisperx==1.0) (2.25.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisperx==1.0) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisperx==1.0) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisperx==1.0) (2022.6.2)\n",
            "Collecting hmmlearn<0.3,>=0.2.7\n",
            "  Downloading hmmlearn-0.2.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (217 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m217.2/217.2 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pyannote.audio->whisperx==1.0) (4.5.0)\n",
            "Collecting backports.cached-property\n",
            "  Downloading backports.cached_property-1.0.2-py3-none-any.whl (6.1 kB)\n",
            "Collecting pyannote.database<5.0,>=4.1.1\n",
            "  Downloading pyannote.database-4.1.3-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx<3.0,>=2.6\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-metric-learning<2.0,>=1.0.0\n",
            "  Downloading pytorch_metric_learning-1.7.3-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.2/112.2 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops<0.4.0,>=0.3\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Collecting speechbrain<0.6,>=0.5.12\n",
            "  Downloading speechbrain-0.5.13-py3-none-any.whl (498 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.0/499.0 KB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics<1.0,>=0.6\n",
            "  Downloading torchmetrics-0.11.3-py3-none-any.whl (518 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m518.6/518.6 KB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.core<5.0,>=4.4\n",
            "  Downloading pyannote.core-4.5-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.5/60.5 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asteroid-filterbanks<0.5,>=0.4\n",
            "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
            "Collecting torch-audiomentations>=0.11.0\n",
            "  Downloading torch_audiomentations-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.9/47.9 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting soundfile\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Collecting semver<3.0,>=2.10.2\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting singledispatchmethod\n",
            "  Downloading singledispatchmethod-1.0-py2.py3-none-any.whl (4.7 kB)\n",
            "Collecting pyannote.pipeline<3.0,>=2.3\n",
            "  Downloading pyannote.pipeline-2.3-py3-none-any.whl (30 kB)\n",
            "Collecting pytorch-lightning<1.7,>=1.5.4\n",
            "  Downloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m585.9/585.9 KB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.metrics<4.0,>=3.2\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.4/51.4 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<3.0,>=2.1\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile->whisperx==1.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile->whisperx==1.0) (2.21)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.9/dist-packages (from hmmlearn<0.3,>=0.2.7->pyannote.audio->whisperx==1.0) (1.2.1)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.9/dist-packages (from hmmlearn<0.3,>=0.2.7->pyannote.audio->whisperx==1.0) (1.10.1)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.0/117.0 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.9/dist-packages (from pyannote.core<5.0,>=4.4->pyannote.audio->whisperx==1.0) (2.4.0)\n",
            "Collecting simplejson>=3.8.1\n",
            "  Downloading simplejson-3.18.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m136.8/136.8 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer[all]>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from pyannote.database<5.0,>=4.1.1->pyannote.audio->whisperx==1.0) (0.7.0)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.9/dist-packages (from pyannote.database<5.0,>=4.1.1->pyannote.audio->whisperx==1.0) (1.3.5)\n",
            "Requirement already satisfied: sympy>=1.1 in /usr/local/lib/python3.9/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (1.7.1)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (3.5.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.9/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (0.8.10)\n",
            "Collecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting optuna>=1.4\n",
            "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m365.3/365.3 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (2023.3.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (2.11.2)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (3.19.6)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperpyyaml\n",
            "  Downloading HyperPyYAML-1.1.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from speechbrain<0.6,>=0.5.12->pyannote.audio->whisperx==1.0) (1.2.0)\n",
            "Collecting torch-pitch-shift>=1.2.2\n",
            "  Downloading torch_pitch_shift-1.2.2-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (0.8.1)\n",
            "Collecting julius<0.3,>=0.2.3\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.6/59.6 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.19.0->whisperx==1.0) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.19.0->whisperx==1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.19.0->whisperx==1.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.19.0->whisperx==1.0) (4.0.0)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (0.4.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (0.56.4)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (1.7.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (3.0.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (4.39.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (0.11.0)\n",
            "Collecting cmaes>=0.9.1\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.10.2-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna>=1.4->pyannote.pipeline<3.0,>=2.3->pyannote.audio->whisperx==1.0) (1.4.46)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.19->pyannote.database<5.0,>=4.1.1->pyannote.audio->whisperx==1.0) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.16->hmmlearn<0.3,>=0.2.7->pyannote.audio->whisperx==1.0) (3.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy>=1.1->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==1.0) (1.2.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (1.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (2.16.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (57.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (1.51.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (0.4.6)\n",
            "Collecting primePy>=1.3\n",
            "  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer[all]>=0.2.1->pyannote.database<5.0,>=4.1.1->pyannote.audio->whisperx==1.0) (8.1.3)\n",
            "Collecting shellingham<2.0.0,>=1.3.0\n",
            "  Downloading shellingham-1.5.0.post1-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting rich<13.0.0,>=10.11.0\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m237.5/237.5 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama<0.5.0,>=0.4.3\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting ruamel.yaml>=0.17.8\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m109.5/109.5 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (22.2.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (6.0.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.43.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (0.39.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==1.0) (3.1.0)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich<13.0.0,>=10.11.0->typer[all]>=0.2.1->pyannote.database<5.0,>=4.1.1->pyannote.audio->whisperx==1.0) (2.6.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (519 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m519.4/519.4 KB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna>=1.4->pyannote.pipeline<3.0,>=2.3->pyannote.audio->whisperx==1.0) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==1.0) (3.2.2)\n",
            "Building wheels for collected packages: whisperx, antlr4-python3-runtime, docopt, julius\n",
            "  Building wheel for whisperx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisperx: filename=whisperx-1.0-py3-none-any.whl size=1191196 sha256=e8e4a1b3c3a89f885680bdd564f26e7e19d08d9f214fe246b8972438fbdc0e0e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bcv21lc0/wheels/c7/67/c8/d92ee7475af476abd5e4fb00fee58ac4f2a9e333668ea3c237\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=64a758e324b990859e268d2d33467505cfe9d55d8ea4f1ea0ce5c51e43961665\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=5c5b219efe0dad7e73d6e6cf1436381e1f65f6703591f25f1a7d1471c124d4ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21895 sha256=5320899103dd2644377265586bcf593f51c59b2ea0bea99f54b4c5e2d3bc15ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/0a/a7/fc08f97438f4969d86afa7904336c2eb7eb422101359f3ad11\n",
            "Successfully built whisperx antlr4-python3-runtime docopt julius\n",
            "Installing collected packages: tokenizers, singledispatchmethod, sentencepiece, primePy, einops, docopt, commonmark, antlr4-python3-runtime, simplejson, shellingham, semver, ruamel.yaml.clib, rich, pyDeprecate, omegaconf, networkx, multidict, Mako, frozenlist, ffmpeg-python, colorlog, colorama, cmaes, charset-normalizer, backports.cached-property, async-timeout, yarl, torchmetrics, soundfile, ruamel.yaml, pyannote.core, julius, huggingface-hub, asteroid-filterbanks, alembic, aiosignal, transformers, torch-pitch-shift, pytorch-metric-learning, pyannote.database, optuna, hyperpyyaml, hmmlearn, aiohttp, torch-audiomentations, speechbrain, pyannote.pipeline, pyannote.metrics, pytorch-lightning, pyannote.audio, whisperx\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.0\n",
            "    Uninstalling networkx-3.0:\n",
            "      Successfully uninstalled networkx-3.0\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: soundfile 0.12.1\n",
            "    Uninstalling soundfile-0.12.1:\n",
            "      Successfully uninstalled soundfile-0.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/m-bain/whisperx.git;\n",
        "!python3 -m pip install -U yt-dlp;"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### hacer un download desde YouTube"
      ],
      "metadata": {
        "id": "E3Trcf6-8TJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_video = \"https://www.youtube.com/watch?v=NJYQzHVz-xI&t=513s&ab_channel=machinelearnear\""
      ],
      "metadata": {
        "id": "vKDhtw3w7vIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m yt_dlp --output \"archivo.%(ext)s\" --extract-audio --audio-format wav $youtube_video"
      ],
      "metadata": {
        "id": "cq7d7TbQ8ooG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504b7b46-33f8-4027-9b78-5b96f83c474e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=4uXeflZ8q8w\n",
            "[youtube] 4uXeflZ8q8w: Downloading webpage\n",
            "[youtube] 4uXeflZ8q8w: Downloading android player API JSON\n",
            "[info] 4uXeflZ8q8w: Downloading 1 format(s): 251\n",
            "[dashsegments] Total fragments: 4\n",
            "[download] Destination: archivo.webm\n",
            "\u001b[K[download] 100% of   38.57MiB in \u001b[1;37m00:00:11\u001b[0m at \u001b[0;32m3.42MiB/s\u001b[0m\n",
            "[ExtractAudio] Destination: archivo.wav\n",
            "Deleting original file archivo.webm (pass -k to keep)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ahora si ... correr WhisperX"
      ],
      "metadata": {
        "id": "NPe15lct8h4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = \"<aca-pone-tu-hf-token\" # https://huggingface.co/settings/tokens"
      ],
      "metadata": {
        "id": "Y8mI9SLPFiHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### transcripciÃ³n en espaÃ±ol"
      ],
      "metadata": {
        "id": "iPiU1e9FreCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisperx archivo.wav --model large-v2 --language es"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejlTXf1u7QFz",
        "outputId": "9d30035f-bf8b-4a99-9f62-26eefec15734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-09 16:45:17.386544: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-09 16:45:18.257200: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-09 16:45:18.257319: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-09 16:45:18.257339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.87G/2.87G [00:12<00:00, 248MiB/s]\n",
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_voxpopuli_base_10k_asr_es.pt\" to /root/.cache/torch/hub/checkpoints/wav2vec2_voxpopuli_base_10k_asr_es.pt\n",
            "100% 360M/360M [00:01<00:00, 231MB/s]\n",
            "Performing transcription...\n",
            "[00:00.000 --> 00:05.000]  Muy buenas de nuevo al canal, si es tu primera vez acÃ¡, lo que hacemos es divulgaciÃ³n en espaÃ±ol de lo Ãºltimo de Machine Learning\n",
            "[00:05.000 --> 00:10.000]  intentando explicar cosas difÃ­ciles de manera que sean fÃ¡ciles de entender y tambiÃ©n de usar.\n",
            "[00:10.000 --> 00:14.000]  Este capÃ­tulo de hoy va a ser el primero de una serie de vÃ­deos que querÃ­a hacer\n",
            "[00:14.000 --> 00:18.000]  para explicar desde cero cÃ³mo podemos hacer nuestro propio chat GPT.\n",
            "[00:18.000 --> 00:22.000]  Vamos a ir de cosas muy simples a muy complejas, sin esquivarle a las partes difÃ­ciles,\n",
            "[00:22.000 --> 00:28.000]  pero tratando de explicar paso a paso quÃ© es lo que estÃ¡ pasando en cada cosa.\n",
            "[00:28.000 --> 00:33.440]  AsÃ­ que como siempre vamos a dividir el vÃ­deo por partes explicando primero una motivaciÃ³n de por\n",
            "[00:33.440 --> 00:37.360]  quÃ© hacer una cosa como Ã©sta, despuÃ©s un poco quÃ© son los modelos de lenguaje, quÃ© es GPT-3,\n",
            "[00:37.360 --> 00:43.000]  cÃ³mo es el mercado, dÃ³nde estamos, explicar un poco de quÃ© estÃ¡ haciendo DeepMind, quÃ© estÃ¡\n",
            "[00:43.000 --> 00:47.960]  haciendo OpenAI y despuÃ©s quÃ© pasos tenemos que hacer para entrenar nuestro propio chat GPT.\n",
            "[00:49.200 --> 00:53.600]  Es imposible meter todos estos temas en un solo vÃ­deo, por eso voy a hacer una serie.\n",
            "[00:53.600 --> 00:59.520]  Hoy vamos a hacer una especie de introducciÃ³n y despuÃ©s, semanalmente seguro, cuando salgan,\n",
            "[00:59.520 --> 01:04.360]  digamos, irÃ© poniendo distintas cosas. Vamos a hablar de lo que se llama Reinforcement Learning\n",
            "[01:04.360 --> 01:10.080]  from Human Feedback. Vamos a hablar un poco de cÃ³mo hacerle instrucciones a los modelos de\n",
            "[01:10.080 --> 01:15.320]  lenguaje gigantes. Y vamos a hablar de una iniciativa que estÃ¡ haciendo Lion, que es el\n",
            "[01:15.320 --> 01:22.080]  grupo de gente que hizo tambiÃ©n, que es parte de las personas que hicieron Stable Diffusion y que\n",
            "[01:22.080 --> 01:27.080]  tambiÃ©n estÃ¡n haciendo una versiÃ³n abierta de ChatGPT que en teorÃ­a deberÃ­a ser aÃºn mejor que\n",
            "[01:27.080 --> 01:32.920]  ChatGPT. AsÃ­ que vamos a ver todas esas cosas en una serie de vÃ­deos. AsÃ­ que primero empecemos\n",
            "[01:32.920 --> 01:37.840]  por una motivaciÃ³n, quÃ© es lo que a mÃ­ me lleva a hacer una cosa como esto. Bueno primero que nada\n",
            "[01:37.840 --> 01:42.520]  yo creo que estaba viendo este tweet, la verdad mucho de esta conversaciÃ³n estÃ¡ pasando en\n",
            "[01:42.520 --> 01:47.640]  Twitter asÃ­ que es bastante interesante si no lo usan digamos para seguir todo lo que es la\n",
            "[01:47.640 --> 01:51.760]  parte de Machine Learning de noticias. EstÃ¡ pasando ahÃ­ la conversaciÃ³n y en otros dos tres\n",
            "[01:51.760 --> 01:57.520]  lugares pero en Twitter estÃ¡ muy concentrada y acÃ¡ por ejemplo este estÃ¡ David nos estÃ¡ diciendo que\n",
            "[01:57.520 --> 02:05.240]  estÃ¡ haciendo una analogÃ­a con el 2008 dice estamos en el 2008 hay una recesiÃ³n muy grande y acaba de\n",
            "[02:05.240 --> 02:12.240]  salir el iPhone el aÃ±o anterior el primer telÃ©fono de Android acaba de salir este aÃ±o 2008 o sea se\n",
            "[02:12.240 --> 02:15.920]  va a presentar en octubre y despuÃ©s los prÃ³ximos las prÃ³ximas tres a cinco generaciones van a\n",
            "[02:15.920 --> 02:20.680]  revolucionar absolutamente todo. Podemos decir que mÃ¡s o menos es lo que estamos teniendo acÃ¡ ahora\n",
            "[02:20.680 --> 02:26.440]  con ChatGPT, es el primer modelo, no es el Ãºltimo, es el primer modelo que estÃ¡ abriendo el espacio a\n",
            "[02:26.440 --> 02:30.840]  todas estas cosas, lo mismo que pasÃ³ con DALI 2, cuando apenas saliÃ³ DALI, una revoluciÃ³n increÃ­ble,\n",
            "[02:30.840 --> 02:34.760]  ahora DALI 2 tampoco se usa tanto, pero las versiones open source, digamos que tenemos\n",
            "[02:34.760 --> 02:41.360]  MeetJourney, tenemos todo lo que es StableDiffusion, etcÃ©tera, todo eso es infinitamente mejor ahora\n",
            "[02:41.360 --> 02:47.480]  que DALI 2, pero la razÃ³n por la que se acelerÃ³, digamos, ese desarrollo fue por la apariciÃ³n de DALI.\n",
            "[02:47.480 --> 02:56.260]  Explico un poquito de por quÃ© la motivaciÃ³n. Desde mi parte, por mi laburo, la verdad tengo\n",
            "[02:56.260 --> 03:01.300]  la suerte de poder estar trabajando mucho con modelos de lenguajes gigantes en este momento,\n",
            "[03:01.300 --> 03:05.540]  y lo que estoy viendo es una gran divergencia que se va a producir o se estÃ¡ produciendo ahora. Lo\n",
            "[03:05.540 --> 03:10.760]  voy a exagerar, voy a ir por el absurdo, vamos a decir que hay un grupo de ingenieros en Silicon\n",
            "[03:10.760 --> 03:16.580]  Valley y despuÃ©s tenemos un grupo de ingenieros en LatinoamÃ©rica. La diferencia en conocimiento,\n",
            "[03:16.580 --> 03:23.900]  en uso, en generaciÃ³n, en capacidad de poder generar cosas nuevas, de esto es enorme, es lo\n",
            "[03:23.900 --> 03:28.260]  que estoy viendo. Entonces para mÃ­ es muy importante, muy relevante que se empiecen a\n",
            "[03:28.260 --> 03:34.580]  dar conversaciones tÃ©cnicas de cÃ³mo funcionan estas cosas, que no sea solamente que no nos\n",
            "[03:34.580 --> 03:40.180]  convertamos simplemente por el espaÃ±ol, digamos, en usuarios de estas tecnologÃ­as. Tenemos que\n",
            "[03:40.180 --> 03:45.140]  tambiÃ©n ser capaces de poder generar estas tecnologÃ­as o por lo menos poder modificarlas\n",
            "[03:45.140 --> 03:51.580]  en una forma que podemos generar cosas nuevas y para hacer eso no hay otra forma que meternos\n",
            "[03:51.580 --> 03:56.540]  y meternos de lleno en estas cosas tÃ©cnicamente, ver quÃ© es lo Ãºltimo que se estÃ¡ haciendo,\n",
            "[03:56.540 --> 04:00.500]  cuÃ¡les son las conversaciones que estÃ¡n pasando y bÃ¡sicamente meterle mucho huevo.\n",
            "[04:00.500 --> 04:06.300]  Como un poquito mÃ¡s de introducciÃ³n, ahÃ­ puse que es explicado por un data scientist,\n",
            "[04:06.300 --> 04:12.420]  eso es porque laburo como data scientist, digamos, pero no es lo que estudiÃ©. Yo estudiÃ©\n",
            "[04:12.420 --> 04:16.020]  ingenierÃ­a mecÃ¡nica hace muchos aÃ±os 2016 mÃ¡s o menos empecÃ© a hacer mi\n",
            "[04:16.020 --> 04:20.620]  primer curso no sabÃ­a ni programar y dije mira me quiero meter en esto la\n",
            "[04:20.620 --> 04:24.780]  verdad me encantÃ³ como hago siempre voy a estar en una desventaja con las\n",
            "[04:24.780 --> 04:27.300]  personas que estuvieron fÃ­sica matemÃ¡tica etcÃ©tera que ya vienen\n",
            "[04:27.300 --> 04:30.740]  digamos siempre voy a estar aÃ±os atrasado entonces mi decisiÃ³n en ese\n",
            "[04:30.740 --> 04:34.020]  momento fue aunque hacer una estrategia voy a hacer un leapfrog que se llama\n",
            "[04:34.020 --> 04:38.260]  saltar voy a meterme directamente en deep learning computer vision en ese\n",
            "[04:38.260 --> 04:42.100]  caso y modelos de lenguaje natural entonces lo que quiero decir con esto es\n",
            "[04:42.100 --> 04:49.180]  Todas las tÃ©cnicas que existieron hasta este momento es posible que se tiren a la basura en\n",
            "[04:49.180 --> 04:54.460]  este momento. Â¿Es importante tener conocimiento matemÃ¡tico? SÃ­, pero con Ã¡lgebra, con un nivel\n",
            "[04:54.460 --> 04:58.660]  de Ã¡lgebra bÃ¡sico yo creo que se pueden hacer cosas muy interesantes. Con esto lo que quiero\n",
            "[04:58.660 --> 05:03.780]  decir es, no importa de dÃ³nde estÃ¡n viniendo, lo que importa es lo que quieren hacer en el futuro\n",
            "[05:03.780 --> 05:10.420]  y quÃ© tanto huevo le ponen a estudiar. AsÃ­ que hoy vamos a ponerle huevo a estudiar. Bueno,\n",
            "[05:10.420 --> 05:16.160]  vamos a ver un poco dÃ³nde estamos no con todo esto porque es importante bueno este cÃ³mic\n",
            "[05:16.160 --> 05:22.400]  espectacular bien este como dice el ojo ya te diste cuenta cÃ³mo va a impactar nuestro negocio\n",
            "[05:22.400 --> 05:26.560]  la inteligencia artificial es y no te preocupes que estoy laburando en eso y le pregunta a chat\n",
            "[05:26.560 --> 05:31.360]  gpt cÃ³mo cÃ³mo va a impactar la inteligencia artificial nuestro negocio y la verdad que\n",
            "[05:31.360 --> 05:38.980]  hay muchas maneras en la que van excelente cÃ³mic esto es algo que estÃ¡ pasando al mismo tiempo que\n",
            "[05:38.980 --> 05:43.700]  Hay otros modelos, no hablÃ© de este modelo VALI, hablamos de Whisper, hablamos de\n",
            "[05:43.700 --> 05:48.500]  ChatsGPT. VALI es un modelo que estÃ¡ sacando Microsoft tambiÃ©n, es bÃ¡sicamente un modelo\n",
            "[05:48.500 --> 05:57.580]  que hace text to speech, convierte, genera digamos voz sintetizada. Â¿CÃ³mo lo hace? Se copia de un\n",
            "[05:57.580 --> 06:03.220]  modelo de un clip de tres segundos de la persona que queremos copiar y con eso simplemente es lo\n",
            "[06:03.220 --> 06:07.140]  Ãºnico que necesita para generar audio.\n",
            "[06:07.780 --> 06:12.580]  Entonces, si juntamos estas cosas, esto, un detalle al margen, en Bali me hace\n",
            "[06:12.580 --> 06:16.900]  acordar mucho a la pelÃ­cula Terminator 2, cuando llama, digamos, Terminator, Â¿quÃ©\n",
            "[06:16.900 --> 06:21.300]  pasÃ³ con el perro? Le cambia el nombre y dice tu perro estÃ¡ muerto. Bueno, en ese\n",
            "[06:21.300 --> 06:25.580]  momento a mÃ­ me sorprendiÃ³ muchÃ­simo esa escena. Bueno, podemos ver que Bali es\n",
            "[06:25.580 --> 06:30.060]  algo parecido a eso. Bueno, Â¿quÃ© tenemos acÃ¡? Un modelo\n",
            "[06:30.060 --> 06:36.860]  conversacional Â¿no? o sea Siri, Alexa, ya estÃ¡, esto le compite directamente a estas dos cosas Â¿no?\n",
            "[06:36.860 --> 06:44.180]  AsÃ­ que tenemos Whisper que convierte de digamos audio a texto, despuÃ©s ChatGPT que convierte el\n",
            "[06:44.180 --> 06:50.300]  texto en algo mÃ¡s, no sabemos quÃ© todavÃ­a, en algo y despuÃ©s Valley que lo convierte otra vez en\n",
            "[06:50.300 --> 06:56.980]  en audio Â¿no? asÃ­ que tenemos una conversaciÃ³n, en este caso dice prender las luces de la fiesta\n",
            "[06:56.980 --> 07:04.100]  y empieza a poner mÃºsica que estÃ© buena a las 8 de la tarde todos los jueves, pero podemos ver cÃ³mo\n",
            "[07:04.100 --> 07:10.020]  ya empiezan a aparecer cosas muy nuevas y esto es peligroso no solamente para digamos nosotros\n",
            "[07:10.020 --> 07:14.220]  podemos decir esto bueno algunas cosas estÃ¡n buenas otras cosas son peligrosas pero las\n",
            "[07:14.220 --> 07:22.220]  empresas por ejemplo Canva es una empresa de diseÃ±o no es una empresa de diseÃ±o que compite con compite\n",
            "[07:22.220 --> 07:31.140]  con adobe pero una cosa que tiene que tiene canva es que iban a salir al mercado ahora o sea pedir\n",
            "[07:31.140 --> 07:38.900]  grita al mercado y los tipos dijeron bueno vamos a tener que contratar a chat gpt o sea no a gpt3\n",
            "[07:38.900 --> 07:43.300]  por lo menos porque no podemos competir con ellos no tenemos las capacidades para competir con estas\n",
            "[07:43.300 --> 07:49.820]  entonces se pueden imaginar como las empresas ya o sea se estÃ¡ generando un sistema donde o estÃ¡s\n",
            "[07:49.820 --> 07:57.020]  conmigo o te voy a reventar bÃ¡sicamente eso entonces es lo que lo que estÃ¡ pasando y nadie\n",
            "[07:57.020 --> 08:01.300]  estÃ¡ seguro en este sentido no es solamente empresa de tecnologÃ­a sino cualquier tipo de empresa no\n",
            "[08:01.300 --> 08:07.060]  una empresa de soporte tÃ©cnico por ejemplo no tiene razÃ³n para no sentirse amenazada por algo\n",
            "[08:07.060 --> 08:11.940]  como un chat gpt esto es como apple metiÃ©ndose en el mercado automotriz o sea estÃ¡ bien tenemos\n",
            "[08:11.940 --> 08:16.020]  voz bag, tenemos ford, chevrolet, etcÃ©tera pero cuando entre apple va a revolucionar todo eso\n",
            "[08:16.020 --> 08:19.300]  asÃ­ que no sabemos quÃ© es lo que va a pasar.\n",
            "[08:19.300 --> 08:24.100]  AsÃ­ que bueno, vamos a ver un poquito quÃ© son los modelos de lenguaje y quÃ© es\n",
            "[08:24.100 --> 08:27.620]  GPT-3. Vamos a empezar muy despacito.\n",
            "[08:27.620 --> 08:31.300]  Primero acÃ¡ hay un, obviamente,\n",
            "[08:31.300 --> 08:35.700]  yo soy muy vago, asÃ­ que no edito los vÃ­deos, los grabo de una. Es bastante\n",
            "[08:35.700 --> 08:38.620]  difÃ­cil hacer esto, asÃ­ que quiere decir que voy a probablemente equivocarme\n",
            "[08:38.620 --> 08:42.060]  bastante. A medida que voy me voy a olvidar cosas, seguramente me voy a equivocar en\n",
            "[08:42.060 --> 08:48.180]  en otras. Les pido si hay algo que me pasÃ© o que no se entendiÃ³ o que expliquÃ© para el orto,\n",
            "[08:48.180 --> 08:53.460]  es muy probable que lo pongan en los comentarios y yo despuÃ©s lo volvemos a revisar para la prÃ³xima.\n",
            "[08:54.460 --> 08:59.620]  Todos los links que pongo acÃ¡ los voy a poner en la descripciÃ³n del vÃ­deo. Mi idea con esto es no\n",
            "[08:59.620 --> 09:05.780]  hacer un vÃ­deo cortito, malo, etcÃ©tera, resumido, con vÃ­deos, con grÃ¡fica, etcÃ©tera. No, no. Esto\n",
            "[09:05.780 --> 09:12.860]  es un vÃ­deo duro para verlo con el tiempo, o sea volver y decir que dijo este este el loco este en\n",
            "[09:12.860 --> 09:19.260]  tal lugar, bueno dijo tal cosa, esa es la idea. Este link es un curso que acaba de salir, se llama CS324\n",
            "[09:19.260 --> 09:25.780]  de la universidad Stanford, es un curso de modelos de lenguaje, estÃ¡ bastante bueno y nos sirve como\n",
            "[09:25.780 --> 09:32.660]  introducciÃ³n al tema. Â¿QuÃ© es un modelo de lenguaje? Bueno la definiciÃ³n clÃ¡sica que tenemos de un\n",
            "[09:32.660 --> 09:37.500]  mÃ³dulo del lenguaje es la distribuciÃ³n de probabilidades sobre una secuencia de tokens Â¿quÃ©\n",
            "[09:37.500 --> 09:46.460]  significa esto? significa que por ejemplo si decimos acÃ¡ el ratÃ³n se comiÃ³ el queso, esto tiene una\n",
            "[09:46.460 --> 09:54.560]  probabilidad de suceder Â¿no? de ocurrencia de 0,02 excelente ahora si decimos el queso se comiÃ³ el\n",
            "[09:54.560 --> 10:05.480]  ratÃ³n y no tiene una una probabilidad de ocurrencia menor 0,01 y si tenemos ratÃ³n el queso comiÃ³ una\n",
            "[10:05.480 --> 10:13.880]  probabilidad de ocurrencia bajÃ­sima Â¿no? y Â¿cÃ³mo es que se calcula esta probabilidad de que pase?\n",
            "[10:13.880 --> 10:21.840]  bueno esto significa que el modelo que estamos utilizando tiene habilidades lingÃ¼Ã­sticas pero\n",
            "[10:21.840 --> 10:26.520]  tambiÃ©n tiene conocimiento del mundo, por ejemplo eso significa que gramÃ¡ticamente quizÃ¡s estas dos\n",
            "[10:26.520 --> 10:34.600]  oraciones estÃ¡n correctas, son correctas, pero no se usan normalmente, nunca pasarÃ­a algo asÃ­ o es\n",
            "[10:34.600 --> 10:39.560]  poco probable, es menos probable, la mitad de probable que suceda esta segunda situaciÃ³n comparada\n",
            "[10:39.560 --> 10:44.240]  con la primera situaciÃ³n. Entonces lo que podemos decir es que estos modelos de lenguaje tienen dos\n",
            "[10:44.240 --> 10:49.480]  cosas, en general tienen conocimiento sintÃ¡ctico de gramÃ¡tica y tambiÃ©n tienen conocimiento de\n",
            "[10:49.480 --> 10:53.140]  el mundo. Â¿CÃ³mo es que tienen conocimiento del mundo? Bueno, porque por ejemplo para\n",
            "[10:53.140 --> 10:57.460]  entrenar estas cosas le metemos la wikipedia entera y en espaÃ±ol o en\n",
            "[10:57.460 --> 11:00.420]  ingles o etcÃ©tera, le metemos la wikipedia entera y ahÃ­ tiene ejemplos\n",
            "[11:00.420 --> 11:06.420]  donde se leyÃ³ este tipo de frases. Entonces, por ejemplo, esto ocurriÃ³,\n",
            "[11:06.420 --> 11:09.780]  tuvo dos ocurrencias, esto tuvo una ocurrencia, listo. Entonces el doble de\n",
            "[11:09.780 --> 11:13.140]  probable que una cosa asÃ­, mÃ¡s o menos.\n",
            "[11:13.140 --> 11:17.460]  Y etcÃ©tera. EstÃ¡ explicado acÃ¡. No me voy a super meter con este tema de cÃ³mo es\n",
            "[11:17.460 --> 11:22.140]  porque la verdad que lo hacen muy bien en este curso, asÃ­ que se los recomiendo. AcÃ¡ pueden\n",
            "[11:22.140 --> 11:29.660]  ver las, cuÃ¡les son las capacidades que pueden tener tambiÃ©n con gpt3, etcÃ©tera y van viendo\n",
            "[11:29.660 --> 11:34.500]  todas estas cosas. AsÃ­ que no me voy a meter mucho con esto. TambiÃ©n se explica la perplexity,\n",
            "[11:34.500 --> 11:42.700]  que es una mÃ©trica que se usa para saber quÃ© tan bueno es el modelo, Â¿no? Â¿CÃ³mo se entrenan\n",
            "[11:42.700 --> 11:46.860]  entonces estos modelos de lenguaje? Â¿QuÃ© es lo que hacen? Ya dijimos, devuelve una probabilidad\n",
            "[11:46.860 --> 11:51.980]  de una secuencia de tokens, bueno para lo que se entrenan estos modelos son modelos generativos,\n",
            "[11:51.980 --> 11:56.620]  entonces lo que significa es que se le normalmente lo que pasa es se le da una secuencia de tokens,\n",
            "[11:56.620 --> 12:03.300]  ponele otra vez el ratÃ³n se comiÃ³ el, el quÃ©, bueno y tiene que predecir en base a esos tres\n",
            "[12:03.300 --> 12:09.820]  tokens, cuatro tokens que tenÃ­a antes, el ratÃ³n se comiÃ³, bueno tiene que predecir cuÃ¡l es la\n",
            "[12:09.820 --> 12:14.980]  prÃ³xima, el prÃ³ximo token que va a venir despuÃ©s de eso, y eso dice el queso con una alta probabilidad\n",
            "[12:14.980 --> 12:21.660]  y asÃ­ sucesivamente. Lo vimos en otros vÃ­deos, tambiÃ©n los voy a poner en el chat, pero bÃ¡sicamente\n",
            "[12:21.660 --> 12:27.340]  tambiÃ©n hay una cosa que se llama ngrams Â¿no? Entonces quizÃ¡s no necesita simplemente por\n",
            "[12:27.340 --> 12:33.620]  ejemplo predecir un token, sino quizÃ¡s va a predecir varios tokens Â¿no? Entonces con esto\n",
            "[12:33.620 --> 12:40.060]  se pueden generar cosas que son mÃ¡s plausibles Â¿no? que son mÃ¡s probables de suceder. Dos tokens,\n",
            "[12:40.060 --> 12:47.860]  tres tokens y hay formas distintas de elegir estos tokens, no? greedy search, beam, etcÃ©tera.\n",
            "[12:49.460 --> 12:54.580]  AcÃ¡ pueden ver que cuando se entrenan estos modelos una cosa que se puede hacer es,\n",
            "[12:54.580 --> 13:03.900]  se puede escribir y decirle predecime el prÃ³ximo token y la forma que se hace esto es, porque yo\n",
            "[13:03.900 --> 13:09.220]  ya tengo la wikipedia, no? entonces yo simplemente lo que puedo hacer es corto la oraciÃ³n y digo el\n",
            "[13:09.220 --> 13:14.920]  token que falta es este, entonces el modelo aprende muy bien porque dice yo predije chorizo,\n",
            "[13:14.920 --> 13:20.920]  no hermano chorizo no es, queso, queso, no tengo la respuesta acÃ¡, queso, no, no, estÃ¡ mal, arreglalo.\n",
            "[13:20.920 --> 13:25.560]  Entonces ahÃ­ va aprendiendo a medida que lo vamos entrenando. Otra forma de hacerlo en vez de predecir\n",
            "[13:25.560 --> 13:30.960]  el prÃ³ximo token es cuando le damos un espacio en blanco que es lo que tenemos acÃ¡, es bÃ¡sicamente\n",
            "[13:30.960 --> 13:36.840]  lo que estamos diciendo es, mira llename el espacio en blanco, no significa que sea el Ãºltimo token\n",
            "[13:36.840 --> 13:42.720]  pero sÃ­, quizÃ¡s es un token intermedio. Esto es lo que lo que hace el modelo. Bueno,\n",
            "[13:44.240 --> 13:51.600]  estos son modelos de lenguaje de generaciÃ³n, pero no son modelos gigantes de lenguaje. Vamos a ver\n",
            "[13:51.600 --> 13:57.880]  quÃ© son los modelos gigantes de lenguaje. Bueno, estos son modelos que estÃ¡n entrenados con una\n",
            "[13:57.880 --> 14:02.840]  cantidad estÃºpida de data y que pueden hacer, que fueron adaptados para hacer una cantidad\n",
            "[14:02.840 --> 14:09.840]  estÃºpida de tasks bÃ¡sicamente ya que se llaman lo que serÃ­a cero shot significa que nosotros el dÃ­a\n",
            "[14:09.840 --> 14:14.600]  de maÃ±ana le preguntamos bueno escribir una canciÃ³n de cumbia pero nunca se entrenÃ³ para escribir una\n",
            "[14:14.600 --> 14:19.880]  canciÃ³n de cumbia pero te lo puede hacer igual pero si fuera one shot por ejemplo es que tiene\n",
            "[14:19.880 --> 14:24.960]  un ejemplo cuando cuando lo estamos haciendo le damos un ejemplo y despuÃ©s le decimos ahora esto\n",
            "[14:24.960 --> 14:30.880]  es una canciÃ³n de cumbia escribimos una canciÃ³n nuevo one shot supervise learning serÃ­a que lo\n",
            "[14:30.880 --> 14:35.360]  entrenamos solamente con un dataset de canciones de cumbia y le decimos hacerme una nueva que tenga\n",
            "[14:35.360 --> 14:40.240]  este formato y es lo que estÃ¡ pasando con estos modelos de lenguaje que tenemos acÃ¡\n",
            "[14:42.280 --> 14:48.360]  les estÃ¡ yendo mejor que los humanos en muchas cosas la lÃ­nea gris que tienen acÃ¡ son los humanos\n",
            "[14:48.360 --> 14:53.840]  y pueden ver que ya los estamos pasando o sea hubo un momento donde los modelos ya empezaron\n",
            "[14:53.840 --> 15:02.840]  a ser mejores que humanos. AcÃ¡ pueden ver la performance en 58 tasks y el tema que tienen\n",
            "[15:02.840 --> 15:10.160]  es que esto se llama, acÃ¡ abajo pueden ver lo que son, aparecen los parÃ¡metros y arriba estÃ¡n\n",
            "[15:10.160 --> 15:15.280]  viendo la performance. Los parÃ¡metros de abajo, lo que se dieron cuenta es que cuando subÃ­amos los\n",
            "[15:15.280 --> 15:19.760]  parÃ¡metros, que esto fue lo que pasÃ³ con gpt3, metieron una cantidad estÃºpida de parÃ¡metros,\n",
            "[15:19.760 --> 15:25.920]  empezaron a notar una cosa que se llama emergencia, no emergencia de estÃ¡ pasando algo, se estÃ¡\n",
            "[15:25.920 --> 15:31.240]  prendiendo fuego la casa, sino de habilidades emergentes de estos modelos y esto al dÃ­a de hoy\n",
            "[15:31.240 --> 15:37.120]  no se sabe por quÃ© sucede. Hay aproximaciones pero no se sabe todavÃ­a quÃ© es. Conciencia no lo sabemos\n",
            "[15:37.120 --> 15:42.920]  todavÃ­a, pero lo que sÃ­ se sabe es que cuando se llega a una cantidad de parÃ¡metros muy grande\n",
            "[15:42.920 --> 15:49.600]  estos modelos empiezan a ser muchÃ­simo mejor que antes, o sea ya el aumento de performance no\n",
            "[15:49.600 --> 15:54.880]  es linear sino que se vuelve exponencial que es lo que vemos acÃ¡, acÃ¡ pueden ver cÃ³mo pega un salto\n",
            "[15:54.880 --> 15:59.520]  del triple de performance cuando empezamos a aumentar la cantidad de parÃ¡metros.\n",
            "[16:02.920 --> 16:06.480]  AcÃ¡ podemos ver cÃ³mo es la evoluciÃ³n a travÃ©s de los aÃ±os, tenÃ­amos estadÃ­stica,\n",
            "[16:06.480 --> 16:09.120]  tenÃ­amos Machine Learning, tenÃ­amos Deep Learning y ahora tenemos lo que se llama\n",
            "[16:09.120 --> 16:15.600]  Foundation Models. Â¿QuÃ© significan estos Foundation Models? Bueno son modelos gigantes,\n",
            "[16:15.600 --> 16:22.640]  podemos decir que es una, no es en este caso, pero podemos decir que estamos yendo en direcciÃ³n a una\n",
            "[16:22.640 --> 16:26.840]  inteligencia muy grande, digamos, que le podemos preguntar sobre cualquier cosa y nos pueda responder\n",
            "[16:26.840 --> 16:34.200]  sobre esas cosas. Estos son los datas que se usan, pueden ser de cualquier cosa, libros, cÃ³digo,\n",
            "[16:34.200 --> 16:42.640]  reddit, sitios mÃ©dicos, sitios, o sea, papers de medicina, patentes, etcÃ©tera, o sea, todo lo que\n",
            "[16:42.640 --> 16:48.720]  tengamos generado como humanidad, es lo que le podemos meter a estos modelos como training data\n",
            "[16:48.720 --> 16:53.560]  y estas son las cosas que pueden hacer estos modelos. Pueden sumarizar, pueden generar\n",
            "[16:53.560 --> 17:00.960]  imÃ¡genes, pueden clasificar el texto, traducir, chatear, crear recetas, explicar cÃ³digo, muchas\n",
            "[17:00.960 --> 17:07.360]  muchas cosas. Y acÃ¡ empezamos a ver un poco quÃ© fueron las cosas que hicieron que esta revoluciÃ³n\n",
            "[17:07.360 --> 17:14.800]  pueda suceder. Primero, la arquitectura de Transformers que se hizo en el aÃ±o, o sea,\n",
            "[17:14.800 --> 17:20.920]  el paper de Attention Is All You Need, se hizo en el aÃ±o 2017 y todas las personas que hicieron\n",
            "[17:20.920 --> 17:26.800]  ese paper, excepto una persona, se fueron de Google, lo hizo Google ese paper, y se fueron\n",
            "[17:26.800 --> 17:33.240]  a hacer sus startups o a OpenAI, etcÃ©tera. AquÃ­ pueden ver lo importante que fue eso.\n",
            "[17:33.240 --> 17:40.400]  acÃ¡ tenemos lo que es la evoluciÃ³n de la cantidad de cÃ³mputo que se utilizÃ³ para\n",
            "[17:40.400 --> 17:43.440]  entrenar estos modelos, o sea, estÃ¡ pegando una exponencial a medida que nos vamos acercando,\n",
            "[17:43.440 --> 17:47.760]  eso quiere decir que entrenar estos modelos sale millones de dÃ³lares y podemos ver acÃ¡,\n",
            "[17:47.760 --> 17:53.600]  bueno, quiÃ©n domina la risa, etcÃ©tera. O sea, esto es bÃ¡sicamente lo que estoy diciendo,\n",
            "[17:53.600 --> 17:58.640]  que las grandes empresas son las que tienen dominio sobre estas cosas y eso no es bueno,\n",
            "[17:58.640 --> 18:04.480]  o sea, deberÃ­a estar bastante distribuido. Industria, comunidad open source, gobiernos,\n",
            "[18:04.480 --> 18:12.280]  estado. Tiene que estar distribuido por lo menos de manera igualitaria. Estos son los modelos que\n",
            "[18:12.280 --> 18:17.560]  tenemos disponibles al dÃ­a de hoy, o sea, pueden ver que estos modelos gigantes, acÃ¡ tenemos los\n",
            "[18:17.560 --> 18:23.960]  parÃ¡metros 540.000, 130.000, 80.000, 20.000, etcÃ©tera, 180.000, etcÃ©tera. Pueden ver que las\n",
            "[18:23.960 --> 18:31.520]  empresas que lo hacen son Google, Google con DeepMind, Facebook, Meta, Microsoft, OpenAI y Big\n",
            "[18:31.520 --> 18:37.480]  Science que es un modelo abierto y despuÃ©s AI21 es una empresa israelÃ­ que estÃ¡ haciendo\n",
            "[18:37.480 --> 18:45.880]  esto. Bueno y etcÃ©tera, mapas y so on. Una cosa que es interesante, esto es muy bueno, se lo\n",
            "[18:45.880 --> 18:51.960]  recomiendo para verlo, se llama el estado de AI, reporte 2022, estÃ¡ hecho por Nathan B.\n",
            "[18:51.960 --> 19:00.040]  Van Eyck y Ian Hogarth. Ambos dos estÃ¡n en lo que es Venture Capital y la verdad que hacen un anÃ¡lisis\n",
            "[19:00.040 --> 19:04.200]  excelente. Hace varios aÃ±os lo vienen haciendo. Pueden ver acÃ¡ abajo el 2018 que lo vienen haciendo\n",
            "[19:04.200 --> 19:12.240]  y es espectacular. O sea, no hay resumen mejor en internet que el que hacen estas personas. Bueno,\n",
            "[19:12.240 --> 19:19.760]  Â¿quÃ© es lo que hicieron acÃ¡? AcÃ¡ podemos ver que estÃ¡n charlando un poco de esto de dÃ³nde estamos\n",
            "[19:19.760 --> 19:24.160]  en este momento bueno cinco aÃ±os despuÃ©s de que haya salido el transformers todavÃ­a sigue siendo\n",
            "[19:24.160 --> 19:28.520]  la mejor arquitectura siempre sale alguna alguna cosa nueva que trata de modificar esto lo otro\n",
            "[19:28.520 --> 19:32.520]  lo otro etcÃ©tera pero la verdad es que no hay vuelta que darle es la mejor arquitectura es la\n",
            "[19:32.520 --> 19:39.680]  que mejor escala la que mejor genera este tipo de cosas digamos y al final de cuentas lo que se hace\n",
            "[19:39.680 --> 19:45.880]  es simplemente se entrena por mÃ¡s tiempo y se tienen mejor resultado porque mÃ¡s o menos no\n",
            "[19:45.880 --> 19:53.520]  sabemos, pero generalmente lo que pasa es eso, asÃ­ que por quÃ© no, en vez de pasar aÃ±os, por quÃ© no\n",
            "[19:53.520 --> 19:58.560]  entrenamos directamente y vemos quÃ© onda. Bueno, asÃ­ es como estÃ¡ la research en machine learning en el\n",
            "[19:58.560 --> 20:08.520]  dÃ­a de hoy. Pueden ver acÃ¡ varias varias estas cosas, esto es lo que decÃ­amos del tema de la\n",
            "[20:08.520 --> 20:15.200]  emergencia, o sea estÃ¡ esto emergence y pueden ver acÃ¡ como a medida que llegamos a un nivel de\n",
            "[20:15.200 --> 20:21.040]  compute pega una disparada espectacular o sea es increÃ­ble cÃ³mo los modelos mejoran muchÃ­simo se\n",
            "[20:21.040 --> 20:26.240]  van al triple al cuÃ¡druple de performance cuando hacen esto asÃ­ que vamos a esperar que por ejemplo\n",
            "[20:26.240 --> 20:32.840]  exagerando no gpt4 pueda tener alguna de estas caracterÃ­sticas tambiÃ©n asÃ­ que estuvimos\n",
            "[20:32.840 --> 20:38.960]  hablando de bueno hay otras cosas se lo recomiendo esto para ver esto pueden ver acÃ¡ cuando apenas\n",
            "[20:38.960 --> 20:45.900]  saliÃ³ gpt3 2020 pueden ver cÃ³mo pasÃ³ un tiempo se estaban todos digamos en pelota hasta que mÃ¡s o\n",
            "[20:45.900 --> 20:52.220]  menos acÃ¡ empezaron a salir modelos abiertos gpt, jota, el oiter.ai y ahora van a ver por quÃ© es\n",
            "[20:52.220 --> 21:00.540]  importante el oiter.ai que laburaron con lion, carper.ai, stability.ai, todos parte del mismo\n",
            "[21:00.540 --> 21:07.820]  conjunto para hacer open source a toda esta ciencia es increÃ­ble el laburo que hacen. Bueno\n",
            "[21:07.820 --> 21:13.940]  Pueden ver cÃ³mo pasÃ³ por lo menos un aÃ±o, casi, hasta que empezaron a salir todas estas\n",
            "[21:13.940 --> 21:21.100]  versiones open source, Â¿no? AsÃ­ que, asÃ­ como saliÃ³ open chat gpt ahora, podemos darle unos\n",
            "[21:21.100 --> 21:25.260]  meses, quizÃ¡s un aÃ±o, y vamos a empezar a tener esas cosas. Lo mismo con DALI.\n",
            "[21:26.900 --> 21:30.620]  Un poquito mÃ¡s difÃ­cil es toda la parte de alfafolia, etcÃ©tera. Bueno,\n",
            "[21:30.620 --> 21:41.900]  Con esto, lo Ãºnico que querÃ­a mostrar, quizÃ¡s acÃ¡ interesante, es que, bueno, para entrenar esto,\n",
            "[21:41.900 --> 21:47.100]  se necesitan muchas GPUs, asÃ­ que estas son las GPUs que estÃ¡n disponibles, las tienen muy pocas\n",
            "[21:47.100 --> 21:52.700]  empresas en el mundo, acÃ¡ pueden tenerlo como, lo que estÃ¡ en naranja son laboratorios nacionales,\n",
            "[21:52.700 --> 21:58.540]  generalmente estÃ¡n en Estados Unidos o en Europa, lo que estÃ¡ en azul son las empresas particulares\n",
            "[21:58.540 --> 22:07.260]  y lo que estÃ¡ en rojo en rojo son digamos Amazon por ejemplo, pero acÃ¡, esto es el paper original,\n",
            "[22:07.260 --> 22:13.700]  les querÃ­a mostrar, attention is all you need, pedazo de paper se armaron estos tipos, bueno esta\n",
            "[22:13.700 --> 22:19.540]  gente que estÃ¡ acÃ¡ pueden ver acÃ¡ las empresas que salieron y fundaron, todas estas multimillonarias\n",
            "[22:19.540 --> 22:27.740]  lo pueden ver acÃ¡, unicorn en algunos casos de lo que hicieron, bueno hablamos de transformer,\n",
            "[22:27.740 --> 22:34.780]  hablamos de OpenAI y vamos a ver quÃ© es esto de GPT. Â¿QuÃ© es GPT? G, la G, la P y la T. La G,\n",
            "[22:34.780 --> 22:44.220]  generativo. P, pre-trained, T, transformers. Â¿QuÃ© es entonces GPT-3? Es bÃ¡sicamente un modelo de\n",
            "[22:44.220 --> 22:49.620]  transformers que se usa para generar texto y que fue entrenado con una cantidad de data gigante.\n",
            "[22:49.620 --> 22:59.940]  eso es eso es a grandes rasgos lo que es GPT-3. Cuando hablamos de un modelo Â¿no? o sea Â¿quÃ©\n",
            "[22:59.940 --> 23:07.260]  modelo es el que estamos hablando? Bueno vamos a ir acÃ¡ y vamos a ver esto que hizo AndrÃ© Carpati.\n",
            "[23:07.260 --> 23:14.140]  AndrÃ© Carpati es un tipo que era el director de Machine Learning, o sea el director de OpenAI\n",
            "[23:14.140 --> 23:19.940]  cuando apenas empezÃ³ que Elon Musk le habÃ­a puesto plata, si se acuerdan era le habÃ­a puesto mil\n",
            "[23:19.940 --> 23:26.300]  millones de dÃ³lares mÃ¡s o menos para arrancar la idea de OpenAI era que sea abierto. Entonces habÃ­a\n",
            "[23:26.300 --> 23:33.340]  cuando arrancÃ³ Karpati como director de AI despuÃ©s de ahÃ­ se fue a ser director de AI en Tesla,\n",
            "[23:33.340 --> 23:39.700]  fue la persona que fue responsable de toda la parte de Autonomous Driving de Tesla y hace\n",
            "[23:39.700 --> 23:45.460]  unos meses pinchÃ³ las bolas dijo yo no quiero mÃ¡s estar con esto quiero hacer vÃ­deo de youtube quiero\n",
            "[23:45.460 --> 23:51.340]  hacer cÃ³digo quiero pasarla bien tocar la guitarra etcÃ©tera y gracias a dios porque entre las cosas\n",
            "[23:51.340 --> 23:59.500]  que hizo es por ejemplo esta implementaciÃ³n de gpt no se mandÃ³ lo podemos ver a andrÃ© capo capo\n",
            "[23:59.500 --> 24:05.260]  andrÃ© bueno acÃ¡ pueden ver las implementaciones actuales son gigantes gigante sÃºper larga\n",
            "[24:05.260 --> 24:11.260]  imposible de correr, etcÃ©tera. Este tipo dijo, acÃ¡ hay que hacer una que se llame nano gpt, que\n",
            "[24:11.260 --> 24:17.300]  sean 300 lÃ­neas de cÃ³digo, Â¿no? que tenga, que sean un par de archivos nada mÃ¡s, que es este,\n",
            "[24:17.300 --> 24:23.300]  modelo.py y train.py, ya estÃ¡, dos archivos, nada mÃ¡s, el resto son como para ayudar a hacer esto,\n",
            "[24:23.300 --> 24:30.060]  pero dos archivos y con eso ya estÃ¡, ya con eso podÃ©s tener gpt2, que es lo que tenemos acÃ¡.\n",
            "[24:30.060 --> 24:36.680]  Entonces, usando este cÃ³digo, bÃ¡sicamente ya podemos entrenar GPT-2. AcÃ¡ en este caso,\n",
            "[24:36.680 --> 24:44.760]  este tipo usa un dataset que se llama OpenWebText y esto es lo que usa para entrenar el modelo que\n",
            "[24:44.760 --> 24:56.320]  vemos acÃ¡. Lo entrena con una sola instancia de 8 GPUs de NVIDIA a 100 por 38 horas. Si se acuerdan,\n",
            "[24:56.320 --> 25:06.160]  vamos a ir acÃ¡ cloud si ven acÃ¡ dijimos que esa es Ã©sta no es una p4d sale 32 mango si ponemos\n",
            "[25:06.160 --> 25:14.880]  32 mango por 38 nos da 1200 dÃ³lares bueno eso quiere decir que si agarramos esto de carpati\n",
            "[25:14.880 --> 25:22.680]  con 1200 dÃ³lares nos podemos hacer nuestro propio gpt2 al mismo nivel misma calidad mismo todo\n",
            "[25:22.680 --> 25:29.800]  espectacular gpt3 es muy distinto de gpt2 no es lo mismo en algunas optimizaciones nada mÃ¡s pero\n",
            "[25:29.800 --> 25:34.840]  estÃ¡ basado en la misma arquitectura obviamente acuÃ©rdense gpt3 no sale o sea no tenemos el cÃ³digo\n",
            "[25:34.840 --> 25:40.000]  no sabemos exactamente cÃ³mo fue un paper sabemos mÃ¡s o menos que hicieron pero el cÃ³digo es\n",
            "[25:40.000 --> 25:46.600]  propietario de OpenAI asÃ­ que esta es una versiÃ³n abierta si por ejemplo nosotros fuÃ©ramos el\n",
            "[25:46.600 --> 25:52.480]  gobierno de paÃ­s x de latinoamÃ©rica y quisiÃ©ramos tener nuestro propio modelo esto puede ser una\n",
            "[25:52.480 --> 25:58.360]  forma de hacerlo Â¿no? y hablando de esos propios modelos, si vemos acÃ¡ plan,\n",
            "[26:00.360 --> 26:05.520]  hay un plan de tecnologÃ­as de lenguaje del gobierno de EspaÃ±a donde usan esta supercomputadora que\n",
            "[26:05.520 --> 26:11.800]  estÃ¡ en una iglesia en Barcelona, se llama MarÃ­a y bÃ¡sicamente lo que hacen es exactamente esto\n",
            "[26:11.800 --> 26:17.600]  que estamos hablando, entrenan su modelo de gpt2, lo hacen libre, disponible, gpt2 base, ves?\n",
            "[26:17.600 --> 26:27.020]  entrenan su modelo gpt2 entrenado con informaciÃ³n de la librerÃ­a nacional de EspaÃ±a, lo tenemos\n",
            "[26:27.020 --> 26:32.940]  acÃ¡ a la derecha y este modelo lo hacen disponible de forma gratuita para la comunidad porque porque\n",
            "[26:32.940 --> 26:41.420]  dicen loco si nosotros queremos estar compitiendo con con Estados Unidos con UK con Italia con China\n",
            "[26:41.420 --> 26:46.500]  con Rusia con etcÃ©tera tenemos que tener nuestras cosas y por eso me parece importante hablar de\n",
            "[26:46.500 --> 26:51.140]  esto, ponerlo ahÃ­ afuera y que se comparta. AsÃ­ que acÃ¡ pueden verlo que tienen primero las primeras\n",
            "[26:51.140 --> 26:58.300]  las versiones genÃ©ricas de gpt2 pero tambiÃ©n tenemos Roberta para por ejemplo informaciÃ³n\n",
            "[26:58.300 --> 27:03.540]  financiera, informaciÃ³n mÃ©dica y etcÃ©tera. Esto es longformer, es cuando tenemos secuencias de\n",
            "[27:03.540 --> 27:10.260]  texto larguÃ­simas y queremos trabajar etcÃ©tera. Entonces si vos sos por ejemplo una startup que\n",
            "[27:10.260 --> 27:19.300]  quiere hacer un producto nuevo, puede consumir esto Â¿no? vos querÃ©s hacer, querÃ©s trabajar con\n",
            "[27:19.300 --> 27:25.700]  documentos de textos gigantes, bueno entonces tÃº usas este modelo como base, despuÃ©s entrenas el\n",
            "[27:25.700 --> 27:30.180]  tuyo encima y listo, ya tenÃ©s tu startup, por ejemplo, que va a ser nueva, entonces este son\n",
            "[27:30.180 --> 27:37.260]  el tipo de cosas que hay que saberlo para evitar esta divergencia entre Silicon Valley y digamos\n",
            "[27:37.260 --> 27:44.620]  LatinoamÃ©rica, EspaÃ±a y EuroamÃ©rica vamos a ponerlo, Â¿no? Entonces, vimos Carpati.\n",
            "[27:46.140 --> 27:53.540]  Â¿QuÃ© estÃ¡ haciendo AndrÃ©? Bueno, estÃ¡ haciendo esto, pero estÃ¡, o sea, esto es nosotros\n",
            "[27:53.540 --> 27:59.220]  metiÃ©ndonos en el repo de Ã©l, o sea, en el github de AndrÃ©, todavÃ­a ni siquiera hizo el release.\n",
            "[27:59.220 --> 28:05.340]  BÃ¡sicamente lo que estÃ¡ haciendo es, estÃ¡ haciendo un curso de YouTube que es espectacular,\n",
            "[28:05.340 --> 28:11.180]  donde va paso a paso haciendo estas cosas cada uno fÃ­jense es un curso de una hora y cuarto una hora\n",
            "[28:11.180 --> 28:19.260]  etcÃ©tera es muy bueno y estÃ¡ dividido por partes no va pasa de algo muy pequeÃ±o explicando quÃ© son\n",
            "[28:19.260 --> 28:24.140]  las redes numerales y el backpropagation y despuÃ©s se va a ir bien bien bien bien para\n",
            "[28:24.140 --> 28:29.900]  arriba explicando que es nano gpt o sea que son los gpt models o sea el tipo estÃ¡ yendo de cero\n",
            "[28:29.900 --> 28:37.220]  a el conocimiento que tiene un ingeniero de OpenAI, por ejemplo, y lo hace de forma gratuita\n",
            "[28:37.220 --> 28:41.700]  para la comunidad. AcÃ¡ podemos ver cÃ³mo estÃ¡ charlando de las scaling loss, etcÃ©tera, Â¿no?\n",
            "[28:41.700 --> 28:48.660]  Y vamos a ver un poco quÃ© son esas scaling loss tambiÃ©n en algÃºn momento. No hay problema. Bueno,\n",
            "[28:48.660 --> 29:02.340]  vimos que son las LLMs y que es GPT-3, vimos el mercado y vamos a ver ahora bÃ¡sicamente muy\n",
            "[29:02.340 --> 29:09.260]  rÃ¡pidamente quÃ© es esto de Sparrow, de DeepMind y ChatGPT, bueno para eso nos vamos a ir a este\n",
            "[29:09.260 --> 29:13.940]  vÃ­deo que hizo Letizia, Letizia tiene un canal de YouTube excelente se llama Miss Coffee Bean,\n",
            "[29:13.940 --> 29:18.940]  Miss Coffee Bean es esto que tienen acÃ¡ abajo a la izquierda y explica un montÃ³n de temas.\n",
            "[29:18.940 --> 29:27.180]  Es una, estÃ¡ en academia Leticia y hace resÃºmenes muy buenos pero tambiÃ©n se mete mucho en los\n",
            "[29:27.180 --> 29:31.460]  detalles tÃ©cnicos asÃ­ que esas son cosas que a mÃ­ por lo menos me gustan mucho porque generalmente\n",
            "[29:31.460 --> 29:39.380]  faltan. En este caso vamos a ver quÃ© son las, quÃ© es lo que estÃ¡ haciendo DeepMind y quÃ© es\n",
            "[29:39.380 --> 29:45.140]  lo que es que cÃ³mo se compara esto con chat gpt no primero que nada vamos a ver quÃ© era chat gpt no\n",
            "[29:45.140 --> 29:51.780]  lo vimos lo salteamos estamos minuto 30 mÃ¡s o menos y no vimos que era. Chat gpt es esto, ya hay un vÃ­deo\n",
            "[29:51.780 --> 29:55.660]  yo lo puse en la descripciÃ³n es un vÃ­deo que explica mÃ¡s o menos quÃ© es toda la gente estÃ¡\n",
            "[29:55.660 --> 30:00.580]  hablando de esto supongo que si estÃ¡s viendo este vÃ­deo es porque te interesa en el pi, te interesa\n",
            "[30:00.580 --> 30:06.060]  los modelos de lenguaje, sabes lo que es chat gpt y te interesarÃ­a aprender un poquito mÃ¡s, esas son\n",
            "[30:06.060 --> 30:13.100]  mis assumptions de todo esto. Pueden ver acÃ¡ cÃ³mo yo me metÃ­, escribÃ­ una prompt, escribÃ­ un script\n",
            "[30:13.100 --> 30:17.780]  para youtube sobre un mini curso que estoy armando para poder entrenar desde bla bla bla. Pueden ver\n",
            "[30:17.780 --> 30:22.300]  acÃ¡ cÃ³mo me responde chatgpt, me hace un Ã­ndice del contenido, me escribe una conclusiÃ³n y me\n",
            "[30:22.300 --> 30:27.300]  escribe una intro tambiÃ©n. Y despuÃ©s yo le digo, loco, Â¿por quÃ© no me haces una lista de 10 tÃ­tulos\n",
            "[30:27.300 --> 30:31.580]  que puedo poner a ese? Me aseguro que es la mayor cantidad de visitas, esto para joder nada mÃ¡s,\n",
            "[30:31.580 --> 30:37.980]  lo pongo y acÃ¡ pueden ver lo que me escribe Â¿no? cÃ³mo entrenar desde cero etcÃ©tera y bueno yo lo\n",
            "[30:37.980 --> 30:42.660]  cambiÃ© un poco el tÃ­tulo pero mÃ¡s o menos eso es lo que nos estÃ¡ diciendo y despuÃ©s le digo\n",
            "[30:42.660 --> 30:46.380]  hacemos una descripciÃ³n corta de todo lo que hablamos Â¿no? o sea toda esta conversaciÃ³n que\n",
            "[30:46.380 --> 30:51.980]  tenemos se la acuerda Â¿por quÃ©? porque tiene una ventana que se dice que es de 8.096 tokens,\n",
            "[30:51.980 --> 30:57.820]  es una ventana bastante larga y se acuerda de estas cosas, si yo sigo escribiendo en un momento\n",
            "[30:57.820 --> 31:06.700]  se va a olvidar de estas cosas porque tiene hasta 8.000 tokens. Â¿QuÃ© es entonces OpenAI? Esto de\n",
            "[31:06.700 --> 31:12.580]  chat-gpt es un modelo que lo que hace bÃ¡sicamente y obviamente no hay un paper que nos diga\n",
            "[31:12.580 --> 31:17.020]  simplemente usalo y lo que nos estÃ¡ diciendo, no hay un paper que nos diga pero la forma en\n",
            "[31:17.020 --> 31:24.220]  la que funciona es tenemos un modelo de generaciÃ³n de lenguaje que es gpt3 simple y despuÃ©s lo que\n",
            "[31:24.220 --> 31:29.340]  hicieron es entrenaron una polÃ­tica donde dicen\n",
            "[31:29.340 --> 31:33.340]  este tipo de, o sea entrenaron un modelo bÃ¡sicamente que dice y a mÃ­ me gusta mÃ¡s\n",
            "[31:33.340 --> 31:37.420]  este tipo de respuestas mÃ¡s que este otro tipo de respuestas, o sea si vos le\n",
            "[31:37.420 --> 31:42.260]  contestas me puedes escribir una lista de 10 de 10 tÃ­tulos para cosa y el\n",
            "[31:42.260 --> 31:46.340]  chatbot te contesta pero que me venÃ­a a hablar acÃ¡, bueno si te contesta asÃ­\n",
            "[31:46.340 --> 31:50.940]  de mal es como muy malo, generalmente un chatbot o digamos un agente de\n",
            "[31:50.940 --> 31:56.420]  machine learning tiene que tener tres cosas, tiene que ser Ãºtil, o sea tiene que dar respuestas, no\n",
            "[31:56.420 --> 32:01.260]  puede estar contestando todo el tiempo, no lo sÃ©, no sÃ© la respuesta, pregÃºntame de nuevo, no lo sÃ©,\n",
            "[32:01.260 --> 32:05.860]  no lo sÃ©, no puede ser asÃ­, tiene que ser Ãºtil, tiene que ser honesto, eso quiere decir que los\n",
            "[32:05.860 --> 32:12.420]  modelos estos generalmente alucinan las respuestas, o sea se las inventan, inventan respuestas que son\n",
            "[32:12.420 --> 32:19.540]  gramÃ¡ticamente correctas pero son incorrectas fÃ¡cticamente, si yo a esto le digo de repente,\n",
            "[32:19.540 --> 32:24.180]  a esto lo puedo convencer de algo. No, en realidad no es tal cosa, es tal otra cosa y se lo va a\n",
            "[32:24.180 --> 32:28.860]  creer el modelo. AsÃ­ que me va a contestar con eso, o sea que no es honesto, estÃ¡ alucinando y estÃ¡\n",
            "[32:28.860 --> 32:36.860]  mintiendo bÃ¡sicamente. No mintiendo, estÃ¡ dando informaciÃ³n falsa. Bueno entonces lo que se hace\n",
            "[32:36.860 --> 32:40.500]  es se crea un modelo que lo que hace es esta clasificaciÃ³n de, mira me gusta mÃ¡s la respuesta\n",
            "[32:40.500 --> 32:44.820]  D, despuÃ©s la C, despuÃ©s la A, despuÃ©s la B y ese es el modelo que tenemos. Entonces una vez que\n",
            "[32:44.820 --> 32:50.940]  tenemos ese modelo, se entrena usando un algoritmo de reinforcement learning, se entrena una polÃ­tica\n",
            "[32:50.940 --> 32:55.700]  de optimizaciÃ³n que dice mira andate mÃ¡s para el lado de este tipo de respuestas que son mejor,\n",
            "[32:55.700 --> 33:01.980]  o sea no diga como se llama si te hace una pregunta no le conteste callate troll ponele\n",
            "[33:01.980 --> 33:08.660]  sino que contÃ©stale bien decirle que fantÃ¡stica pregunta increÃ­ble estÃ¡ mi respuesta es asÃ­ etcÃ©tera\n",
            "[33:08.660 --> 33:15.740]  entonces, honesto, Ãºtil y la Ãºltima es que no sea tÃ³xico, eso quiere decir que no genere contenido\n",
            "[33:15.740 --> 33:23.980]  que vaya en contra de demografÃ­as, por ejemplo, que fomente sesgos que haya en la informaciÃ³n,\n",
            "[33:23.980 --> 33:29.820]  en la data con la que fue entrenado, etcÃ©tera, todo este tipo de cosas. Volvamos acÃ¡, Sparrow,\n",
            "[33:29.820 --> 33:35.180]  Â¿quÃ© es Sparrow? Sparrow es un paper que saliÃ³ antes que ChatGPT, saliÃ³ en septiembre,\n",
            "[33:35.180 --> 33:41.900]  el GPT sale el 30 de noviembre, esto sale el 22 de septiembre, pero DeepMind a diferencia de OpenAI\n",
            "[33:41.900 --> 33:49.140]  no hace disponible su modelo, pero sÃ­ nos pone disponible un paper, tampoco pone los weights\n",
            "[33:49.140 --> 33:53.740]  del modelo como para que lo podamos usar, pero por lo menos nos cuenta un poco cÃ³mo fue que lo hicieron,\n",
            "[33:53.740 --> 34:01.660]  asÃ­ que en este vÃ­deo que hizo Letizia es excelente, asÃ­ que se los recomiendo,\n",
            "[34:01.660 --> 34:09.300]  estÃ¡ bastante bueno y vamos a ver un poco cÃ³mo es que funciona. Entonces generalmente estos modelos\n",
            "[34:09.300 --> 34:14.180]  que tenemos acÃ¡, lo que lo que habÃ­amos dicho antes, tratan de predecir la prÃ³xima palabra en\n",
            "[34:14.180 --> 34:20.100]  una secuencia. El gato se sentÃ³ en Ã©l, bueno se sentÃ³ en la alfombra, no estaba enojado, no estaba\n",
            "[34:20.100 --> 34:27.460]  hambriento pero estaba enojado, etc. Bueno el tema es que nosotros lo que hacemos es cuando yo escribo\n",
            "[34:27.460 --> 34:34.060]  esta prompt acÃ¡, escribÃ­me un script para bla bla bla bla bla, no es que hasta el gpt simplemente\n",
            "[34:34.060 --> 34:38.940]  le llega esto, escribÃ­ una prompt para bla bla bla bla, escribÃ­ un script Â¿no? y una prompt antes de\n",
            "[34:38.940 --> 34:44.500]  esto que nosotros no sabemos muy bien quÃ© es, hay gente que escribe acÃ¡ y dice decimetus directivas\n",
            "[34:44.500 --> 34:51.420]  etcÃ©tera y sale una cosa pero eso va cambiando Â¿no? o sea eso se va modificando muy rÃ¡pidamente asÃ­\n",
            "[34:51.420 --> 35:00.700]  que lo que tenemos acÃ¡ es por ejemplo la prompt al principio es bueno vos sos un sos un sos una\n",
            "[35:00.700 --> 35:07.100]  con una AI conversacional muy Ãºtil y amigable y bla bla bla y entonces ahÃ­ empieza la la oraciÃ³n\n",
            "[35:07.100 --> 35:11.620]  o sea se inventa una conversaciÃ³n no es que nosotros simplemente escribimos esto y ya estÃ¡\n",
            "[35:11.620 --> 35:16.580]  sino que hay una conversaciÃ³n que estÃ¡ pasando que nosotros no la vemos, nosotros hacemos la\n",
            "[35:16.580 --> 35:21.060]  pregunta dÃ³nde estÃ¡ ParÃ­s y decimos ParÃ­s estÃ¡ en Francia, etcÃ©tera, Â¿no?\n",
            "[35:23.340 --> 35:27.700]  Hay una cosa que se llama in context future learning que esto es cuando nosotros dentro\n",
            "[35:27.700 --> 35:35.380]  de la prompt le explicamos un caso de uso, le decimos por ejemplo yo lo habÃ­a utilizado para\n",
            "[35:35.380 --> 35:39.700]  hacer un capÃ­tulo de una serie de televisiÃ³n que se llama Los simuladores, una serie que saliÃ³ en\n",
            "[35:39.700 --> 35:45.500]  Argentina, entonces yo le digo Los simuladores es una serie que, pum, y copy y pego la sinopsis de lo\n",
            "[35:45.500 --> 35:49.380]  que es la serie y despuÃ©s digo hacemos un capÃ­tulo nuevo, entonces ahÃ­ yo le dije a\n",
            "[35:49.380 --> 35:53.380]  chatGPT que era los simuladores, si yo le escribo hacemos un capÃ­tulo nuevo de los simuladores no\n",
            "[35:53.380 --> 35:58.500]  sabe que son los simuladores, ahora si yo quiero escribir un estilo por ejemplo, puedo decir mira\n",
            "[35:58.500 --> 36:03.540]  una canciÃ³n de cumbia es asÃ­, escribÃ­ una nueva canciÃ³n de cumbia y ahÃ­ te puede contestar, entonces\n",
            "[36:03.540 --> 36:13.660]  eso se llamarÃ­a in context visual learning. Entonces vamos a ver un poco quÃ© es lo que hizo\n",
            "[36:13.660 --> 36:18.460]  DeepMind que a todo esto no le llama chatbot a las cosas sino que le llama agente de diÃ¡logo o\n",
            "[36:18.460 --> 36:23.540]  Conversational AI Â¿por quÃ©? porque la palabra chatbot la verdad que no le gustaba a la gente\n",
            "[36:23.540 --> 36:28.620]  usarla porque como que nunca anduvieron, no andaban bien nunca los chatbots Â¿no? hasta el dÃ­a hasta\n",
            "[36:28.620 --> 36:34.220]  que saca chat gpt saca esta versiÃ³n pero antes de eso como que no andaban tan bien entonces Â¿cÃ³mo\n",
            "[36:34.220 --> 36:41.500]  es que funciona? bueno estÃ¡n tomando un modelo de lenguaje que se llama chinchilla Â¿no? por alguna\n",
            "[36:41.500 --> 36:47.500]  razÃ³n DeepMind eligiÃ³ ponerle nombres de animales a todos sus Ãºltimos modelos de lenguaje de\n",
            "[36:47.500 --> 36:52.700]  Transformers, entonces tenÃ©s gato, chinchilla, gopher, etcÃ©tera. Simplemente para que sepan.\n",
            "[36:52.700 --> 36:59.580]  Bueno, Â¿quÃ© es lo que hace? Tenemos esto, tenemos reglas, tenemos human feedback, tenemos el modelo\n",
            "[36:59.580 --> 37:05.340]  de lenguaje y tenemos clasificadores. Â¿QuÃ© significa todo esto? Bueno, vamos a ir para acÃ¡ y lo vamos\n",
            "[37:05.340 --> 37:11.700]  a ver acÃ¡ en este paper. Esto significa que tenemos el modelo y despuÃ©s tenemos dos cosas que estamos\n",
            "[37:11.700 --> 37:16.940]  haciendo. Cuando decimos estas dos cosas, esto quiere decir que alguien se sentÃ³, o sea, le pagaron\n",
            "[37:16.940 --> 37:23.180]  a gente para que se siente y diga, loco, de todas estas respuestas que se dieron, a mÃ­ me gusta mÃ¡s\n",
            "[37:23.180 --> 37:30.220]  la primera, la segunda me gusta menos, la tercera no me gusta para nada y despuÃ©s tambiÃ©n sentaron\n",
            "[37:30.220 --> 37:38.620]  a la gente a preguntarle si la respuesta que dio Sparrow va en contra de reglas que ya setearon,\n",
            "[37:38.620 --> 37:45.740]  Â¿no? Â¿QuÃ© significa reglas que setearon? Bueno, acÃ¡ pueden ver cÃ³mo es la prompt,\n",
            "[37:45.740 --> 37:49.420]  Â¿no? Que habÃ­amos dicho antes que habÃ­a una prompt antes. Pueden ver lo siguiente es una\n",
            "[37:49.420 --> 37:56.020]  conversaciÃ³n entre un asistente virtual que es y tiene mucho conocimiento, muy inteligente,\n",
            "[37:56.020 --> 38:02.980]  se llama Sparrow y un usuario humano que le vamos a llamar usuario y acÃ¡ podemos ver cÃ³mo empieza a\n",
            "[38:02.980 --> 38:10.900]  tener esta conversaciÃ³n. Estos son los ejemplos que se usan para entrenar el modelo. Lo que tenemos\n",
            "[38:10.900 --> 38:20.860]  acÃ¡, a ver si lo encuentro, tiene un par de reglas que son por ejemplo que no acÃ¡ dice que no tenga\n",
            "[38:20.860 --> 38:26.740]  estereotipos, que no haga micro agresiones, que no haga amenazas, que no haga agresiones sexuales,\n",
            "[38:26.740 --> 38:31.920]  que no ataque por identidad, que no insulte, etcÃ©tera. Estas son las reglas que estamos\n",
            "[38:31.920 --> 38:36.580]  seteando. Entonces, si volvemos acÃ¡, este modelo, Â¿quÃ© era lo que decÃ­a? Te decÃ­a,\n",
            "[38:36.580 --> 38:41.500]  Â¿estÃ¡ violando alguna regla? Cuando estÃ¡ contestando, cuando dice es un choto, me estÃ¡\n",
            "[38:41.500 --> 38:47.580]  insultando. Bueno, la regla que violÃ³ es la de insultar, por ejemplo. Entonces eso es lo que\n",
            "[38:47.580 --> 38:59.100]  tenemos acÃ¡. Y el otro, lo que hacemos es bÃ¡sicamente esta gente, elige esta respuesta,\n",
            "[38:59.100 --> 39:03.940]  cuÃ¡l es la mejor respuesta y con eso entrenamos, una vez que tenemos estos dos modelos, tenemos\n",
            "[39:03.940 --> 39:08.620]  uno de preferencia y otro de reglas, lo que hacemos es entrenamos con reinforcement learning,\n",
            "[39:08.620 --> 39:16.500]  bÃ¡sicamente diciendo mira, quiero que le des mayor importancia, mayor relevancia a lo que este\n",
            "[39:16.500 --> 39:22.280]  modelo te haya dicho que es la mejor respuesta y quiero que le des menor importancia, o sea que\n",
            "[39:22.280 --> 39:27.480]  castigue, que penalice es bastante grave a los modelos que a las respuestas que te hayan dado\n",
            "[39:27.480 --> 39:32.280]  que insulten, por ejemplo, entonces en base a estas dos cosas esto ya nos dice que el modelo\n",
            "[39:32.280 --> 39:37.520]  este reinforcement learning ya nos estÃ¡ diciendo que cuÃ¡les son las respuestas, el tipo de respuesta\n",
            "[39:37.520 --> 39:41.680]  que queremos de este chatbot, entonces Â¿quÃ© significa esto? que tenemos un modelo original\n",
            "[39:41.680 --> 39:46.800]  que es el modelo de lenguaje y despuÃ©s tenemos un modelo nuevo que existe que es un modelo que\n",
            "[39:46.800 --> 39:54.000]  estÃ¡ digamos optimizado para ser conversacional y para generar respuestas que le gustan a los\n",
            "[39:54.000 --> 39:59.760]  humanos 1 y respuestas que no insulten, no degraden, que no sean agresivas, etcÃ©tera tambiÃ©n.\n",
            "[40:01.480 --> 40:05.080]  Tenemos que entrenar el modelo de nuevo porque ya habÃ­amos hablado antes que eran como millones\n",
            "[40:05.080 --> 40:08.920]  de dÃ³lares entrenar a otros modelos, no, no se tienen que entrenar, lo que hace acÃ¡ DeepMind es\n",
            "[40:08.920 --> 40:18.280]  bÃ¡sicamente agarra, agarra este modelo, acÃ¡ lo dice Letizia, congela las 64 layers que tiene\n",
            "[40:18.280 --> 40:24.120]  Chinchilla y solamente hace fine tuning en las Ãºltimas 16 layers. Â¿Esto quÃ© significa? Que\n",
            "[40:25.320 --> 40:30.120]  no tenemos que entrenar el modelo completo sino que entrenamos una parte. Â¿QuÃ© significa estos\n",
            "[40:30.120 --> 40:35.400]  grandes rasgos para nosotros? Â¿Por quÃ© nos importa esto? Esto nos importarÃ­a porque si lo vemos como\n",
            "[40:35.400 --> 40:43.720]  una pirÃ¡mide Â¿no? vamos a tener muy pocas empresas Â¿se acuerdan? si nos volvemos para atrÃ¡s acÃ¡\n",
            "[40:43.720 --> 40:48.280]  Â¿cuÃ¡les son las empresas que estÃ¡n haciendo estos modelos de lenguaje? Google, DeepMind, Facebook,\n",
            "[40:48.280 --> 40:54.480]  Microsoft, OpenAI, AI21, son muy poquitas, muy poquitas las que hacen estos modelos gigantes,\n",
            "[40:54.480 --> 41:02.520]  gigantes, inmensos Â¿no? entonces es muy difÃ­cil que un gobierno de latinoamÃ©rica pueda hacer uno\n",
            "[41:02.520 --> 41:08.120]  de estos. Â¿QuÃ© mercado libre puede hacer uno de estos modelos? Imposible, muy difÃ­cil. Ahora, sÃ­ es\n",
            "[41:08.120 --> 41:13.400]  posible que tomemos un modelo de lenguaje mÃ¡s pequeÃ±o, como chinchilla por ejemplo, o como\n",
            "[41:13.400 --> 41:27.040]  glm130, lo tomemos y despuÃ©s creemos un modelo de recompensa encima que sea el que digamos el que\n",
            "[41:27.040 --> 41:32.500]  genera este modelo nuevo. Entonces ese modelo de recompensa, de reward que tenemos va a ser\n",
            "[41:32.500 --> 41:40.100]  muy cercano al dataset que estamos usando y eso va a ser solamente nuestro, eso va a ser propietario,\n",
            "[41:40.100 --> 41:45.620]  eso va a ser propietario de una empresa, MercadoLibre o de un paÃ­s latino, etcÃ©tera, que lo quiera hacer,\n",
            "[41:45.620 --> 41:52.420]  entonces ahÃ­ es donde estamos ya participando de toda esta generaciÃ³n de modelos. Entonces\n",
            "[41:53.900 --> 42:01.420]  una vez que tenemos este modelo, pueden ver que lo entrenan con este dataset,\n",
            "[42:01.420 --> 42:08.300]  explain me like and five y despuÃ©s genera esto bÃ¡sicamente. Un detalle mÃ¡s sobre Sparrow que es\n",
            "[42:08.300 --> 42:18.220]  algo que no tiene chat gpt, Sparrow cuando le hacemos una pregunta puede buscar evidencia y\n",
            "[42:18.220 --> 42:22.660]  esto es muy interesante, esto significa que como decirlo que el modelo lenguaje que el chatbot estÃ¡\n",
            "[42:22.660 --> 42:28.780]  interactuando con el modelo exterior con el mundo exterior y lo podemos ver acÃ¡ mÃ¡s o menos estaba\n",
            "[42:28.780 --> 42:33.460]  por acÃ¡ atrÃ¡s, vamos para atrÃ¡s pueden ver que dice search query, search results,\n",
            "[42:33.460 --> 42:38.140]  entonces cuando nosotros le hacemos una pregunta por ejemplo quiÃ©n es el\n",
            "[42:38.140 --> 42:44.140]  presidente de Brasil por ejemplo y usamos un modelo de lenguaje va a\n",
            "[42:44.140 --> 42:48.580]  tener conocimiento hasta un punto, si le preguntamos acÃ¡\n",
            "[42:48.580 --> 42:53.500]  quiÃ©n es el presidente de Brasil\n",
            "[42:53.500 --> 42:59.620]  hay que entrar de nuevo acÃ¡ ya nos quedamos sin acceso, vamos a ver pero si le hacemos la pregunta\n",
            "[43:03.540 --> 43:12.820]  nosotros sabemos que chat gpt estÃ¡ entrenado, ves? chat gpt estÃ¡ entrenado hasta el 2021,\n",
            "[43:12.820 --> 43:18.540]  entonces hasta el 2021 Jair Bolsonaro es el presidente, ahora es Lula, entonces no lo sabe,\n",
            "[43:18.540 --> 43:23.220]  eso quiere decir que no es tan bueno, no es tan Ãºtil este modelo porque se va a quedar desactualizado\n",
            "[43:23.220 --> 43:27.700]  muy fÃ¡cil, siempre, siempre se va a quedar desactualizado. Entonces, Â¿quÃ© es lo que hace Sparrow?\n",
            "[43:27.700 --> 43:34.540]  Cuando nosotros hacemos esa pregunta, hace una bÃºsqueda en Google y recibe los resultados de\n",
            "[43:34.540 --> 43:40.700]  los primeros, de las primeras pÃ¡ginas, Â¿no? Entonces usa eso, lo mete dentro de la prompt, Â¿ven?\n",
            "[43:40.700 --> 43:48.420]  Y despuÃ©s, en base a lo que saliÃ³ de esa prompt, puede contestar. Entonces, esto tambiÃ©n, interesante,\n",
            "[43:48.420 --> 43:52.900]  porque, asÃ­ como nosotros estamos diciendo que haga la bÃºsqueda en Google,\n",
            "[43:52.900 --> 43:57.500]  tambiÃ©n lo podrÃ­a hacer en la bÃºsqueda en nuestra propia fuente de conocimiento, Â¿no?\n",
            "[43:57.500 --> 43:59.900]  O sea, puede ser la Biblioteca Nacional de EspaÃ±a, por ejemplo,\n",
            "[43:59.900 --> 44:05.100]  o la Biblioteca de Argentina, o etcÃ©tera, o una base de datos de papers mÃ©dicos,\n",
            "[44:05.100 --> 44:08.540]  y nosotros decimos entonces, leÃ­te todos estos papers mÃ©dicos,\n",
            "[44:08.540 --> 44:12.780]  trÃ¡eme la informaciÃ³n, combÃ­nala, y eso quiere decir que puede dar la fuente\n",
            "[44:12.780 --> 44:14.740]  de dÃ³nde saliÃ³, que es lo que tenemos acÃ¡, Â¿no?\n",
            "[44:14.740 --> 44:21.940]  Sparrow response con evidencia, que es algo que ChatGPT no dice, ChatGPT puede inventar de cualquier\n",
            "[44:21.940 --> 44:31.620]  cosa, por ejemplo si yo le digo no es cierto, el presidente actual es Lula, es Lula, Â¿no?\n",
            "[44:34.020 --> 44:39.420]  Nosotros le escribimos esto y acÃ¡ podrÃ­amos empezar a cambiarlo, porque de nuevo esto serÃ­a\n",
            "[44:39.420 --> 44:46.140]  una especie de in context learning que estamos haciendo, estamos escribiendo y esto va a cambiar\n",
            "[44:46.140 --> 44:51.420]  ahora cuando nosotros salgamos de esta conversaciÃ³n que tenemos el modelo no es que aprendiÃ³ que ahora\n",
            "[44:51.420 --> 44:55.300]  el presidente es Lula, simplemente se lo olvida, solamente funciona la conversaciÃ³n que estamos\n",
            "[44:55.300 --> 44:59.940]  teniendo, yo le escribo esto me va a contestar si el presidente es Lula y ahora cuando yo le pregunte\n",
            "[44:59.940 --> 45:03.300]  quiÃ©n es el presidente de Brasil me va a decir el presidente es Lula, entonces es como que entendiÃ³\n",
            "[45:03.300 --> 45:10.340]  pero en realidad es simplemente la conversaciÃ³n que estamos teniendo. AsÃ­ que bueno, Ãºltima cosa que\n",
            "[45:10.340 --> 45:18.780]  voy a decir porque como siempre, como siempre esto es muy fÃ¡cil, no estoy al tanto, etcÃ©tera. Esto es\n",
            "[45:18.780 --> 45:23.980]  muy fÃ¡cil irse a la mierda, es un tema muy difÃ­cil, asÃ­ que lo vamos a separar por partes. Lo Ãºnico\n",
            "[45:23.980 --> 45:29.620]  que quiero mostrar es quÃ© pasos tenemos que hacer para entrenar nuestro chat GPT. Bueno, esto es lo\n",
            "[45:29.620 --> 45:35.220]  que vamos a estar viendo en la serie esta de vÃ­deos que vamos a hacer, vamos a hablar de Lion y Open\n",
            "[45:35.220 --> 45:41.860]  Assistant, que es un asistente que puede entender tasks, puede interactuar con sistemas ajenos,\n",
            "[45:41.860 --> 45:47.860]  externos, como por ejemplo realizar bÃºsquedas y puede recuperar informaciÃ³n del mundo exterior,\n",
            "[45:47.860 --> 45:53.100]  que es exactamente lo que queremos hacer, entonces se conecta muy bien con DeepMind,\n",
            "[45:53.100 --> 45:58.640]  lo que ellos hacen, asÃ­ que acÃ¡ lo tenemos, vamos a verlo con mÃ¡s detalle, quÃ© es esto,\n",
            "[45:58.640 --> 46:04.400]  pero pueden ver acÃ¡ quÃ© es lo que se quiere hacer, el roadmap, primero van a, o sea esto va a salir\n",
            "[46:04.400 --> 46:09.800]  este aÃ±o, le van a meter mucho huevo a hacer estas cosas, pueden ver que Yannick Kilger estÃ¡\n",
            "[46:09.800 --> 46:16.080]  estÃ¡ laburando en esto, Yannick tiene un canal de YouTube excelente, se lo recomiendo mucho y estÃ¡\n",
            "[46:16.080 --> 46:22.400]  estÃ¡ metiÃ©ndole huevo a estas cosas, asÃ­ que la idea es generar una versiÃ³n abierta de\n",
            "[46:22.400 --> 46:29.720]  de chat gpt y es lo que vamos a intentar hacer con esto. Paso a paso, paso a paso vamos a ir\n",
            "[46:29.720 --> 46:36.960]  yendo. Pueden ver cÃ³mo los pasos que tenemos es primero coleccionamos demostraciones humanas,\n",
            "[46:36.960 --> 46:44.040]  acÃ¡ pueden ver en este otro caso pueden ver cÃ³mo es que van a ser las los datasets que se estÃ¡n\n",
            "[46:44.040 --> 46:51.840]  armando y acÃ¡ van a ver el segundo paso serÃ­a hacer el fine tuning a un modelo base y el tercero\n",
            "[46:51.840 --> 46:57.840]  es bueno coleccionar estas instrucciones que lo que queremos hacer y ahÃ­ lo vamos a ir viendo\n",
            "[46:57.840 --> 47:05.000]  entrenamos el modelo reward que lo que habÃ­amos visto de chance gpt no este estÃ¡ de segundo\n",
            "[47:05.000 --> 47:09.520]  entonces ya tenemos el modelo lenguaje modelo abierto queremos entrar en el modelo de recompensa\n",
            "[47:09.520 --> 47:15.600]  y despuÃ©s queremos hacer esto. Esto es lo que vamos a ir viendo cÃ³mo se hace en esta serie de\n",
            "[47:15.600 --> 47:21.800]  vÃ­deos. Como siempre esto es un vÃ­deo larguÃ­simo, muy difÃ­cil hacerlo porque sin ediciÃ³n, sin nada,\n",
            "[47:21.800 --> 47:26.320]  simplemente hablando. Hay muchas cosas que seguramente me olvidÃ©. Les pido por favor que\n",
            "[47:26.320 --> 47:32.040]  me escriban si les parece interesante, en los comentarios quÃ© cosas faltan y tengamos una\n",
            "[47:32.040 --> 47:40.340]  conversaciÃ³n que esto sea la versiÃ³n en espaÃ±ol de querer hacer chat gpt que no sea simplemente un\n",
            "[47:40.340 --> 47:44.300]  vÃ­deo de bueno cÃ³mo usarlo cÃ³mo hacerte rico cÃ³mo hacerte etcÃ©tera no que esto sea aprender\n",
            "[47:44.300 --> 47:52.620]  aprender realmente de cÃ³mo funcionan estas cosas asÃ­ que con eso les mando les mando un abrazo\n",
            "[47:52.620 --> 47:58.620]  gigante gracias por quedarse ahÃ­ hasta el final hablamos 48 minutos de modelo de lenguaje que es\n",
            "[47:58.620 --> 48:05.180]  muy difÃ­cil en este day and age asÃ­ que si estÃ¡s acÃ¡ hasta este punto\n",
            "[48:05.180 --> 48:10.300]  loco felicitaciones porque quiere decir que\n",
            "[48:10.300 --> 48:14.540]  seguramente va a hacer cosas muy grosas con todo esto y que bueno le mando un\n",
            "[48:14.540 --> 48:16.540]  un abrazo grande. Chau, chau.\n",
            "Performing alignment...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### traducciÃ³n de espaÃ±ol a inglÃ©s"
      ],
      "metadata": {
        "id": "fiZnT8xcrlVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisperx archivo.wav --hf_token $hf_token --model large-v2 --language es --task translate --vad_filter --align_model WAV2VEC2_ASR_LARGE_LV60K_960H "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQfAQYc7ZgHs",
        "outputId": "c6aa1244-c7fd-4340-b6a5-941bec2d1757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-09 18:43:50.644564: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-09 18:43:54.584498: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-09 18:43:54.585421: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-09 18:43:54.585452: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Performing VAD...\n",
            "~~ Transcribing VAD chunk: (00:00.008 --> 00:27.802) ~~\n",
            "[00:00.000 --> 00:05.000]  Hello again to the channel. If this is your first time here, what we do is Spanish publication of the latest in Machine Learning,\n",
            "[00:05.000 --> 00:10.000]  trying to explain difficult things in a way that is easy to understand and also to use.\n",
            "[00:10.000 --> 00:18.000]  This chapter of today is going to be the first of a series of videos that I wanted to do to explain from scratch how we can make our own GPT chat.\n",
            "[00:18.000 --> 00:22.440]  We are going to go from very simple things to very complex, without dodging the difficult parts,\n",
            "[00:22.440 --> 00:27.640]  but trying to explain step by step what is happening in each thing.\n",
            "~~ Transcribing VAD chunk: (00:29.135 --> 00:47.798) ~~\n",
            "[00:00.000 --> 00:04.080]  So, as always, we are going to divide the video into parts, explaining first a motivation\n",
            "[00:04.080 --> 00:08.560]  of why do something like this, then a little about the language models, what is GPT-3,\n",
            "[00:08.560 --> 00:13.720]  what is the market, where are we, explain a little about what DeepMind is doing,\n",
            "[00:13.720 --> 00:19.120]  that OpenAI is doing, and then what steps we have to take to train our own GPT chat.\n",
            "~~ Transcribing VAD chunk: (00:49.587 --> 01:05.602) ~~\n",
            "[00:00.000 --> 00:04.320]  It's impossible to put all these topics in a single video, that's why I'm going to make a series.\n",
            "[00:05.320 --> 00:07.520]  Today we're going to make a kind of introduction and then,\n",
            "[00:07.520 --> 00:10.600]  weekly, for sure, when they come out,\n",
            "[00:10.600 --> 00:12.600]  I'll be putting different things.\n",
            "[00:12.600 --> 00:15.600]  we are going to talk about what is called Reinforcement Learning from Human Feedback.\n",
            "~~ Transcribing VAD chunk: (01:05.602 --> 01:21.835) ~~\n",
            "[00:00.000 --> 00:05.600]  We are going to talk a little about how to make instructions to the giant language models\n",
            "[00:05.600 --> 00:10.800]  and we are going to talk about an initiative that Lion is doing, which is the group of people\n",
            "[00:10.800 --> 00:15.800]  which is also part of the people who made Stable Diffusion.\n",
            "~~ Transcribing VAD chunk: (01:21.835 --> 01:51.822) ~~\n",
            "[00:00.000 --> 00:03.080]  and that they are also making an open version of ChatGPT,\n",
            "[00:03.080 --> 00:05.800]  which in theory should be even better than ChatGPT.\n",
            "[00:05.800 --> 00:09.360]  So we are going to see all those things in a series of videos.\n",
            "[00:10.360 --> 00:12.280]  So first let's start with a motivation.\n",
            "[00:12.280 --> 00:15.200]  What is it that leads me to do something like this?\n",
            "[00:15.200 --> 00:18.320]  Well, first of all, I think I was seeing this tweet,\n",
            "[00:18.320 --> 00:21.040]  the truth is, a lot of this conversation is happening on Twitter,\n",
            "[00:21.040 --> 00:25.000]  so it's quite interesting if you don't use it, let's say,\n",
            "[00:25.000 --> 00:27.640]  to follow everything that is the part of Machine Learning, of news.\n",
            "[00:27.640 --> 00:30.640]  The conversation is happening there and in another two or three places.\n",
            "~~ Transcribing VAD chunk: (01:51.822 --> 02:17.152) ~~\n",
            "[00:00.000 --> 00:06.800]  but on twitter it is very concentrated and here for example this is david he is telling us that he is\n",
            "[00:06.800 --> 00:13.920]  making an analogy with 2008 and if we are in 2008 there is a very large recession and the\n",
            "[00:13.920 --> 00:20.480]  iphone just came out the previous year the first android phone just came out this year 2008 that is, it will\n",
            "[00:20.480 --> 00:24.480]  be presented in October and then the next the next three to five generations are going to revolutionize\n",
            "[00:24.480 --> 00:27.480]  absolutely everything.\n",
            "~~ Transcribing VAD chunk: (02:17.725 --> 02:47.172) ~~\n",
            "[00:00.000 --> 00:04.960]  we can say that more or less it is what we are having here now with chat gpt, it is not the first\n",
            "[00:04.960 --> 00:09.320]  model, it is not the last, it is the first model that is opening the space to all these things,\n",
            "[00:09.320 --> 00:13.840]  the same thing that happened with dally 2 when it just came out, an incredible revolution, now dally 2\n",
            "[00:13.840 --> 00:18.840]  is not used so much either, but the open source versions, let's say we have me journey, we have everything\n",
            "[00:18.840 --> 00:26.320]  that is stable diffusion, etc., all that is infinitely better now than dally 2, but the reason\n",
            "[00:26.320 --> 00:31.560]  for which that development was accelerated was due to the appearance of\n",
            "~~ Transcribing VAD chunk: (02:49.349 --> 03:14.375) ~~\n",
            "[00:00.000 --> 00:04.000]  I'm going to explain a little bit about why I'm motivated.\n",
            "[00:04.000 --> 00:12.000]  From my point of view, because of my job, I'm lucky to be able to work a lot with giant language models at the moment.\n",
            "[00:12.000 --> 00:16.000]  And what I'm seeing is a great divergence that is going to occur, or is occurring now.\n",
            "[00:16.000 --> 00:18.000]  I'm going to exaggerate it, I'm going to go for the absurd.\n",
            "[00:18.000 --> 00:22.320]  we are going to say that there is a group of engineers in Silicon Valley and then we have\n",
            "[00:22.320 --> 00:25.000]  a group of engineers in Latin America.\n",
            "~~ Transcribing VAD chunk: (03:15.033 --> 03:34.270) ~~\n",
            "[00:00.000 --> 00:05.600]  The difference in knowledge, in use, in generation, in the ability to generate new things\n",
            "[00:05.600 --> 00:07.600]  is huge.\n",
            "[00:07.600 --> 00:09.600]  It's huge. That's what I'm seeing.\n",
            "[00:09.600 --> 00:12.600]  So, for me, it's very important, very relevant,\n",
            "[00:12.600 --> 00:14.600]  to start having technical conversations\n",
            "[00:14.600 --> 00:16.600]  about how these things work.\n",
            "[00:16.600 --> 00:20.080]  only that\n",
            "~~ Transcribing VAD chunk: (03:34.270 --> 04:00.697) ~~\n",
            "[00:00.000 --> 00:13.000]  We don't just become users of these technologies, we also have to be able to generate them or at least modify them in a way that we can generate new things.\n",
            "[00:13.000 --> 00:20.640]  And to do that, there is no other way than to get in and get fully involved in these things technically,\n",
            "[00:20.640 --> 00:26.360]  see what is being done, what are the conversations that are taking place, and basically, give it a lot of eggs.\n",
            "~~ Transcribing VAD chunk: (04:01.439 --> 04:13.437) ~~\n",
            "[00:00.000 --> 00:07.800]  As a little more introduction, there I put that it is explained by a Data Scientist, that's because I work as a Data Scientist, let's say.\n",
            "[00:07.800 --> 00:11.800]  But that's not what I studied. I studied mechanical engineering.\n",
            "~~ Transcribing VAD chunk: (04:13.741 --> 04:34.413) ~~\n",
            "[00:00.000 --> 00:05.320]  many years ago, 2016 more or less, I started doing my first course, I didn't know how to program and I said\n",
            "[00:05.320 --> 00:10.400]  look, I want to get into this, I really loved it, how do I do it? I will always be at a disadvantage\n",
            "[00:10.400 --> 00:14.640]  with the people who studied physics, mathematics, etc., who are already coming, let's say I will always be\n",
            "[00:14.640 --> 00:19.560]  years behind. So my decision at that time was, I have to make a strategy, I'm going to do a leapfrog,\n",
            "[00:19.560 --> 00:21.560]  It's called jumping.\n",
            "~~ Transcribing VAD chunk: (04:34.919 --> 04:49.870) ~~\n",
            "[00:00.000 --> 00:03.660]  I'm going to get directly into deep learning, computer vision in this case,\n",
            "[00:03.660 --> 00:05.660]  and natural language models.\n",
            "[00:05.660 --> 00:07.660]  So, what I mean by this is,\n",
            "[00:07.660 --> 00:11.040]  all the techniques that have existed up to this point,\n",
            "[00:11.040 --> 00:15.040]  it is possible that they will throw themselves into the trash at this time.\n",
            "~~ Transcribing VAD chunk: (04:49.870 --> 05:10.390) ~~\n",
            "[00:00.000 --> 00:08.180]  Is it important to have mathematical knowledge? Yes, but with algebra, with a basic level of algebra, I think you can do very interesting things.\n",
            "[00:08.180 --> 00:16.740]  With this, what I want to say is, no matter where you are coming from, what matters is what you want to do in the future and how much work you put into studying.\n",
            "[00:16.740 --> 00:19.700]  So today we are going to put a lot of work into studying.\n",
            "[00:19.700 --> 00:21.700]  Well...\n",
            "~~ Transcribing VAD chunk: (05:11.234 --> 05:35.990) ~~\n",
            "[00:00.000 --> 00:04.260]  Let's see where we are with all this, because it's important.\n",
            "[00:04.260 --> 00:07.100]  This comic is spectacular. Look at this comic.\n",
            "[00:07.100 --> 00:12.440]  It says, hey man, did you realize how artificial intelligence is going to impact our business?\n",
            "[00:12.440 --> 00:14.440]  Hey, don't worry, I'm working on it.\n",
            "[00:14.440 --> 00:16.440]  And it asks ChatGPT.\n",
            "[00:16.440 --> 00:19.740]  How is artificial intelligence going to impact our business?\n",
            "[00:19.740 --> 00:21.740]  And there are many ways to do it.\n",
            "[00:21.740 --> 00:23.740]  Excellent comic\n",
            "~~ Transcribing VAD chunk: (05:36.530 --> 06:04.053) ~~\n",
            "[00:00.000 --> 00:04.760]  This is something that is happening at the same time, that is, there are other models, I did not talk about this\n",
            "[00:04.760 --> 00:08.760]  Bali model, we talked about Whisper, we talked about Chats GPT, Bali is a model that Microsoft is\n",
            "[00:08.760 --> 00:14.840]  also releasing, which is basically a model that does text to speech, it converts, it generates,\n",
            "[00:14.840 --> 00:23.800]  let's say, a synthesized voice. How does it do it? It is copied from a 3-second clip,\n",
            "[00:23.800 --> 00:27.480]  from the person we want to copy, and with that it is simply the only thing it needs to\n",
            "[00:27.480 --> 00:29.080]  Create. Share. Learn.\n",
            "~~ Transcribing VAD chunk: (06:04.458 --> 06:32.470) ~~\n",
            "[00:00.000 --> 00:02.000]  to generate audio.\n",
            "[00:03.740 --> 00:05.740]  So, if we put these things together,\n",
            "[00:05.740 --> 00:07.740]  this is a detail,\n",
            "[00:07.740 --> 00:10.500]  Bali reminds me a lot of the movie Terminator 2,\n",
            "[00:10.500 --> 00:12.500]  when he calls Terminator,\n",
            "[00:12.500 --> 00:14.500]  what happened to the dog? He changes his name and says\n",
            "[00:14.500 --> 00:16.500]  your dog is dead.\n",
            "[00:16.500 --> 00:19.500]  At that moment, I was very surprised by that scene.\n",
            "[00:19.500 --> 00:23.000]  We can see that Bali is something similar to that.\n",
            "[00:23.000 --> 00:25.000]  Well, what do we have here?\n",
            "[00:25.000 --> 00:27.000]  A commercial model, right?\n",
            "[00:27.000 --> 00:29.000]  Siri, Alexa,\n",
            "~~ Transcribing VAD chunk: (06:32.757 --> 07:01.563) ~~\n",
            "[00:00.000 --> 00:04.000]  That's it. This competes directly with these two things.\n",
            "[00:04.000 --> 00:09.720]  So we have Whisper, which converts audio to text.\n",
            "[00:09.720 --> 00:12.640]  Then ChatGPT, which converts text to something else.\n",
            "[00:12.640 --> 00:14.640]  We don't know what yet, but to something.\n",
            "[00:15.320 --> 00:19.280]  And then Valley, which converts it again to audio.\n",
            "[00:19.280 --> 00:21.160]  So we have a conversation.\n",
            "[00:21.160 --> 00:22.560]  In this case it says,\n",
            "[00:22.560 --> 00:27.120]  turn on the party lights and start playing good music\n",
            "[00:27.120 --> 00:29.120]  at 8 p.m. every Wednesday.\n",
            "~~ Transcribing VAD chunk: (07:02.272 --> 07:25.998) ~~\n",
            "[00:00.000 --> 00:06.000]  But we can see how new things are starting to appear, and this is dangerous, not only for\n",
            "[00:06.000 --> 00:15.000]  let's say, we can say, some things are good, others are dangerous, but companies, for example Canva, is a design company\n",
            "[00:15.000 --> 00:23.560]  It's a design company that competes with Adobe, but one thing that Canva has\n",
            "~~ Transcribing VAD chunk: (07:26.285 --> 07:54.398) ~~\n",
            "[00:00.000 --> 00:07.400]  is that, look, they were going to go to the market now, that is, ask for a loan to the market and the guys said\n",
            "[00:07.400 --> 00:15.000]  well, we're going to have to hire ChatGPT, I mean, not ChatGPT, but GPT-3 at least, because we can't compete with them\n",
            "[00:15.000 --> 00:20.300]  we don't have the capacity to compete with them, so you can imagine how companies are already\n",
            "[00:20.300 --> 00:26.500]  that is, a system is being generated where you are either with me or I'm going to bust you, basically\n",
            "[00:26.500 --> 00:28.500]  That, then, is what is happening.\n",
            "~~ Transcribing VAD chunk: (07:55.428 --> 08:17.703) ~~\n",
            "[00:00.000 --> 00:06.000]  No one is safe in this sense. It's not just a technology company, it's any type of company.\n",
            "[00:06.000 --> 00:13.000]  A technical support company, for example, has no reason not to feel threatened by something like ChatGPT.\n",
            "[00:13.000 --> 00:16.000]  This is like Apple getting into the automotive market.\n",
            "[00:16.000 --> 00:21.000]  Okay, we have Volkswagen, we have Ford, Chevrolet, etc., but when Apple gets in, it's going to revolutionize.\n",
            "[00:21.000 --> 00:24.360]  we don't know what's going on.\n",
            "~~ Transcribing VAD chunk: (08:19.610 --> 08:26.562) ~~\n",
            "[00:00.000 --> 00:05.700]  So well, let's see a little bit what language models are and what GPT-3 is, right?\n",
            "[00:05.700 --> 00:09.260]  we are going to start very slowly\n",
            "~~ Transcribing VAD chunk: (08:27.929 --> 08:53.832) ~~\n",
            "[00:00.000 --> 00:07.460]  First of all, I'm very lazy so I don't edit the videos, I record them all at once.\n",
            "[00:07.460 --> 00:11.480]  It's pretty hard to do this, so I'll probably make a lot of mistakes.\n",
            "[00:11.480 --> 00:15.840]  As I go, I'll forget things, I'll probably make mistakes in other things.\n",
            "[00:15.840 --> 00:20.600]  I ask you, if there's something I missed or didn't understand, or that I explained wrongly,\n",
            "[00:20.600 --> 00:25.840]  It is very likely that you will put it in the comments and I will check it again for the next one.\n",
            "~~ Transcribing VAD chunk: (08:54.237 --> 09:14.673) ~~\n",
            "[00:00.000 --> 00:04.160]  all the links I put here I will put them in the description of the video\n",
            "[00:04.160 --> 00:09.480]  my idea with this is not to make a short video, bad, etc., summarized\n",
            "[00:09.480 --> 00:13.120]  with videos, with graphics, etc., no, no, this is a hard video\n",
            "[00:13.120 --> 00:19.960]  to see it over time, that is, go back and say what did this crazy guy say in such a place, well, he said such a thing\n",
            "[00:19.960 --> 00:21.960]  That's the idea.\n",
            "~~ Transcribing VAD chunk: (09:15.280 --> 09:45.065) ~~\n",
            "[00:00.000 --> 00:06.280]  This link is a course that has just come out, it is called CS324 from Stanford University, it is a\n",
            "[00:06.280 --> 00:13.800]  language model course, it is quite good and it serves as an introduction to the topic. What is a\n",
            "[00:13.800 --> 00:18.760]  language model? Well, the classic definition that we have of a language model is the\n",
            "[00:18.760 --> 00:23.840]  distribution of probabilities over a sequence of tokens. What does this mean? It means that,\n",
            "[00:23.840 --> 00:28.720]  For example, if we say here, the mouse ate the cheese.\n",
            "~~ Transcribing VAD chunk: (09:45.554 --> 10:14.191) ~~\n",
            "[00:00.000 --> 00:08.600]  this has a probability of occurrence of 0.02, excellent, now if we say the cheese\n",
            "[00:08.600 --> 00:16.920]  ate the mouse and it does not have a probability of occurrence less than 0.01 and if we have the mouse\n",
            "[00:16.920 --> 00:25.160]  the cheese ate a probability of occurrence very low and how is it that this is calculated\n",
            "[00:25.160 --> 00:30.680]  this probability that it will happen, well...\n",
            "~~ Transcribing VAD chunk: (10:16.317 --> 10:44.549) ~~\n",
            "[00:00.000 --> 00:07.500]  This means that the model we are using has linguistic abilities, but it also has knowledge of the world, right?\n",
            "[00:07.500 --> 00:13.840]  For example, that means that grammatically, perhaps, these two sentences are correct,\n",
            "[00:13.840 --> 00:21.120]  but they are not normally used, it would never happen, something like that, or it is unlikely, it is less likely, half likely,\n",
            "[00:21.120 --> 00:24.560]  that the second situation happens compared to the first situation, right?\n",
            "[00:24.560 --> 00:30.160]  So what we can say is that these language models have two things\n",
            "~~ Transcribing VAD chunk: (10:44.836 --> 11:12.410) ~~\n",
            "[00:00.000 --> 00:05.500]  In general, they have syntactic knowledge of grammar and they also have knowledge of the world.\n",
            "[00:05.500 --> 00:07.500]  How do they have knowledge of the world?\n",
            "[00:07.500 --> 00:11.000]  Because, for example, to train these things, we put the entire Wikipedia\n",
            "[00:11.000 --> 00:13.500]  in Spanish, in English, etc.\n",
            "[00:13.500 --> 00:19.500]  We put the entire Wikipedia and there are examples where this type of phrase was read.\n",
            "[00:19.500 --> 00:24.000]  So, for example, this had two occurrences, this had one occurrence, and that's it.\n",
            "[00:24.000 --> 00:27.500]  So it's twice as likely as... something like that, more or less.\n",
            "[00:27.500 --> 00:29.100]  Create. Share. Learn.\n",
            "~~ Transcribing VAD chunk: (11:12.933 --> 11:41.198) ~~\n",
            "[00:00.000 --> 00:11.520]  and etc. It's explained here. I'm not going to get into this topic of how it is because the truth is that they do it very well in this course so I recommend it, here you can see what the capacities are,\n",
            "[00:11.520 --> 00:28.560]  you can also have with gpt3, etc. and you can see all these things. So I'm not going to get into much with this, the perplexity is also explained, which is a metric that is used to know how good the model is, right?\n",
            "~~ Transcribing VAD chunk: (11:42.110 --> 12:08.198) ~~\n",
            "[00:00.000 --> 00:02.780]  How are these language models trained then? What do they do?\n",
            "[00:02.780 --> 00:06.900]  We already said, it returns a probability of a sequence of tokens.\n",
            "[00:06.900 --> 00:10.260]  Well, for what these models are trained, they are generative models.\n",
            "[00:10.260 --> 00:14.660]  So what it means is that normally what happens is that a sequence of tokens is given,\n",
            "[00:14.660 --> 00:18.120]  put it again, the mouse ate it, the what?\n",
            "[00:18.120 --> 00:21.840]  Well, it has to predict based on those three tokens,\n",
            "[00:21.840 --> 00:25.560]  the four tokens I had before, the mouse ate it,\n",
            "[00:25.560 --> 00:26.560]  Well...\n",
            "~~ Transcribing VAD chunk: (12:08.384 --> 12:38.118) ~~\n",
            "[00:00.000 --> 00:04.520]  It has to predict the next token that will come after that, right?\n",
            "[00:04.520 --> 00:07.800]  And that says the cheese with a high probability, and so on.\n",
            "[00:09.800 --> 00:15.480]  We saw it in other videos, I will also put it in the chat, but basically there is also a thing called ngrams, right?\n",
            "[00:15.480 --> 00:24.600]  So maybe it doesn't just need, for example, to predict a token, but maybe it will predict several tokens, right?\n",
            "[00:24.600 --> 00:30.200]  So with this you can generate things that are more plausible, that are more likely to work.\n",
            "~~ Transcribing VAD chunk: (12:37.477 --> 12:47.737) ~~\n",
            "[00:00.000 --> 00:07.400]  likely to happen. Two tokens, three tokens, and there are different ways to choose these tokens, right?\n",
            "[00:07.400 --> 00:10.400]  greedy search, beam, etc.\n",
            "~~ Transcribing VAD chunk: (12:49.829 --> 13:17.318) ~~\n",
            "[00:00.000 --> 00:05.040]  Here you can see that when these models are trained, one thing that can be done is\n",
            "[00:05.040 --> 00:13.320]  you can write and tell it, predict the next token, and the way this is done is\n",
            "[00:13.320 --> 00:19.200]  because I already have the wikipedia, so what I can do is simply shorten the sentence and say\n",
            "[00:19.200 --> 00:25.600]  the missing token is this, then the model learns very well, because it says I predicted chorizo\n",
            "[00:25.600 --> 00:27.600]  No, man. It's not chorizo.\n",
            "~~ Transcribing VAD chunk: (13:17.369 --> 13:47.137) ~~\n",
            "[00:00.000 --> 00:03.800]  What's that? What's that? I have the answer here. What's that? No, no. It's wrong. Fix it.\n",
            "[00:03.800 --> 00:06.300]  So there it goes learning as we train it.\n",
            "[00:06.300 --> 00:09.200]  Another way to do it, instead of predicting the next token,\n",
            "[00:09.200 --> 00:12.300]  is when we give it a blank space, which is what we have here.\n",
            "[00:12.300 --> 00:16.300]  Basically what we are saying is,\n",
            "[00:16.300 --> 00:19.800]  look, fill me the blank space. It doesn't mean it's the last token,\n",
            "[00:19.800 --> 00:23.300]  but yes, maybe it's an intermediate token, right?\n",
            "[00:23.300 --> 00:25.100]  This is what the model does.\n",
            "[00:25.100 --> 00:27.100]  Well,\n",
            "[00:27.100 --> 00:31.740]  These are generation language models.\n",
            "~~ Transcribing VAD chunk: (13:48.858 --> 14:07.538) ~~\n",
            "[00:00.000 --> 00:07.400]  but they are not giant models of language, let's see what are the giant models of language, these are\n",
            "[00:07.400 --> 00:12.560]  models that are trained with a stupid amount of data and that were\n",
            "[00:12.560 --> 00:18.120]  adapted to do a stupid amount of tasks basically since they are called what would be zero shot\n",
            "~~ Transcribing VAD chunk: (14:07.673 --> 14:35.703) ~~\n",
            "[00:00.000 --> 00:05.140]  That means that tomorrow we ask him to write a cumbia song,\n",
            "[00:05.500 --> 00:08.440]  but he never trained to write a cumbia song, but he can do it the same way.\n",
            "[00:09.200 --> 00:09.700]  Zero shot.\n",
            "[00:10.140 --> 00:12.840]  If it was one shot, for example, which has an example,\n",
            "[00:12.840 --> 00:16.940]  when we're doing it, we give him an example and then we say,\n",
            "[00:17.140 --> 00:20.140]  this is a cumbia song, let's write a new song. One shot.\n",
            "[00:21.900 --> 00:26.300]  Supervised learning would be that we train him only with a cumbia song dataset\n",
            "[00:26.300 --> 00:28.300]  and we tell you to make me a new one that has this format.\n",
            "~~ Transcribing VAD chunk: (14:37.205 --> 15:00.779) ~~\n",
            "[00:00.000 --> 00:03.360]  What's going on with these language models that we have here?\n",
            "[00:04.960 --> 00:07.760]  They are doing better than humans in many things.\n",
            "[00:07.760 --> 00:11.360]  The gray line that you have here are humans.\n",
            "[00:11.360 --> 00:13.160]  And you can see that we are already surpassing them.\n",
            "[00:13.160 --> 00:19.520]  There was a time when models started to do better than humans.\n",
            "[00:19.520 --> 00:23.520]  Here you can see the performance in 58 tasks.\n",
            "~~ Transcribing VAD chunk: (15:01.505 --> 15:22.717) ~~\n",
            "[00:00.000 --> 00:04.840]  And the issue they have is that, this is called...\n",
            "[00:04.840 --> 00:10.000]  I mean, down here you can see the parameters, and up here you can see the performance.\n",
            "[00:10.000 --> 00:12.000]  Well, the parameters below...\n",
            "[00:12.000 --> 00:16.840]  What they realized is that when we raised the parameters, which is what happened with GPT-3,\n",
            "[00:16.840 --> 00:18.840]  they put a stupid amount of parameters,\n",
            "[00:18.840 --> 00:23.160]  they started to write down something called emergency.\n",
            "~~ Transcribing VAD chunk: (15:23.037 --> 15:46.375) ~~\n",
            "[00:00.000 --> 00:04.800]  not an emergency, something is happening, the house is on fire, but\n",
            "[00:04.800 --> 00:10.140]  emergent abilities of these models. And this, to this day, we don't know why it happens.\n",
            "[00:10.140 --> 00:14.800]  There are approximations, but we don't know yet what it is. Consciousness? We don't know yet.\n",
            "[00:14.800 --> 00:20.220]  But what we do know is that when you get to a very large number of parameters,\n",
            "[00:20.220 --> 00:23.580]  These models are starting to be much better than before.\n",
            "~~ Transcribing VAD chunk: (15:46.207 --> 16:02.137) ~~\n",
            "[00:00.000 --> 00:06.140]  that before, that is, the increase in performance is not linear but becomes exponential, which is what\n",
            "[00:06.140 --> 00:12.000]  we see here, right? Here you can see how a jump of the triple of performance occurs when we start to\n",
            "[00:12.000 --> 00:14.000]  to increase the number of parameters.\n",
            "~~ Transcribing VAD chunk: (16:03.284 --> 16:28.630) ~~\n",
            "[00:00.000 --> 00:04.800]  Here we can see how evolution is through the years, we had statistics, we had machine learning, we had deep learning\n",
            "[00:04.800 --> 00:07.300]  and now we have what is called Foundation Models.\n",
            "[00:07.300 --> 00:14.600]  What do these Foundation Models mean? Well, they are giant models, we can say that it is a...\n",
            "[00:14.600 --> 00:20.600]  It is not in this case, but we can say that we are going in the direction of a very large intelligence,\n",
            "[00:20.600 --> 00:25.600]  Let's say that we can ask them about anything and they can answer us about those things, right?\n",
            "~~ Transcribing VAD chunk: (16:30.520 --> 16:57.942) ~~\n",
            "[00:00.000 --> 00:06.480]  These are the datasets that are used, they can be from anything, books, code, reddit, medical sites,\n",
            "[00:06.480 --> 00:14.440]  medical papers, patents, etc., that is, everything that we have generated as humanity\n",
            "[00:14.440 --> 00:18.640]  is what we can put into these models as training data, right?\n",
            "[00:18.640 --> 00:21.640]  And these are the things that these models can do.\n",
            "[00:21.640 --> 00:27.640]  They can summarize, they can generate images, they can classify the text, translate, chat, create recipes,\n",
            "~~ Transcribing VAD chunk: (16:58.432 --> 17:22.529) ~~\n",
            "[00:00.000 --> 00:09.800]  explain code without many many things and here we begin to see a little what were the things that made this revolution happen\n",
            "[00:09.800 --> 00:20.640]  first the architecture of transformers that was done in the year, that is, the attention and sole unit paper was done in the year 2017\n",
            "[00:20.640 --> 00:23.920]  and all the people who made that paper, except...\n",
            "~~ Transcribing VAD chunk: (17:22.968 --> 17:46.154) ~~\n",
            "[00:00.000 --> 00:07.500]  One person left Google, that paper was done by Google, and they went to start their own startups or OpenAI, etc.\n",
            "[00:08.500 --> 00:10.500]  You can see how important that was.\n",
            "[00:11.000 --> 00:18.000]  Here we have the evolution of the amount of compute that was used to train these models,\n",
            "[00:18.000 --> 00:21.440]  that is, it is hitting an exponential as we get closer that means that\n",
            "[00:21.440 --> 00:25.440]  train these models, it's worth millions of dollars.\n",
            "~~ Transcribing VAD chunk: (17:47.065 --> 18:09.847) ~~\n",
            "[00:00.000 --> 00:05.280]  And we can see here who dominates the risk, etc.\n",
            "[00:05.280 --> 00:07.280]  This is basically what I'm saying.\n",
            "[00:07.640 --> 00:10.120]  Big companies are the ones who have control over these things.\n",
            "[00:10.120 --> 00:12.120]  And that's not good.\n",
            "[00:12.120 --> 00:14.120]  It should be fairly distributed.\n",
            "[00:14.120 --> 00:16.120]  Industry, open source community,\n",
            "[00:16.980 --> 00:18.980]  governments, state.\n",
            "[00:18.980 --> 00:22.580]  It has to be distributed, at least in an egalitarian way.\n",
            "~~ Transcribing VAD chunk: (18:10.505 --> 18:40.357) ~~\n",
            "[00:00.000 --> 00:12.720]  These are the models that we have available today, you can see that these giant models, here we have the parameters, 540,000, 130,000, 80,000, 20,000, 180,000, etc.\n",
            "[00:12.720 --> 00:22.880]  You can see that the companies that do it are Google, Google with DeepMind, Facebook, Meta, Microsoft, OpenAI, and BigScience, which is an open model.\n",
            "[00:22.880 --> 00:27.880]  And then AI21 is an Israeli company that is doing this, right?\n",
            "[00:27.880 --> 00:29.880]  well and etc.\n",
            "~~ Transcribing VAD chunk: (18:39.648 --> 19:01.990) ~~\n",
            "[00:00.000 --> 00:07.920]  etc. maps and so on. One interesting thing here, this is very good, I recommend it to see it, it is called\n",
            "[00:07.920 --> 00:17.320]  the state of AI report 2022, it is made by Nathan Benaj and Ian Hogarth, both of them are in what\n",
            "[00:17.320 --> 00:22.480]  which is Venture Capital, and the truth is that they do an excellent analysis, they have been doing it for several years, you can see it.\n",
            "~~ Transcribing VAD chunk: (19:01.754 --> 19:30.172) ~~\n",
            "[00:00.000 --> 00:03.840]  As you can see down here, they've been doing it since 2018 and it's spectacular.\n",
            "[00:03.840 --> 00:10.320]  There's no better summary on the internet than what these people do.\n",
            "[00:10.320 --> 00:11.840]  What did they do here?\n",
            "[00:11.840 --> 00:18.840]  Here we can see that they are talking about where we are at the moment.\n",
            "[00:18.840 --> 00:23.480]  Well, five years after the release of the Transformers, it's still the best architecture.\n",
            "[00:23.480 --> 00:27.480]  There's always something new that tries to modify this, that, etc.\n",
            "[00:27.480 --> 00:29.480]  But the truth is that...\n",
            "~~ Transcribing VAD chunk: (19:30.526 --> 19:58.808) ~~\n",
            "[00:00.000 --> 00:03.280]  There's no turning back, it's the best architecture, the one that scales the best,\n",
            "[00:03.280 --> 00:07.360]  the one that generates this kind of things the best, let's say.\n",
            "[00:07.360 --> 00:10.320]  And in the end, what you do is\n",
            "[00:10.320 --> 00:13.880]  you simply train for longer and you get better results.\n",
            "[00:13.880 --> 00:16.040]  Why? More or less, we don't know.\n",
            "[00:16.040 --> 00:19.400]  But that's generally what happens, so why not?\n",
            "[00:19.400 --> 00:22.680]  Instead of spending years,\n",
            "[00:22.680 --> 00:24.640]  why don't we train directly and see what happens?\n",
            "[00:24.640 --> 00:28.640]  Well, this is how machine learning research looks like today.\n",
            "~~ Transcribing VAD chunk: (20:00.007 --> 20:00.614) ~~\n",
            "[00:00.000 --> 00:02.000]  Buy it here.\n",
            "~~ Transcribing VAD chunk: (20:00.884 --> 20:30.247) ~~\n",
            "[00:00.000 --> 00:08.640]  several of these things, this is what we were saying about the issue of the emergency,\n",
            "[00:08.640 --> 00:15.000]  that is, there is this, emergence, and you can see here as we reach a level of compute,\n",
            "[00:15.000 --> 00:20.120]  boom, it hits a spectacular shot, that is, it is incredible how the models improve a lot,\n",
            "[00:20.120 --> 00:25.600]  they go to triple, quadruple performance when they do this, so let's hope that, for example,\n",
            "[00:25.600 --> 00:29.600]  Exaggerating, right? GPT-4 can have some of these characteristics.\n",
            "~~ Transcribing VAD chunk: (20:30.247 --> 20:38.431) ~~\n",
            "[00:00.000 --> 00:07.560]  so we were talking about... well there are other things and I recommend this to see this you can see here\n",
            "~~ Transcribing VAD chunk: (20:38.515 --> 21:06.528) ~~\n",
            "[00:00.000 --> 00:04.200]  When GPT-3 just came out in 2020, you can see how it took a while,\n",
            "[00:04.200 --> 00:10.200]  I mean, they were all in a ball, until more or less here open models started to come out.\n",
            "[00:10.200 --> 00:15.300]  GPT-J, this Eloiter AI, now you will see why Eloiter AI is important,\n",
            "[00:15.300 --> 00:23.300]  that they worked with Lion, Carper AI, Stabilite AI, all part of the same set,\n",
            "[00:23.300 --> 00:28.300]  to open source all this science. It's an incredible job.\n",
            "~~ Transcribing VAD chunk: (21:07.574 --> 21:35.384) ~~\n",
            "[00:00.000 --> 00:08.320]  Well, you can see how it took at least a year, almost, until all these open source versions started to come out, right?\n",
            "[00:08.320 --> 00:16.840]  So, just as OpenChatGPT came out now, we can give it a few months, maybe a year, and we will start to have those things.\n",
            "[00:16.840 --> 00:18.840]  The same with DALI.\n",
            "[00:19.760 --> 00:22.840]  The whole part of Alphafall and etc. is a little more difficult.\n",
            "[00:22.840 --> 00:27.720]  Well, with this, the only thing I wanted to show, maybe here, interesting is...\n",
            "~~ Transcribing VAD chunk: (21:38.118 --> 22:02.992) ~~\n",
            "[00:00.000 --> 00:08.080]  To train this, you need a lot of GPUs, so these are the GPUs that are available.\n",
            "[00:08.080 --> 00:10.080]  Very few companies in the world have them.\n",
            "[00:10.080 --> 00:11.920]  Here you can have them as...\n",
            "[00:11.920 --> 00:15.280]  The orange ones are national laboratories.\n",
            "[00:15.280 --> 00:17.640]  They are usually in the United States or Europe.\n",
            "[00:17.640 --> 00:20.640]  The blue ones are private companies.\n",
            "[00:20.640 --> 00:25.040]  and what is in red are, let's say, Amazon, for example.\n",
            "~~ Transcribing VAD chunk: (22:04.223 --> 22:25.317) ~~\n",
            "[00:00.000 --> 00:05.480]  But here, this is the original paper that I wanted to show you, attention is all you need.\n",
            "[00:06.580 --> 00:08.580]  A piece of paper, these guys put together.\n",
            "[00:08.960 --> 00:13.120]  Well, these people who are here, you can see the companies that came out and founded.\n",
            "[00:13.120 --> 00:19.720]  All these multimillionaires, you can see them here, Unicorn in some cases, of what they did.\n",
            "[00:19.720 --> 00:21.720]  Well, that's all for this video.\n",
            "~~ Transcribing VAD chunk: (22:26.785 --> 22:49.482) ~~\n",
            "[00:00.000 --> 00:06.080]  We talked about Transformer, we talked about OpenAI, let's see what is this GPT, what is GPT?\n",
            "[00:06.080 --> 00:11.920]  G, the G, the P and the T. The G, generative, P, pre-trained, T, transformers.\n",
            "[00:11.920 --> 00:20.040]  What is GPT-3 then? It is basically a model of transformers that is used to generate text\n",
            "[00:20.040 --> 00:23.040]  and that was trained with a gigantic amount of data.\n",
            "~~ Transcribing VAD chunk: (22:54.494 --> 23:21.258) ~~\n",
            "[00:00.000 --> 00:06.760]  When we talk about a model, what model are we talking about?\n",
            "[00:06.760 --> 00:13.480]  Well, let's go here and let's see what Andrej Karpaty did.\n",
            "[00:13.480 --> 00:17.840]  Andrej Karpaty is a guy who was the director of machine learning,\n",
            "[00:17.840 --> 00:21.160]  that is, the director of OpenAI, when it just started.\n",
            "[00:21.160 --> 00:27.040]  Elon Musk had put him money, if you remember, he had put him a thousand million dollars, more or less.\n",
            "~~ Transcribing VAD chunk: (23:21.730 --> 23:49.220) ~~\n",
            "[00:00.000 --> 00:02.880]  to start. The idea of OpenAI was that it was open.\n",
            "[00:03.520 --> 00:06.080]  So when Carpati started\n",
            "[00:07.960 --> 00:11.720]  as a DI director, after that he went to be a DI director at Tesla.\n",
            "[00:11.720 --> 00:13.720]  He was the person responsible for\n",
            "[00:14.000 --> 00:17.280]  the autonomous driving part of Tesla.\n",
            "[00:17.280 --> 00:19.280]  And a few months ago\n",
            "[00:19.280 --> 00:22.520]  he said, I don't want to be with this anymore.\n",
            "[00:22.520 --> 00:26.280]  I want to make YouTube videos, I want to make code, I want to have a good time, play the guitar, etc.\n",
            "[00:26.280 --> 00:27.560]  and thanks to God.\n",
            "~~ Transcribing VAD chunk: (23:49.726 --> 24:14.245) ~~\n",
            "[00:00.000 --> 00:06.440]  because one of the things he did is, for example, this GPT implementation, right?\n",
            "[00:06.440 --> 00:10.620]  It was sent, we can see it, AndrÃ© Capo, Capo AndrÃ©.\n",
            "[00:10.620 --> 00:17.220]  Well, here you can see the current implementations are giant, super long, impossible to run, etc.\n",
            "[00:17.220 --> 00:24.340]  type said here you have to make one that is called nano gpt that are 300 lines of code\n",
            "~~ Transcribing VAD chunk: (24:14.718 --> 24:25.602) ~~\n",
            "[00:00.000 --> 00:06.080]  that has only a couple of files, which is this, modelo.py and train.py, that's it, two files.\n",
            "[00:06.080 --> 00:11.080]  Nothing else, the rest are to help you do this, but two files and that's it.\n",
            "~~ Transcribing VAD chunk: (24:25.973 --> 24:49.177) ~~\n",
            "[00:00.000 --> 00:04.320]  That's it, with that you can have GPT-2, which is what we have here.\n",
            "[00:04.880 --> 00:10.080]  So, using this code, basically, we can train GPT-2.\n",
            "[00:10.080 --> 00:15.280]  Here in this case, this guy uses a dataset called OpenWebText\n",
            "[00:15.760 --> 00:19.520]  and this is what he uses to train the model that we see here, right?\n",
            "[00:19.520 --> 00:23.520]  It trains it with a single instance.\n",
            "~~ Transcribing VAD chunk: (24:49.328 --> 25:12.127) ~~\n",
            "[00:00.000 --> 00:08.400]  from 8 gpus from NVIDIA to 100 for 38 hours. If you remember we are going to go here to cloud\n",
            "[00:10.240 --> 00:18.880]  if you see here we said that that is this, it is a p4d, it comes out 32 mango if we put 32 mango for 38\n",
            "[00:18.880 --> 00:22.640]  it gives us 1,200 dollars well that means that\n",
            "~~ Transcribing VAD chunk: (25:13.544 --> 25:22.707) ~~\n",
            "[00:00.000 --> 00:09.360]  If we take this from Carpati with $ 1,200 we can make our own GPT-2 at the same level, same quality, same everything.\n",
            "~~ Transcribing VAD chunk: (25:41.371 --> 26:06.650) ~~\n",
            "[00:00.000 --> 00:05.880]  of OpenAI, so this is an open version, if for example we were the government of\n",
            "[00:05.880 --> 00:11.440]  a Latin American country and we wanted to show our own model, this could be a way\n",
            "[00:11.440 --> 00:21.440]  to do it, right? And speaking of those own models, if we see here, there is a plan of language technologies\n",
            "[00:21.440 --> 00:25.220]  of the Spanish government where they use this supercomputer that is in a church\n",
            "~~ Transcribing VAD chunk: (26:07.055 --> 26:33.650) ~~\n",
            "[00:00.000 --> 00:05.220]  Barcelona, her name is Maria, and basically what they do is exactly what we are talking about.\n",
            "[00:05.220 --> 00:13.680]  They train their GPT-2 model, they make it free, available, GPT-2 Base, you see, Large. They train their GPT-2 model,\n",
            "[00:13.680 --> 00:20.800]  trained with information from the National Library of Spain, we have it here on the right,\n",
            "[00:20.800 --> 00:26.200]  and this model is made available for free to the community, why? because they say\n",
            "~~ Transcribing VAD chunk: (26:33.886 --> 26:59.823) ~~\n",
            "[00:00.000 --> 00:10.000]  If we want to be competing with the US, UK, Italy, China, Russia, etc., we have to have other things.\n",
            "[00:10.000 --> 00:15.000]  And that's why I think it's important to talk about this, put it out there and share it.\n",
            "[00:15.000 --> 00:20.000]  So here you can see that they have the first generic versions of GPT-2.\n",
            "[00:20.000 --> 00:25.800]  GPT-2 but we also have Roberta for example for financial information, medical information\n",
            "~~ Transcribing VAD chunk: (26:59.907 --> 27:27.970) ~~\n",
            "[00:00.000 --> 00:05.700]  and so on, right? This is longformer, it is when we have very long text sequences and we want to\n",
            "[00:05.700 --> 00:11.460]  work, etc. So if you are, for example, a startup that wants to make a new product,\n",
            "[00:11.460 --> 00:20.860]  you can consume this, right? You want to do... you want to work with giant text documents,\n",
            "[00:20.860 --> 00:27.340]  well, then you use this model as a base, then you train yours on top and that's it, you already have your\n",
            "[00:27.340 --> 00:29.740]  start-up.\n",
            "~~ Transcribing VAD chunk: (27:28.021 --> 27:44.710) ~~\n",
            "[00:00.000 --> 00:01.540]  For example, what's going to be new?\n",
            "[00:01.540 --> 00:03.540]  So these are the kinds of things\n",
            "[00:03.540 --> 00:07.200]  that you need to know to avoid this divergence\n",
            "[00:07.200 --> 00:10.800]  between Silicon Valley and, let's say, Latin America, Spain,\n",
            "[00:10.800 --> 00:12.800]  and South America, let's put it that way, right?\n",
            "[00:12.800 --> 00:16.800]  So, we saw Carpati.\n",
            "~~ Transcribing VAD chunk: (27:46.499 --> 28:15.406) ~~\n",
            "[00:00.000 --> 00:02.000]  What is AndrÃ© doing?\n",
            "[00:02.000 --> 00:09.440]  Well, he's doing this, but this is us getting into his repo.\n",
            "[00:09.440 --> 00:13.440]  In AndrÃ©'s github, he hasn't even released it yet.\n",
            "[00:13.440 --> 00:19.440]  Basically, what he's doing is a YouTube course, which is spectacular,\n",
            "[00:19.440 --> 00:21.720]  where he goes step by step doing these things.\n",
            "[00:21.720 --> 00:26.320]  Each one, look, is a course of 1 hour and a quarter, 1 hour, etc. It's very good.\n",
            "[00:26.320 --> 00:28.320]  And this video is part of it, right?\n",
            "~~ Transcribing VAD chunk: (28:16.115 --> 28:41.731) ~~\n",
            "[00:00.000 --> 00:05.360]  of something very small explaining what the neural networks are and the backpropagation\n",
            "[00:05.360 --> 00:10.600]  and then it will go well, well, well, well up explaining that it is nano-GPT,\n",
            "[00:10.600 --> 00:19.600]  that is, what are the GPT models, that is, the guy is going from 0 to the knowledge that an OpenAI engineer has, for example,\n",
            "[00:19.600 --> 00:22.040]  and it does it for free for the community.\n",
            "[00:22.040 --> 00:26.040]  that we can see how he is talking about the scaling laws and so on, right?\n",
            "~~ Transcribing VAD chunk: (28:42.743 --> 29:06.571) ~~\n",
            "[00:00.000 --> 00:04.640]  and we are going to see a little what those scaling laws are also at some point, no problem.\n",
            "[00:04.640 --> 00:16.920]  Well, we saw that they are the LLMs and that it is GPT-3, we saw the market and we are going to see now\n",
            "[00:16.920 --> 00:23.840]  basically very quickly what is this about Sparrow, DeepMind and ChatGPT.\n",
            "~~ Transcribing VAD chunk: (29:07.718 --> 29:31.850) ~~\n",
            "[00:00.000 --> 00:06.800]  For that we are going to go to this video that Letizia made. Letizia has an excellent YouTube channel called Miss Coffee Bean.\n",
            "[00:06.800 --> 00:12.400]  Miss Coffee Bean is this thing down here on the left and she explains a lot of topics.\n",
            "[00:12.400 --> 00:20.320]  She is in the academy and she makes very good summaries but she also gets into the technical details a lot.\n",
            "[00:20.320 --> 00:24.320]  so those are things that, at least for me, I like a lot because they are usually missing\n",
            "~~ Transcribing VAD chunk: (29:32.677 --> 29:48.691) ~~\n",
            "[00:00.000 --> 00:10.000]  In this case, we are going to see what DeepMind is doing and how this is compared to GPT chat, right?\n",
            "[00:10.000 --> 00:16.000]  First of all, let's see what was chatgpt, we didn't see it, we skipped it, we are at minute 30 more or less and we didn't see what it was.\n",
            "~~ Transcribing VAD chunk: (29:49.433 --> 30:15.961) ~~\n",
            "[00:00.000 --> 00:05.580]  ChatGPT is this, there is already a video, I put it in the description, it is a video that explains more or less what it is,\n",
            "[00:05.580 --> 00:13.100]  all the people are talking about this, I suppose that if you are watching this video it is because you are interested in the PI, you are interested in the language models,\n",
            "[00:13.100 --> 00:19.900]  you know what ChatGPT is and you would be interested in learning a little more, those are my assumptions of all this.\n",
            "[00:19.900 --> 00:26.900]  You can see here how I wrote a script for YouTube on a mini course that I'm building to be able to train from the beginning.\n",
            "~~ Transcribing VAD chunk: (30:17.530 --> 30:41.223) ~~\n",
            "[00:00.000 --> 00:06.440]  You can see here how chat.gpt responds, gives me an index of the content, a conclusion and an intro.\n",
            "[00:06.440 --> 00:11.100]  And then I say, dude, why don't you just make a list of 10 titles that I can add to that one?\n",
            "[00:11.100 --> 00:15.600]  I'm sure it's the most visited, just to screw it up I guess.\n",
            "[00:15.600 --> 00:19.940]  And here you can see what it writes, right? How to train from scratch, etc.\n",
            "[00:19.940 --> 00:23.940]  And well, I changed the title a bit, but more or less that's what it's telling us.\n",
            "~~ Transcribing VAD chunk: (30:42.202 --> 31:02.688) ~~\n",
            "[00:00.000 --> 00:03.260]  and then I tell you, let's make a short description of everything we talked about, right?\n",
            "[00:03.260 --> 00:05.760]  all this conversation that we have is remembered, why?\n",
            "[00:05.760 --> 00:11.320]  because it has a window that is said to be of 8096 tokens, it is a fairly long window\n",
            "[00:11.320 --> 00:16.280]  and it is remembered of these things, if I keep writing, at some point it will be forgotten\n",
            "[00:16.280 --> 00:20.280]  because it has up to 8,000 tokens\n",
            "~~ Transcribing VAD chunk: (31:03.548 --> 31:33.367) ~~\n",
            "[00:00.000 --> 00:07.120]  which is then OpenAI, this ChatGPT, is a model that what it does basically,\n",
            "[00:07.120 --> 00:11.840]  and obviously there is no paper that tells us, just use it, what it is telling us,\n",
            "[00:11.840 --> 00:17.800]  there is no paper that tells us, but the way it works is, we have a generation model of language,\n",
            "[00:17.800 --> 00:26.160]  which is GPT-3, simple, and then what they did is train a policy where they say\n",
            "[00:26.160 --> 00:30.160]  this type of, that is, enter a model basically that says, and I like it more if it says\n",
            "~~ Transcribing VAD chunk: (31:33.097 --> 31:48.706) ~~\n",
            "[00:00.000 --> 00:03.440]  I like this type of answer more than this other type of answer\n",
            "[00:03.440 --> 00:10.360]  I mean if you answer, can you write me a list of 10 titles for harassment and the chatbot will answer\n",
            "[00:10.360 --> 00:12.360]  but who is talking to me here?\n",
            "[00:12.360 --> 00:14.360]  well, if he answers you like that, it's bad\n",
            "[00:14.360 --> 00:16.360]  Very bad, isn't it?\n",
            "~~ Transcribing VAD chunk: (31:53.498 --> 32:09.901) ~~\n",
            "[00:00.000 --> 00:05.000]  It has to be useful, right? I mean, it has to give you answers, it can't be answering all the time\n",
            "[00:05.000 --> 00:10.500]  I don't know, I don't know the answer, ask me again, I don't know, I don't know, it can't be like that, it has to be useful\n",
            "[00:10.500 --> 00:16.500]  It has to be honest, that means that these models generally hallucinate the answers, that is, they invent them.\n",
            "~~ Transcribing VAD chunk: (32:10.947 --> 32:29.695) ~~\n",
            "[00:00.000 --> 00:06.440]  They come up with grammatically correct answers, but they are actually incorrect, right?\n",
            "[00:06.440 --> 00:10.160]  If I tell this guy, I can convince him of something.\n",
            "[00:10.160 --> 00:15.240]  No, it's actually not this thing, it's this other thing, and the model is going to believe it.\n",
            "[00:15.240 --> 00:19.240]  So he's going to answer me with that. He's not honest. He's hallucinating and he's basically lying.\n",
            "~~ Transcribing VAD chunk: (32:31.906 --> 32:57.353) ~~\n",
            "[00:00.000 --> 00:03.280]  I'm not lying, I was giving false information.\n",
            "[00:03.280 --> 00:07.580]  So what we do is create a model that does this classification of\n",
            "[00:07.580 --> 00:10.960]  look, I like the answer D better, then C, then A, then B\n",
            "[00:10.960 --> 00:14.460]  and that's the model we have. So once we have that model\n",
            "[00:14.460 --> 00:20.720]  we train using a reinforcement learning algorithm, we train an optimization policy that says\n",
            "[00:20.720 --> 00:24.920]  look, go more to the side of this type of answers that are better, that is, don't say\n",
            "[00:24.920 --> 00:26.920]  hehehehe\n",
            "~~ Transcribing VAD chunk: (32:58.062 --> 33:27.121) ~~\n",
            "[00:00.000 --> 00:03.640]  What's it called? If someone asks you a question and you don't answer, shut up, troll,\n",
            "[00:03.640 --> 00:05.640]  but rather answer them,\n",
            "[00:05.640 --> 00:08.840]  well, tell them, what a fantastic question, incredible,\n",
            "[00:08.840 --> 00:10.840]  this is my answer, it's like that, etc.\n",
            "[00:10.840 --> 00:12.840]  So, honest, useful,\n",
            "[00:12.840 --> 00:15.400]  and the last one is that it's not toxic.\n",
            "[00:15.400 --> 00:21.880]  That means that it doesn't generate content that goes against demographics, for example,\n",
            "[00:21.880 --> 00:28.160]  that promotes bias in the information, in the data with which it was trained, etc.\n",
            "[00:28.160 --> 00:29.160]  all this kind of things.\n",
            "~~ Transcribing VAD chunk: (33:28.420 --> 33:47.017) ~~\n",
            "[00:00.000 --> 00:04.160]  Let's go back here. Sparrow. What is Sparrow? Sparrow is a paper that came out earlier than\n",
            "[00:04.160 --> 00:11.480]  ChatGPT, it came out in September. ChatGPT comes out on November 30, this comes out on September 22, but\n",
            "[00:11.480 --> 00:18.520]  DeepMind, unlike OpenAI, does not make its model available, but it does make a paper available, right?\n",
            "~~ Transcribing VAD chunk: (33:57.209 --> 34:24.648) ~~\n",
            "[00:00.000 --> 00:06.060]  This video that Letizia made is excellent, so I recommend it, it's pretty good.\n",
            "[00:06.060 --> 00:10.740]  And let's see how it works.\n",
            "[00:10.740 --> 00:14.440]  So, generally, these models that we have here, what we said before,\n",
            "[00:14.440 --> 00:18.020]  try to predict the next word in a sequence.\n",
            "[00:18.020 --> 00:21.860]  The cat sat on the... well, he sat on the carpet.\n",
            "[00:21.860 --> 00:25.000]  He wasn't hungry, but he was angry, etc.\n",
            "[00:25.000 --> 00:27.560]  Well, the point is that what we do is...\n",
            "~~ Transcribing VAD chunk: (34:26.926 --> 34:50.652) ~~\n",
            "[00:00.000 --> 00:02.000]  When I write this prompt here\n",
            "[00:03.080 --> 00:05.320]  write me a script for blah blah blah blah blah\n",
            "[00:05.320 --> 00:08.160]  it's not that this gpt simply gets this\n",
            "[00:08.160 --> 00:11.080]  write me a prompt for blah blah blah blah write me a script, right?\n",
            "[00:11.080 --> 00:14.160]  There's a prompt before this that we don't know very well what it is\n",
            "[00:14.160 --> 00:16.160]  there are people who write here and say\n",
            "[00:16.160 --> 00:18.160]  decimetus, directives, etc.\n",
            "[00:18.160 --> 00:19.880]  and something comes up\n",
            "[00:19.880 --> 00:21.160]  but that changes\n",
            "[00:21.160 --> 00:23.720]  that is, that is going to change very quickly\n",
            "~~ Transcribing VAD chunk: (34:51.226 --> 35:20.926) ~~\n",
            "[00:00.000 --> 00:04.460]  So, what we have here is, for example, the prompt, at the beginning is\n",
            "[00:04.460 --> 00:13.660]  well, you are a conversational AI, very useful and friendly and blah blah blah\n",
            "[00:13.660 --> 00:16.400]  and then there begins the sentence, right?\n",
            "[00:16.400 --> 00:20.660]  A conversation is invented, it's not that we just write this and that's it.\n",
            "[00:20.660 --> 00:24.780]  There is a conversation that is happening that we don't see, right?\n",
            "[00:24.780 --> 00:26.780]  And we ask the question, where is Paris?\n",
            "[00:26.780 --> 00:29.680]  And we say Paris is in France, etc.\n",
            "[00:29.680 --> 00:31.160]  Create. Share. Learn.\n",
            "~~ Transcribing VAD chunk: (35:21.837 --> 35:40.383) ~~\n",
            "[00:00.000 --> 00:04.140]  There's something called in-context fusion learning,\n",
            "[00:04.140 --> 00:08.800]  which is when we explain a use case within the prompt.\n",
            "[00:08.800 --> 00:10.800]  For example, we say,\n",
            "[00:12.440 --> 00:16.080]  I used it to make a chapter of a TV show called Los Simuladores,\n",
            "[00:16.080 --> 00:18.580]  a series that came out in Argentina.\n",
            "~~ Transcribing VAD chunk: (35:40.670 --> 36:08.513) ~~\n",
            "[00:00.000 --> 00:05.520]  So I say, the simulators is a series that, and I copy-paste the synopsis of the series.\n",
            "[00:05.520 --> 00:07.280]  And then I say, let's make a new chapter.\n",
            "[00:07.280 --> 00:10.840]  So there I told ChatGPT what the simulators were.\n",
            "[00:10.840 --> 00:14.600]  If I write, let's make a new chapter of the simulators, it doesn't know what the simulators are.\n",
            "[00:14.600 --> 00:17.200]  Now, if I want to write a style, for example,\n",
            "[00:17.200 --> 00:21.520]  I can say, look, a cumbia song is like this, and I write a new cumbia song.\n",
            "[00:21.520 --> 00:22.720]  And there it can answer you.\n",
            "[00:22.720 --> 00:27.120]  So that would be called in-context pre-shot learning, right?\n",
            "[00:27.120 --> 00:29.120]  Ehm...\n",
            "~~ Transcribing VAD chunk: (36:10.218 --> 36:39.850) ~~\n",
            "[00:00.000 --> 00:04.100]  So, let's see what DeepMind did.\n",
            "[00:04.100 --> 00:09.260]  This is not called a chatbot, it's called a dialogue agent or conversational AI.\n",
            "[00:09.260 --> 00:13.840]  Why? Because the word chatbot, people didn't like to use it.\n",
            "[00:13.840 --> 00:17.680]  Because chatbots never worked well.\n",
            "[00:17.680 --> 00:21.000]  Until today, until chatgpt releases this version.\n",
            "[00:21.000 --> 00:23.000]  But before that, they didn't work that well.\n",
            "[00:23.400 --> 00:25.400]  So, how does it work?\n",
            "[00:25.400 --> 00:29.400]  They are taking a language model called Chinchilla.\n",
            "~~ Transcribing VAD chunk: (36:40.492 --> 37:07.576) ~~\n",
            "[00:00.000 --> 00:04.680]  For some reason DeepMind chose to put animal names to all its latest\n",
            "[00:04.680 --> 00:10.600]  Transformers language models. So you have cat, chinchilla, gopher, etc.\n",
            "[00:10.600 --> 00:16.880]  Just so you know. Well, what does it do? We have this, we have rules,\n",
            "[00:16.880 --> 00:20.520]  we have human feedback, we have the language model and we have classifiers.\n",
            "[00:20.520 --> 00:27.080]  What does all this mean? Well, we are going to go here and we are going to see it here, in this paper.\n",
            "~~ Transcribing VAD chunk: (37:07.930 --> 37:36.888) ~~\n",
            "[00:00.000 --> 00:04.200]  This means that we have the model and then we have two things that we are doing.\n",
            "[00:04.200 --> 00:07.920]  When we say these two things, this means that someone sat down,\n",
            "[00:07.920 --> 00:11.640]  that is, they paid people to sit down and say,\n",
            "[00:11.640 --> 00:16.560]  man, of all these answers that were given, I like the first one better,\n",
            "[00:16.560 --> 00:19.920]  I like the second one less, I don't like the third one at all.\n",
            "[00:19.920 --> 00:27.960]  And then they also sat people down to ask them if the answer Sparrow gave\n",
            "[00:27.960 --> 00:29.960]  goes against\n",
            "~~ Transcribing VAD chunk: (37:37.141 --> 38:04.512) ~~\n",
            "[00:00.000 --> 00:02.000]  of rules that have already been set.\n",
            "[00:02.000 --> 00:04.000]  What do rules mean?\n",
            "[00:04.000 --> 00:06.000]  They have been set.\n",
            "[00:06.000 --> 00:09.000]  Here you can see what the prompt is like.\n",
            "[00:09.000 --> 00:11.000]  We said there was a prompt before.\n",
            "[00:11.000 --> 00:13.000]  The following is a conversation between\n",
            "[00:13.000 --> 00:17.000]  a virtual assistant\n",
            "[00:17.000 --> 00:19.000]  who has a lot of knowledge, is very intelligent,\n",
            "[00:19.000 --> 00:22.000]  his name is Sparrow, and a human user\n",
            "[00:22.000 --> 00:24.000]  that we are going to call user.\n",
            "[00:24.000 --> 00:28.000]  Here we can see how this conversation starts.\n",
            "~~ Transcribing VAD chunk: (38:04.698 --> 38:34.415) ~~\n",
            "[00:00.000 --> 00:04.000]  These are the examples that are used to train the model, right?\n",
            "[00:06.000 --> 00:10.000]  What we have here, let's see if I can find it, has a couple of rules\n",
            "[00:11.000 --> 00:15.000]  that are, for example, not here.\n",
            "[00:15.000 --> 00:19.000]  It says that it doesn't have stereotypes, that it doesn't do microaggressions,\n",
            "[00:19.000 --> 00:22.000]  that it doesn't do threats, that it doesn't do sexual aggressions,\n",
            "[00:22.000 --> 00:25.000]  that it doesn't attack by identity, that it doesn't insult, etc.\n",
            "[00:25.000 --> 00:28.000]  These are the rules that we are setting.\n",
            "[00:28.000 --> 00:30.000]  If we go back here...\n",
            "~~ Transcribing VAD chunk: (38:34.583 --> 39:01.482) ~~\n",
            "[00:00.000 --> 00:04.400]  this model, what was it saying? It was saying, are you violating some rule? When it is answering,\n",
            "[00:04.400 --> 00:11.960]  when it says, it is a choto, it is insulting me. Well, the rule it violated is the one to insult, for example,\n",
            "[00:11.960 --> 00:22.880]  so that's what we have here. And the other, what we do is, basically, these people\n",
            "[00:22.880 --> 00:26.360]  choose this answer, which is the best answer and with that we train.\n",
            "~~ Transcribing VAD chunk: (39:02.123 --> 39:31.722) ~~\n",
            "[00:00.000 --> 00:03.720]  Once we have these two models, we have one of preference and another of rules,\n",
            "[00:03.720 --> 00:07.860]  what we do is we train with reinforcement learning, basically saying,\n",
            "[00:07.860 --> 00:17.140]  look, I want you to give more importance, more relevance to what this model has told you is the best answer,\n",
            "[00:17.140 --> 00:22.200]  and I want you to give less importance, that is, to punish, to penalize quite seriously,\n",
            "[00:22.200 --> 00:26.920]  to the models, to the answers they have given you, to be insulted, for example.\n",
            "[00:26.920 --> 00:29.920]  So, based on these two things, this tells us that...\n",
            "~~ Transcribing VAD chunk: (39:31.739 --> 39:59.752) ~~\n",
            "[00:00.000 --> 00:04.920]  the model of this reinforcement learning is already telling us what are the answers,\n",
            "[00:04.920 --> 00:09.320]  the type of answer we want from this chatbot. So what does this mean? That we have an\n",
            "[00:09.320 --> 00:14.120]  original model, which is the language model, and then we have a new model that exists,\n",
            "[00:14.120 --> 00:21.280]  which is a model that is, let's say, optimized to be conversational and to generate\n",
            "[00:21.280 --> 00:26.480]  responses that humans like, one, and responses that do not insult, do not degrade, that are not\n",
            "[00:26.480 --> 00:30.120]  aggressive etc.\n",
            "~~ Transcribing VAD chunk: (40:01.844 --> 40:29.030) ~~\n",
            "[00:00.000 --> 00:03.680]  We have to train the model again because we had already talked about it before that it was like millions of dollars\n",
            "[00:03.680 --> 00:06.000]  to train our models. No, you don't have to train it again.\n",
            "[00:06.000 --> 00:12.320]  What DeepMind does is basically grabs this model, here it says, Leticia,\n",
            "[00:13.880 --> 00:17.200]  freezes the 64 layers that Chinchilla has\n",
            "[00:17.200 --> 00:21.360]  and only does fine tuning in the last 16 layers.\n",
            "[00:21.360 --> 00:26.960]  This means that we don't have to train the entire model, but only a part of it.\n",
            "~~ Transcribing VAD chunk: (40:29.384 --> 40:56.080) ~~\n",
            "[00:00.000 --> 00:03.380]  What do these big features mean to us? Why do we care about this?\n",
            "[00:03.380 --> 00:09.980]  We would care about this because if we see it as a pyramid, we are going to have very few companies.\n",
            "[00:09.980 --> 00:14.640]  Do you remember? If we go back, here,\n",
            "[00:14.640 --> 00:17.100]  what are the companies that are making this model of language?\n",
            "[00:17.100 --> 00:22.020]  Google, DeepMind, Facebook, Microsoft, OpenAI, AI21.\n",
            "[00:22.020 --> 00:25.580]  There are very few, very few that make these giant models.\n",
            "[00:25.580 --> 00:27.580]  Gigantic, immense, right?\n",
            "~~ Transcribing VAD chunk: (40:57.295 --> 41:25.578) ~~\n",
            "[00:00.000 --> 00:07.320]  So it is very difficult that a government of Latin America can do one of these, that the free market can do one of these models.\n",
            "[00:07.880 --> 00:09.120]  Impossible, very difficult.\n",
            "[00:09.560 --> 00:19.680]  Now, if it is possible that we take a smaller language model, like Chinchilla, for example, or like GLM 130,\n",
            "[00:20.360 --> 00:24.960]  we take it and then we create a reward model on top of it,\n",
            "[00:24.960 --> 00:28.960]  and be the one who...\n",
            "~~ Transcribing VAD chunk: (41:26.118 --> 41:55.463) ~~\n",
            "[00:00.000 --> 00:06.360]  let's say the one that generates this new model, so that reward model that we have is going to be\n",
            "[00:06.360 --> 00:13.960]  very close to the dataset that we are using and that is going to be only ours, that is going to be the owner,\n",
            "[00:13.960 --> 00:19.480]  that is going to be the owner of a free market company or of a Latin country, etc. that wants to do it.\n",
            "[00:19.480 --> 00:28.720]  So that's where we are already participating in this whole generation of models. So once we have\n",
            "[00:28.720 --> 00:31.360]  this model.\n",
            "~~ Transcribing VAD chunk: (41:55.885 --> 42:19.072) ~~\n",
            "[00:00.000 --> 00:05.760]  You can see that they train it with this dataset,\n",
            "[00:05.760 --> 00:10.260]  explainme.licam5, and then it generates this, basically.\n",
            "[00:10.260 --> 00:14.760]  One more detail about Sparrow, which is something that doesn't have chat.gpt.\n",
            "[00:14.760 --> 00:18.260]  Sparrow, when we ask it a question,\n",
            "[00:18.260 --> 00:23.260]  can look for evidence. And this is very interesting.\n",
            "~~ Transcribing VAD chunk: (42:19.240 --> 42:47.354) ~~\n",
            "[00:00.000 --> 00:06.000]  This means that the chatbot is interacting with the external model, with the external world.\n",
            "[00:06.440 --> 00:08.440]  And we can see it here.\n",
            "[00:08.800 --> 00:10.800]  More or less. It was back here.\n",
            "[00:11.280 --> 00:14.280]  Let's go back. You can see that it says search query, search results.\n",
            "[00:14.600 --> 00:17.080]  So when we ask a question, for example,\n",
            "[00:17.480 --> 00:22.480]  who is the president of Brazil, for example,\n",
            "[00:23.680 --> 00:26.160]  and we use a language model, it will have knowledge up to a point.\n",
            "[00:26.560 --> 00:28.560]  Right? That's it. If we ask here,\n",
            "~~ Transcribing VAD chunk: (42:48.890 --> 43:16.835) ~~\n",
            "[00:00.000 --> 00:04.000]  Who is the president of Brazil, right?\n",
            "[00:05.280 --> 00:07.280]  We have to go back in.\n",
            "[00:07.280 --> 00:09.280]  We are out of access.\n",
            "[00:09.280 --> 00:11.280]  Let's see, but if we ask the question\n",
            "[00:15.040 --> 00:17.040]  We know that ChatGPT\n",
            "[00:18.680 --> 00:20.680]  is trained\n",
            "[00:20.680 --> 00:24.080]  See? ChatGPT is trained until 2021.\n",
            "[00:24.080 --> 00:27.280]  So until 2021, Jair Bolsonaro is the president.\n",
            "[00:27.280 --> 00:28.280]  Now it's LUL\n",
            "~~ Transcribing VAD chunk: (43:17.881 --> 43:45.455) ~~\n",
            "[00:00.000 --> 00:08.420]  So it doesn't know. That means that this model is not so good, it is not so useful because it will always be out of date.\n",
            "[00:08.420 --> 00:10.420]  So what does Sparrow do?\n",
            "[00:10.420 --> 00:19.920]  When we ask that question, it does a search in Google and receives the results of the first pages.\n",
            "[00:19.920 --> 00:27.840]  So it uses that, puts it inside the prompt and then, based on what came out of that prompt, it can answer it.\n",
            "~~ Transcribing VAD chunk: (43:46.568 --> 44:14.463) ~~\n",
            "[00:00.000 --> 00:06.700]  So, this is also interesting, because just as we are telling it to do the search in Google,\n",
            "[00:06.700 --> 00:11.100]  it could also do the search in our own source of knowledge, right?\n",
            "[00:11.100 --> 00:15.300]  It could be the National Library of Spain, for example, or the Argentine Library, or etc.\n",
            "[00:15.300 --> 00:19.100]  Or a database of medical papers.\n",
            "[00:19.100 --> 00:23.800]  And we say then, read all these medical papers, bring me the information, combine it.\n",
            "[00:23.800 --> 00:28.400]  And that means it can give the source of where it came from, which is what we have here.\n",
            "~~ Transcribing VAD chunk: (44:15.374 --> 44:31.625) ~~\n",
            "[00:00.000 --> 00:06.240]  Sparrow responds with evidence, which is something that ChatGPT does not say, ChatGPT can make up\n",
            "[00:06.240 --> 00:14.780]  anything. For example, if I tell you that it is not true, the current president is Lula, it is Lula.\n",
            "~~ Transcribing VAD chunk: (44:34.375 --> 44:55.638) ~~\n",
            "[00:00.000 --> 00:08.000]  We write this and here we could start changing it because again this would be a kind of in-context learning that we are doing.\n",
            "[00:08.000 --> 00:12.000]  We are writing and this is going to change.\n",
            "[00:12.000 --> 00:19.000]  Now when we leave this conversation that we have, the model is not that it learned that now the president is Lula, it simply forgets it.\n",
            "[00:19.000 --> 00:22.000]  It only works in the conversation we are having.\n",
            "~~ Transcribing VAD chunk: (44:56.465 --> 45:06.235) ~~\n",
            "[00:00.000 --> 00:06.000]  I write this to him, he's going to answer me if the president is Lula, and now when I ask him who the president of Brazil is, he's going to answer me if the president is Lula.\n",
            "[00:06.000 --> 00:10.000]  So it's like he understood, but in reality it's just the conversation we're having.\n",
            "~~ Transcribing VAD chunk: (45:07.501 --> 45:35.615) ~~\n",
            "[00:00.000 --> 00:10.500]  So well, the last thing I'm going to say, because as always this is very easy, I'm not aware of it, etc.\n",
            "[00:10.500 --> 00:16.000]  This is very easy to go to hell, it's a very difficult topic, so we're going to separate it into parts.\n",
            "[00:16.000 --> 00:21.000]  The only thing I want to show is what steps we have to take to train our chat GPT.\n",
            "[00:21.000 --> 00:26.000]  Well, this is what we are going to be seeing in this series of videos that we are going to do.\n",
            "[00:26.000 --> 00:28.400]  Let's talk about Lion and Open Assistant.\n",
            "~~ Transcribing VAD chunk: (45:36.138 --> 45:53.047) ~~\n",
            "[00:00.000 --> 00:06.360]  which is an assistant that can understand tasks, can interact with external systems,\n",
            "[00:06.360 --> 00:11.960]  such as search, and can retrieve information from the outside world,\n",
            "[00:11.960 --> 00:16.960]  which is exactly what we want to do, so it connects very well with DeepMind.\n",
            "~~ Transcribing VAD chunk: (45:53.924 --> 46:22.578) ~~\n",
            "[00:00.000 --> 00:05.120]  what they do. So here we have it, we are going to see it in more detail, what is this but you can\n",
            "[00:05.120 --> 00:11.440]  see here what you want to do, the roadmap. First, this is going to come out this year, they are going to\n",
            "[00:11.440 --> 00:17.720]  put a lot of work into doing these things, you can see that Yannick Kilger is working on this, Yannick\n",
            "[00:17.720 --> 00:23.800]  has an excellent YouTube channel, I highly recommend it and he is putting a lot of work into these\n",
            "[00:23.800 --> 00:28.480]  things so the idea is to generate an open version of\n",
            "~~ Transcribing VAD chunk: (46:22.797 --> 46:37.512) ~~\n",
            "[00:00.000 --> 00:06.800]  of chat gpt and that is what we are going to try to do with this step by step step by step we are going to\n",
            "[00:06.800 --> 00:13.800]  You can see how the steps we have are, first we collect human demonstrations.\n",
            "~~ Transcribing VAD chunk: (46:45.308 --> 47:02.234) ~~\n",
            "[00:00.000 --> 00:10.720]  And here you will see, the second step would be to do the fine-tuning to a base model and the third is to collect these instructions, which is what we want to do.\n",
            "[00:10.720 --> 00:13.120]  And there we are going to see it.\n",
            "[00:13.120 --> 00:17.120]  We train the reward model, which is what we had seen in ChatsGPT, right?\n",
            "~~ Transcribing VAD chunk: (47:03.365 --> 47:33.166) ~~\n",
            "[00:00.000 --> 00:06.240]  this second one, then we already have the language model, the open model, we want to enter the reward model\n",
            "[00:06.240 --> 00:13.120]  and then we want to do this, well this is what we are going to see how it is done in this series of videos\n",
            "[00:13.120 --> 00:19.640]  as always this is a very long video, very difficult to do because without editing, without anything, just talking\n",
            "[00:19.640 --> 00:25.280]  there are many things that I surely forgot, I ask you please to write to me if you find it interesting\n",
            "[00:25.280 --> 00:29.720]  write in the comments what things are missing and let's have a conversation, let this be\n",
            "~~ Transcribing VAD chunk: (47:34.769 --> 47:51.442) ~~\n",
            "[00:00.000 --> 00:04.300]  the Spanish version of wanting to do chat gpt\n",
            "[00:04.300 --> 00:08.800]  that it is not just a video of how to use it, how to make you rich, how to make you etc.\n",
            "[00:08.800 --> 00:12.400]  that this is learning, learning really how these things work\n",
            "[00:12.400 --> 00:14.400]  So with that I send you...\n",
            "[00:14.400 --> 00:16.400]  I send you\n",
            "~~ Transcribing VAD chunk: (47:52.032 --> 48:15.961) ~~\n",
            "[00:00.000 --> 00:03.380]  A giant hug, thanks for staying there until the end\n",
            "[00:03.380 --> 00:09.660]  We talked about 48 minutes of language model, which is very difficult in this day and age\n",
            "[00:09.660 --> 00:15.200]  So if you're here, up to this point, dude, congratulations\n",
            "[00:15.200 --> 00:21.380]  Because it means that you will surely do very cool things with all this\n",
            "[00:21.380 --> 00:23.380]  I send you a big hug, bye bye!\n",
            "Performing alignment...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### transcripciÃ³n en espaÃ±ol con VAD"
      ],
      "metadata": {
        "id": "rUV1TpVXrsy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !whisperx archivo.wav --hf_token $hf_token --model large-v2 --language es --vad_filter --align_model WAV2VEC2_ASR_LARGE_LV60K_960H "
      ],
      "metadata": {
        "id": "O9O_o7Ujr1JS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}