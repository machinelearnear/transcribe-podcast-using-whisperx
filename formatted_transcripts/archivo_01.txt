Hello again to the channel, if this is your first time here, what I try to do is Spanish divulgation of things that are coming out in Machine Learning, trying to explain it in a way that is a little easier to understand and also to use.
Today what I wanted to talk about is Alpaca, which is a new model that was trained by Stamford, based on a model called Lama from Meta, and how with very little money
Comparatively, you can do something that works very well at the GPT chat level,
but also that it can be more accessible and efficient for everyone.
So, as always, we are going to divide the video into parts.
First, I would like to talk about what this Stanford Alpaca model is.
Then see a little what was the dataset that was used to train this model.
Why is all this important?
Then some crazy people, the truth, compiled this in C++.
So this works very efficiently and uses very little hardware.
And then someone also begins to give their opinion on how to train this model
how much we would get, for example, to have
a new version and finally we are going to see Cabrita which is a Brazilian version of Lama
that was made to test the concept, how much would it really be needed for how much money
you need to be able to have one of these modules so we are going to get to see all this and
I'm going to try to cover a lot of things so it is possible that I forget several things or that I do not explain them in a very clear way, so I ask you to put in the comments any questions you have left after all this.
I'm going to try to be as concise as possible, which is generally a bit difficult for me.
Well, first of all, what is this about Alpaca? Well, Alpaca, 7 billion parameters, is a model that is a fine-tuned version of the Lama model that took the goal, that is, it is trained in 52,000 examples that follow instructions.
The important thing about this is that what these people from Stanford found is that it works very similar to Davinci 003, which is GPT 3.5 and it is quite cheap to make this model, that is, it costs less than $ 600 and we are going to see exactly what it means.
First of all, what is this llama thing? Let's go back a bit. What was this llama thing?
Well, that's it.
Facebook released just a few weeks ago these four models, 7,000, 13,000, 33,000, 65,000, millions of parameters.
It trained them completely in public datasets, such as Crawl, C4, GitHub, Wikipedia, Books, etc.
All this makes a total of about 4.75...
terabytes of data and generates more or less, I think it's 1.4 trillion tokens, right?
To put it in a comparison, when GPT-3 was trained, 300 billion tokens were used.
And in this case we can see how the models, that is, the loss, the training loss,
we can see that it always goes down, right? So this performance is pretty good, right?
that is, the model of 7 billion parameters, perhaps
If he had trained for more time, he would not have reached the same performance as the one of 13 billion parameters.
So, what did Facebook do when it released these models?
It made them open source, they had to be written down in a waitlist, but eventually the checkpoints were leaked
and these models were going around everywhere.
Now, what happened? What was the problem with this model?
Well, the Lama model.
It was trained to complete text, but not in instructions.
So when you chatted with the model, it was pretty bad.
Pretty bad.
So here what Stanford is saying is,
look, the models that follow instructions,
such as GPT 3.5, ChatGPT, Cloud, and Bing's chat,
they are becoming more and more powerful, right?
So, what they are saying here is, well, here we are going to show you what are our results of having done a fine tuning on these models, right?
Here they explain a little bit, etc., how they are doing everything.
The interesting thing is that they do not make the models available for us to download, but
Yes, they are giving us the code they used to train the model and also to generate the data.
But hey, they tell us here that at some point we are going to get them out, we still don't know how.
but what they did was put an interactive demo where we can interact with this model.
One thing they are telling us is that the alpaca model is only for academic uses and any
commercial use is completely prohibited. Why is it prohibited? For three things. First,
Lama, these models have a non-commercial license, so all the variations that are made of
that model are also non-commercial. Second, when they generated the dataset, and now we are going to see
specifically how they did it, they used an OpenAI API.
But if you get into the terms of use of this API, what it is telling you is that models that compete with OpenAI cannot be generated.
So, Alpaca clearly competes with OpenAI, so this could not be used legally.
and finally since they did not make any security measures then they say better
we do not take out alpaca for general use, right?
So, first of all, about Lama, I would like to say that I had made a video a while ago about this,
so if you want to see it, there is a demo, there is also an explanation of the paper and there is a lot of information on that.
I will put the links of everything you are seeing here in the description. So let's go back to this.
How does this work to train or generate data? How was this information generated?
Well, here what they are doing is...
is that, what they say is, look, generally to train a good model that does, that can generate
instructions without a lot of money, what we need is a
and
a good model of language and then we need some type of data that is of high quality,
where these things are explained, how to follow instructions. Now we are going to see what that means.
Generally, how is it done? For example, when the first paper came out, InstractGPT, which was the first
version of ChatGPT.
The way they did it was, they simply generated more or less about 50,000 examples,
they paid people, they paid people and said sit down, start writing and write, for example, this instruction.
I wrote a poem, etc. and then they put a person who writes the poem, right?
So they did it like this 50,000 times and that was what they used to tell these language models,
Look, it's fine how you behave with the English idiom, but I want you to...
follow that you behave in this way, that is, that you know how to follow instructions basically,
more similar to a conversation with a user. So here what it is telling us is, well,
the first part, all this of the language model is very good, we already have it because that is what
Lama does, we already know that they work quite well. Now for the second part we are going to follow a paper
which is called Self-Instruct.
what it is telling us is, look, we are going to use the same model that we said here, Lama,
and we are going to use it to generate this instruction data, right? I mean, what does this mean?
What it means is that we are going to start from 175 instructions, right? That are made by a person,
that we can see here. If we open this and go to the paper
which seems to me to be, well, now we are going to see it, I have the list, I have the list there of how the example is,
but basically we have instructions that are similar in this way, right?
I mean, we have the instruction that says, for example, in this case,
brainstorm a list of possible things that I want to have done for the new year, right?
And here we have that this is the instruction that we send it and the output that we expect from the model is
lose weight, do more exercise, eat better.
etc. Well, these things...
Then what they say is, well, we are going to take these 175 examples and we are going to use them as a base
to call the GPT-3 API from OpenAI and we are going to generate these 52,000 cases.
So here you can see that they also gave us the code to be able to do it and they have it in the github that we are going to see in a while.
When they did this, 50,000 examples, it cost them more or less about 500 dollars, that is, approximately
one cent, for example.
Imagine that you give this to, for example, some contracts and tell them to make me 50,000 examples of this.
Obviously it's going to be a lot more expensive than a cent, for example.
So, once we have these 52,000 examples of instructions,
we do what is known as a supervised fine-tuning of the model, right?
And for that, here what they are using is a training framework from HimeFace
that can distribute the training in several GPUs, etc.
here what they are saying is that we use three hours in eight
NVIDIA A100, etc.
This is, well, it cost $100 to do that.
This is huge, huge, it's a
giant cluster of computers, more or less this costs
about $500,000. You can see that they used it
here in cloud and, well, in three hours
they can do it. But this, if you wanted to have a
of this, it would be impossible to be able to do it.
Well, how did they do the evaluation of this? They generated the results, they generated the model and then they began to evaluate it and the evaluation was done by the five people who made the paper that we have up here.
and took an evaluation set and basically did what would be a blind test and compared the
results the generations that had GPT 3.5 and alpaca and well they said that here alpaca wins 90 of 89 times
in this about that so what they are basically saying is look at the model that we have just
In general, it has the same performance as...
gpt 3.5 at least in the tests that we have just done and here they have let's say these things
here we can see that it is an alpaca as it is different from a llama and here it tells us good an alpaca
is a domesticated species of a chameleon from South America related to llamas and
vicuña is smaller than a llama and has good it has a fur that is thinner and softer blah blah
blah blah blah no good this is an instruction so what are we saying we are asking a
question, right?
but basically we are talking to the model in a conversational way,
first, and second is that we are giving it an instruction,
how it is different from a llama, right?
In this case, for example, here we tell it,
write us an email to give our congratulations
to the people who were admitted to Stanford
and mention that you are also quite interested
in getting to know all of them in person.
Here we can see how it generates this example.
People admitted from Stanford, congratulations on your admission to Stanford, bla bla bla.
Pretty good, pretty good.
So, it starts to tell us the limitations.
The limitations are obviously that it has a limit of information.
It has the same limits as GPT 3.5, which in this case doesn't have access to all the information.
and also, well, that it lies, I mean, it doesn't lie but it hallucinates very easily, and also that it can generate
answers that are toxic and discriminatory, etc.
What did they do here?
The last thing they say is, look, obviously this is quite risky what we are doing because we are telling people, for example, bad actors, that they can generate models that this can easily generate fake news, that is, it can very easily generate false posts and so on.
So why are they going to make a model like this or a technique that...
generates this possibility that bad negative results are generated socially
Well, what they are saying here is, on the one hand, yes, it is possible that this happens, but on the other hand
if we put it out, it will make people start to discuss these things
and a defense mechanism is generated
about these things, especially from the academic community.
so this was exactly what happened so we are going to see now a little well how it works
so well we saw this if I would like to talk a little bit about now how is the method
this Self-Instruct that they used and in this case you can see here
that we had the 175 initial tasks, this is what a human wrote,
so what we do is we put it in randomly, each one is chosen,
and a language model takes it as input and then we say
Well, now that you know this input, generate me...
one more or five more, for example, and we can see how it generates this instruction for us.
Then what it does here is, once it generates this instruction for us, we have a second model
that classifies it and tells us, for example, in this case, the instruction that we are giving it is
if the text that I am going to give you next is in favor or against abortion.
So, the class we are looking for is...
pro-abortion and the input, the text, this is what the user is going to give, it would be, I believe that
women should have the right to choose whether they want or do not want to have an abortion, for example
in this case, so here we can see that in this case we have an output first, that is, we are
generating all this and here we have the pair of what we are talking about, we have the instruction,
we have the class and we have the input and this is what is going to be used later to do this
supervised fine-tuning
to the model. I would like to see here in the paper, here it explains a little more how it works.
Here, one thing that I found interesting is that it tells us how the distribution of these generations of instructions is.
You can see that, generally, if we look at the instruction, we see that these are the ways in which it is most divided.
First we are saying, write me, give me, find, create, make, describe, design, and these are the ways in which it is distributed.
So...
If we give instructions to these models that are trained with this dataset and we put this verb as part of our instruction, it will work much better.
Why? Because it was trained in this way.
If we had generated the dataset in another way, it could be different.
One thing I did want to say is that here we are saying that this,
In other words, generating this data using OpenAI is...
That's what OpenAI is saying. This violates my terms of use.
But we can see that other models, for example this one,
that I also made a video a while ago about this,
for this paper, they used here, if we go further down,
they used GPT-3 to generate the data.
In this case they generated 400,000 examples of this data.
It also seemed super smart what they did here.
The truth is very good, the way they used it, but perhaps this is not violating the terms of use because the resulting model that we have here, let's say, does not compete with OpenAI in the sense that it does not compete with a language model, so this would be ok, but hey, a detail, here we can see
the repo, you can see that this is all very recent, last week, we can see the repo here,
you will see that there are three things that are available, we have the data, 52,000 examples, we have the
code to generate the data, if we want to generate it ourselves, and we have the code to do the
fine tuning of the model. If we open the data that we have here
What do we have here?
Sorry, we go here, that is, in data, it will show us that we have the file.
And if we click on this file, we will see that this is how the data looks.
When we talk about data, what are we talking about?
Well, this is it. Here we have the instruction.
give me three tips to stay healthy and we have the output. Well, eat a balanced diet
and make sure to include a lot of fruits and vegetables, point two, exercise regularly to
keep your body active and strong and three, well, sleep enough and keep a consistent sleep
consistent blah blah blah. Well, this, but 52,000 times, this is what we use as a dataset, right?
So, summary of all this, here again, this blog post is excellent, it is a summary that we have here, the last one that is coming out, we can see that there is, basically what is happening, this model came out, 7 billion parameters, well, people started to say
Where can we run it? Well, we can run it on this GPU. No, something smaller. We can run it on this CPU. Where? No, on a Macbook. Yes, well, can it be a little less? Yes, it can be on, we run it on an iPhone. Well, perfect. Now where do we run it? We run it on an iPhone, on an Android. And now where? On a Raspberry Pi. And so on.
I mean, the code, the inference of this started to be optimized a lot
and this model started to become something that is very cheap to run.
I mean, imagine this, that in time it can become that each one of us can have
our phone, that is, not depending on an internet connection, for example, well here it gives us
what would be a background of all this and we can see examples etc. What we have here is
basically this, it is possible to train a model that has the same level, that is, Lama what it shows us
that it is possible to enter a model that has the same quality as GPT-3, this
that now we are going to see what it means,
it shows us that it is possible to run these models
in hardware that is available
for anyone, and then finally
Alpaca shows us that it is
possible to generate
a small dataset with about 500
dollars and then train a model
for
relatively little money
to have something that works
very well, right?
Well, here we can see the dataset and so on.
Instruction, output, the same as we saw before.
Why is all this important?
Here is an article by Eliezer Zhukovsky,
I thought it was very good.
What the guy says is,
look, I don't think people realize
how big this whole Stanford thing is.
The reason is this.
If you have a giant model,
For example, in this case, OpenAI has its huge model and gives access to people, to its API.
That means that we can give it an input and we get an output.
If we do it, if this happens, you are almost giving it the grandmother's jewel, what it says here.
for others, for the competition, you can almost clone your model.
without having the need for all that work that you put in to make your fine-tuning dataset.
So, what did Stanford do here?
Basically, what they did is, just like OpenAI had to pay people to make its 50,000-example dataset,
which they probably paid them, I don't know, hundreds of thousands of dollars,
They did it with 500 dollars, didn't they?
I mean, we can see how the cost of the input barrier is getting lower and lower, right?
So, basically what he is saying here is that
why Stanford is not selling this access?
Because basically he says, look, I don't want to get into legal mess with all these people
but if you don't mind getting into legal mess or no one is going to find out
You can easily use it.
the access to this API to generate a model that is similar to GPT 3.5.
and this is what it is telling us here.
Basically, there is a thing called knowledge distillation,
which is, for example, we have a huge, huge model
that probably parts of that model,
that is, the parameters of what were, are so many,
are thousands of millions of parameters,
many of those may contain very little information.
So what you can do is distill the model
only keeping the parameters that have a lot of information, right?
If you think of this as a reduction of dimensionality, it could be when the dimensionality is reduced and we have, let's say, axes, for example, that contain the greatest variance, right?
In this case...
That would be to distill the model, not to grab a giant model and then re-train a smaller model.
Well, this is super expensive, super expensive, and you have to have the giant model, access to the giant model to do it.
Instead, what we are doing here with Alpaca is, we are simply doing a very small fine-tuning, right?
So what he is saying here...
is that basically unless these companies, the truth is, have the ability to
or go to look for all the people who make these variations and put a demand on them
immediately or if not, cut access to inputs and outputs, it will be very difficult that
no one else can generate copies of these models, right?
I mean, it's something pretty crazy, and over here it explains it, I think it's in a part here, I think it's this one.
that it is not that someone got into your factory with a little camera and saw your entire
manufacturing process, no, simply what you are seeing is that you enter wood or resources,
that is, simply the resources inside your factory, then they see the product that comes out, a
blender by n and simply by seeing what comes in and what comes out, they can already copy your blender,
not your process of creating that blender, so it is very crazy in what direction all this will go
all this, right? You have to see, a little bit.
Well, we see why it is important, let's see a little bit now...
Very quickly, this is the porting that was made to the Facebook model, from Lama to C++.
So here, I'm not going to spend a lot of time with this, I'm just going to show you that this would be like your Stable Diffusion model.
What happened? Stable Diffusion came out and a very short time later it exploded.
This was in August 2022 but...
In a few months, variations of everything started to come out.
The community started to create a lot of things that made Stable Diffusion go ahead of MidJourney and DALI.
Once the community had ownership of this, they were able to take it in a much more advanced direction.
So maybe that's what is happening
with Lama. Here we can see that initially they needed more or less a GPU, there is the A100, which is a GPU that costs $ 8,000 to run it and now with all these optimizations that were done we can run it with an app to play it on video games, that is, it is not much needed.
Here we have it, Georgi Gervanov is the person who is doing it.
what they say is a person who is in Sofia in Bulgaria and before he had done the porting of
whisper and now he is doing it from llama, there is also another from alpaca, this one in particular is from llama
and the way they do it is spectacular, there are several tricks of how it is working and
Here we can see an example of how it is running on a notebook directly.
This is something crazy, because before we said that to run something that looks like a GPT-3 we needed approximately $50,000 of hardware.
And now we can run it on a notebook.
Why is this important? Because if you imagine a hospital, for example, or a place where there is no internet access,
It can't be connected to GPT-3, for example, to consume it.
And if we have a giant model, it can't run either.
Instead, now what we are saying is,
well, you can take this model that maybe,
for example, it can be a model that gives us some kind of medical information
and we can have it running in a hospital without an internet connection,
simply with a computer where it is running there and people have access to that model, right?
And well, here we have tests, etc.
We can see this, that they ran it in the Raspberry Pi.
How is it possible? I leave you here an article, this is a more technical article,
it explains a little bit how they did it, it is a little mathematical, it explains,
it is quite good, this article does seem to get into that side, right?
Here you can have it, you see, normally you have this model, 7 billion parameters,
generally if you run it at normal precision,
It's going to take up 28 GB of memory on the video card, right?
I mean, what video card has this amount of memory?
14 when you use it at medium precision, mid precision. 8
7 gigs when you use it at 4 and when you use it at 4, 3.5 gigs of memory, so here you can see that there are already many GPUs that have this capacity at the moment, so you can see that this is giving access to many more people, right?
Again, an interesting article, you can try it, here you can see it on the MacBook, on the Raspberry Pi, how it works, etc. It's pretty good.
Now.
The last thing we are going to see here is, well, now if I, because here we are starting from the fact that there is a model that would be Lama and then we are doing a fan tuning that would be with Alpaca, but how can we have the first model, the Lama one, right? Because this model is not open source, right?
I mean, Facebook made it available for researchers, but...
he told us that he does not have a commercial license, so you cannot use it for a company, for example.
So, if we want to make our own version, for us, as we are a company or we are a country or whatever, we want to have our own version, how do we do it?
Here what it is telling us is that if we take the table 15 of the original paper, you can see that to train that model they needed about 82,000 hours of this GPU.
Here they realize that anything, from a dollar to the hour, does not exist.
This is more or less between 2 and 3 dollars per hour for this type of GPU, the 80.
So we can see that here this is going to cost us about 250,000 dollars, not 85 as the person says.
But imagine that we do not have to think of it as individuals who are doing this, but a country, you can imagine.
If a country wants to have its own model of language, wants to be built on this architecture and be in the language of that country,
this is what you would get to have this type of effect.
of models, right? And here, well, once we have it, we can then do that fine-tuning
that we are talking about, right? And it can be run in a browser as well. And here it gives us several
examples of that, right? We can see here how Stable Diffusion, for example, is now running in this
Transformers library in Javascript, which is something that came out of Hive and Face a long time ago.
very little, that means that this is running on the side of the client, it is a web page that is running on the side of the client and is running this type of models, so that is what we are seeing as the future that is going to come.
I wanted to show you something interesting here that made this training even more efficient.
And now they are using something called LoRa, which we saw for Stable Diffusion.
But originally it was something that was made to train language models.
Basically what it tells us is, let's see if it has an image here.
No, it doesn't have an image.
but we can see that here...
Lora, what is it? It is basically a very economical way to train, to do fine-tuning on these models.
Why? Because you don't need to train everything, you just need to train the parameters that are activated more within this model, so it's very little.
and it also has a lot of optimizations, this library from Gameface and this one from Team Deadmau5, which is great.
So what did they do here? Well, they did a fine tuning in 5 hours.
Remember that before we had talked that the fine-tuning was in
Create. Share. Learn.
a cluster of 8 to 100, that's one, that is, every 100 comes out $ 8,000, they are 8 or more actually, that is, it is a huge cluster, no one has access to that.
On the other hand, here what they did is run this model in 5 hours, not 3 hours, 5 hours in a single RTX 4090, which is a fairly expensive GPU but at least people can buy it, right?
Who is giving us here?
It has examples of how different groups were using this code to make their own models.
Here we have it in Brazilian, in Chinese, in Japanese, in French and you can see here the version in Korean and in Japanese of these models.
so if we keep watching, here's a demo
that they made, Alpaca Lora, you can see here I wrote her an instruction, I say you are an
AI assistant who is trained in animals, you are courteous, concise and you are not lazy, ambiguous, and here I ask her
how are you? Tell me, tell me about the alpacas, what are they? And this is the answer she is giving me,
the alpacas are members of the family of the camelids and they are native to South America, they are known for their
good blah blah blah blah
I hope you can see that the answer is quite good, quite good and this was trained in a short time.
Now, let's see this model here, HineFace in Brazilian, what is this?
Well, this is a team.
They named it Cabrita. These guys are geniuses.
And what they did here was very easy.
They took the alpaca dataset, which we saw here.
It was the giant JSON we saw.
Giant JSON with 50,000 examples.
What they did was translate it into Portuguese
and they ran this training of Lora, which is what we are seeing here.
and it gave them a model that works as a chat gpt but only has this, 16 megabytes,
which is what the Lora model weighs, very little.
And here we have the demo, which is this.
Here we have the model and you can consume the demo if we download it, etc.
And this is how it works.
And here they have...
How can I start a career as a Data Scientist?
Write me in a form of a list.
And here it says, start by specializing in a specific field such as
Big Data, Computation, Bioinformatics, Statistics and blah blah blah, but
you can see how
how it compares, right?
That is, the answer of Stanford with that of Cabrita.
How can I start a career?
Get a title of Data Scientist or Data Engineering, blah blah blah, right?
That's it.
putting it back. How did they do it? Because this is interesting, right?
This means that countries can have their own model, similar to this.
So here what it's telling us is, well, how did they do it?
They translated the dataset into Portuguese and then they did that training,
running it again, three hours in an A100 cluster.
This is quite large.
and you can see here
Well, how were the results?
He compares them with the pack and says, look, it's much better,
and this can help make these language models
in languages that are not as represented,
such as Portuguese.
and they trained it in Goal Collab, that's why they used the A100, so now they say, well, our next iteration is going to be something that is commercially clean, that means that they can use a base model that is, that is,
that is free, which is not the case of Lama, Lama is the owner. So, well, we saw all these examples,
I hope it helps you to have an idea of ​​what are the things that are being done,
as you can see here we have a couple of languages ​​not four languages ​​
And this is all closed, it's just some research for now, but you can imagine that as new models come out, instead of LAMA,
that another model comes out, for example, from Stable Diffusion, and that it can be used as a base, this is going to be similar to Stable Diffusion.
That is, many, many models will start to come out in each language, and that will make all this much more accessible.
This is accelerating at a very, very fast pace.
So well, this was a video of alpaca, I hope it is useful, again I tried to cover the whole topic in the best possible way, surely there are things that I forgot, I thought this was very sharp, I think it can be the basis for many changes in the future and enlarge the access of this to
to several people who have access to different types of hardware, not only to large companies
and I also believe that this is happening in a much faster way than expected
maybe we were waiting for this to happen in years and the truth is happening in weeks or months
simply, so well I hope this video is useful to you, it was something I wanted to talk about
the truth and learn also that is why I did it so I send you a very big hug bye bye
